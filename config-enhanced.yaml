items:
  Study Design:
    - condition: null
      comment: >-
        Whether any guideline or checklist, e.g., CLEAR checklist, is used
        in designing and reporting, as appropriate for the study design (e.g., handcrafted
        radiomics or deep learning pipeline).
      description:
        Adherence to radiomics and/or machine learning-specific checklists
        or guidelines
      number: 1
      name: Checklist
      weight: 0.0368
    - condition: null
      comment: >-
        Whether inclusion and exclusion criteria are explicitly defined.
        These should lead to a representative study sample that matches the general
        population of interest for the study aim.
      description: Eligibility criteria that describe a representative study population
      number: 2
      name: ExclusionCriteria
      weight: 0.0735
    - condition: null
      comment: >-
        Whether the reference standard or outcome measure is representative of the current 
        clinical practice and robust. Examples of high-quality reference standards are preferably 
        histopathology, well-established clinical and genomic markers, the latest version of the prognostic 
        tools, guideline-based follow-up or consensus-based expert opinions. Examples of poor quality 
        reference standards are those based on qualitative image evaluation, images that are later 
        used for feature extraction, or outdated versions of prognostic tools.
      description: High-quality reference standard with a clear definition
      number: 3
      name: ReferenceStandard
      weight: 0.0919
  Imaging Data:
    - condition: null
      comment: >-
        Whether more than one institution is involved as a diagnostic imaging
        data source for radiomics analysis.
      description: Multi-center
      number: 4
      name: MultiCenter
      weight: 0.0438
    - condition: null
      comment: >-
        Whether the source of the radiomics data is an imaging technique
        that reflects established standardization approaches, such as acquisition protocol
        guidelines (e.g., PI-RADS specifications).
      description: Clinical translatability of the imaging data source for radiomics analysis
      number: 5
      name: ImagingProtocolGuidelines
      weight: 0.0292
    - condition: null
      comment: >-
        Whether the image acquisition protocol is clearly reported to ensure
        the replicability of the method.
      description: Imaging protocol with acquisition parameters
      number: 6
      name: ImagingProtocolReplicability
      weight: 0.0438
    - condition: null
      comment: >-
        Whether the time interval between the diagnostic imaging exams (used
        as an input for the radiomics analysis) and the outcome measure/reference standard
        acquisition is appropriate to validate the presence or absence of target conditions
        of the radiomics analysis at the moment of the diagnostic imaging exams. If there is
        no mention of when the reference standard was acquired relative to the diagnostic imaging 
        exams, the answer should be "no".
      description: The interval between imaging used and reference standard
      number: 7
      name: ImagingOutcomeInterval
      weight: 0.0292
  Segmentation:
    - condition:
        - 1
      comment: >-
        Whether the rules or the method of the segmentation are defined
        (e.g., margin shrinkage, peri-tumoral sampling, details of segmentation regardless
        of whether manual, semi-automated or automated methods are used). In the case
        of DL-based radiomics, the segmentation can refer to the rough delineation with
        bounding boxes or cropping the image around a region of interest.
      description: Transparent description of segmentation methodology
      number: 8
      name: SegmentationMethodology
      weight: 0.0337
    - condition:
        - 1
        - 2
      comment: >-
        If a segmentation technique that does not require any sort of human
        intervention is used, examples of the results should be presented and a formal
        assessment of its accuracy compared to domain expert annotations included in
        the study (e.g., DICE score or Jaccard index compared with a radiologist's semantic
        annotation).
      description: Formal evaluation of fully automated segmentation
      number: 9
      name: FullyAutomatedSegmentationEvaluation
      weight: 0.0225
    - condition:
        - 1
      comment: >-
        Whether final segmentation in the test set is produced by a single
        reader (manually or with a semi-automated tool) or an entirely automated tool,
        to better reflect clinical practice. Keep in mind that having a radiologist review
        the results of the automated segmentation is not considered clinical practice.
      description: Test set segmentation masks produced by a single reader or automated tool
      number: 10
      name: SegmentationReaderConsistency
      weight: 0.0112
  Image Processing and Feature Extraction:
    - condition: null
      comment: >-
        Whether preprocessing of the images is appropriately performed 
        considering the imaging modality (e.g., gray level normalization for MRI,
        image registration in case of multiple contrasts or modalities) and feature
        extraction techniques (i.e., 2D or 3D) that are used. For instance, in the
        case of large slice thickness (e.g., u22655 mm), extreme upsampling (e.g.,
        1 x 1 x 1 mm3) of the volume might be inappropriate. In such a case, 2D feature
        extraction could be preferable, ensuring in-plane isotropy of the pixels.
        On the other hand, achieving isotropic voxel values should be targeted in
        3D feature extraction, to allow for texture feature rotational invariance.
        Also, whether gray level discretization parameters (bin width, along with
        resulting gray level range, or bin count) are described in full detail. Description
        of different image types used (e.g., original, filtered) should also be included
        (e.g., high and low pass filter combinations for wavelet decomposition, sigma
        values for Laplacian of Gaussian edge enhancement filtering). If the image
        window is fixed, it should be clarified.
      description:
        Appropriate use of image preprocessing techniques with transparent
        description
      number: 11
      name: ImagePreprocessing
      weight: 0.0622
    - condition:
        - 3
      comment: >-
        Whether a standardized software (e.g., compliant with IBSI) was
        used for feature extraction, including information on the version number.
      description: Use of standardized feature extraction software
      number: 12
      name: FeatureExtractionSoftware
      weight: 0.0311
    - condition: null
      comment: >-
        Whether feature types (e.g., hand-crafted, deep features) and feature
        classes (for hand-crafted) are described. Also, if a default configuration statement
        is provided for the remaining feature extraction parameters. A file presenting
        the complete configuration of these settings should be included in the study
        materials (e.g., parameter file such as in YAML format, screenshot if a dedicated
        file for this is not available for the software). In the case of DL, neural
        network architecture along with all image operations should be described.
        Mention of features by name or feature groups is not sufficient. Explicitly 
        check whether the scientific article provides a list of the radiomic features 
        which are being extracted or if it mentions that the default set of parameters 
        was used.
      description: >-
        Transparent reporting of feature extraction parameters, otherwise
        providing a default configuration statement
      number: 13
      name: FeatureExtractionParameterReporting
      weight: 0.0415
  Feature Processing:
    - condition:
        - 4
      comment: >-
        Whether unstable features are removed via test-retest, reproducibility
        analysis by analysis of different segmentations, or stability analysis [i.e.,
        image perturbations]. Instability may be due to random noise introduced by manual
        or even automated image segmentation or exposed in a scan-rescan setting. The
        specific methods used should be clearly presented, with specific results for
        each component in multi-step feature removal pipelines.
      description: Removal of non-robust features
      number: 14
      name: NonRobustFeatureRemoval
      weight: 0.0200
    - condition:
        - 4
      comment: >-
        Whether dimensionality is reduced by selecting the more informative
        features such as with algorithm-based feature selection (e.g., LASSO coefficients,
        Random Forest feature importance), univariate correlation, collinearity, or
        variance analysis. The specific methods used should be clearly presented, with
        specific results for each component in multi-step feature removal pipelines.
      description: Removal of redundant features
      number: 15
      name: RedundantFeatureRemoval
      weight: 0.0200
    - condition:
        - 4
      comment: >-
        Whether the number of instances and features in the final training
        data set is appropriate according to the research question and modeling algorithm.
        This should be demonstrated by statistical means, empirically through consistency
        of performance in validation and testing, or based on previous evidence in the
        literature.
      description: Appropriateness of dimensionality compared to data size
      number: 16
      name: DimensionalityAppropriateness
      weight: 0.0300
    - condition:
        - 5
      comment: >-
        Whether DL pipeline consistency of performance has been assessed
        in a test-retest setting, for example by a scan-rescan approach, use of segmentations
        by different readers, or stability analysis [i.e., image perturbations].
      description: Robustness assessment of end-to-end deep learning pipelines
      number: 17
      name: EndToEndDeepLearningPipelineRobustness
      weight: 0.0200
  Preparation for Modeling:
    - condition: null
      comment: >-
        Whether the training-validation-test data split is done at the
        very beginning of the analysis pipeline, prior to any processing step. Data
        split should be random but reproducible (e.g., fixed random seed), preferably
        without altering outcome variable distribution in the test set (e.g., using
        a stratified data split). Moreover, the data split should be on the patient
        level, not the scan level (i.e., different scans of the same patient should
        be in the same set). Proper data partitioning should guarantee that all data
        processing (e.g., scaling, missing value imputation, oversampling or undersampling)
        is done blinded to the test set data. These techniques should be exclusively
        fitted on training (or development) data sets and then used to transform test
        data at the time of inference. If a single training-validation data split is
        not done and a resampling technique (e.g., cross-validation) is used instead,
        test data should always be handled separately from this.
      description: Proper data partitioning process
      number: 18
      name: DataPartitioning
      weight: 0.0599
    - condition: null
      comment: >-
        Whether potential confounding factors were analyzed, identified
        if present, and removed if necessary (e.g., if it has a strong influence on
        generalizability). These may include different distributions of patient characteristics
        (e.g., gender, lesion stage or grade) across sites or scanners. Be sure to focus on 
        confounders stemming from the acquisition process, such as scanner type, manufacturer,
        or scanner parameters, or from the individual patient characteristics, such as age,
        weight, or comorbidities.
      description: Handling of confounding factors
      number: 19
      name: HandlingOfConfoundingFactors
      weight: 0.0300
  Metrics and Comparison:
    - condition: null
      comment: >-
        Whether appropriate accuracy metrics are reported, such as AUC for
        Receiver Operating Characteristics (ROC) or Precision-Recall (PRC) curves and
        confusion matrix-derived accuracy metrics (e.g., specificity, sensitivity, precision,
        F1 score) for classification tasks; MSE, RMSE, and MAE for regression tasks.
        For classification tasks, the confusion matrix should always be included, to
        allow the calculation of additional metrics. If training a DL network, loss
        curves should be presented.
      description: Use of appropriate performance evaluation metrics for task
      number: 20
      name: PerformanceEvaluationMetrics
      weight: 0.0352
    - condition: null
      comment: >-
        Whether uncertainty measures are included in the analysis, such
        as 95% confidence interval (CI), standard deviation (SD), or standard error
        (SE). Report on methodology to derive that distribution (ie. bootstrapping with
        replacement, etc).
      description: Consideration of uncertainty
      number: 21
      name: UncertaintyAssessment
      weight: 0.0234
    - condition: null
      comment: Whether the final model's calibration is assessed.
      description: Calibration assessment
      number: 22
      name: CalibrationAssessment
      weight: 0.0176
    - condition: null
      comment: >-
        Use of a single imaging set (such as a single MRI sequence rather
        than multiple, or a single phase in a dynamic contrast-enhanced scan) should
        be preferred, as multi-parametric imaging may unnecessarily increase data dimensionality
        and risk of overfitting. Therefore, in the case of multi-parametric studies,
        uni-parametric evaluations should also be performed to justify the need for
        a multi-parametric approach by formally comparing their performance (e.g., DeLong's
        or McNemar's tests). This item is also intended to reward studies that experimentally
        justify the use of more complex models compared to simpler alternatives, in
        regard to input data type. Uni-parametric imaging implies the acquisition of a single 
        imaging modality, as opposed to multi-parametric imaging, which involves the acquisition of multiple 
        imaging modalities.
      description: Use of uni-parametric imaging or proof of its inferiority
      number: 23
      name: UniParametricImagingEvaluation
      weight: 0.0117
    - condition: null
      comment: >-
        Whether a non-radiomic method that is representative of the clinical practice is included
        in the analysis for comparison purposes. Non-radiomic methods might include semantic features, 
        RADS or RECIST scoring, and simple volume or size evaluations. If no non-radiomics method is available, 
        proof of improved diagnostic accuracy (e.g., improved performance of a radiologist assisted by the 
        model's output) or patient outcome (e.g., decision analysis, overall survival) should be provided. 
        In any case, the comparison should be done with an appropriate statistical method to evaluate the 
        added practical and clinical value of the model (e.g., DeLong's test for AUC comparison, decision 
        curve analysis for net benefit comparison, Net Reclassification Index). Furthermore, in case of 
        multiple comparisons, multiple testing correction methods (e.g., Bonferroni) should be considered 
        in order to reduce the false discovery rate provided that the statistical comparison is done with 
        a frequentist approach (rather than Bayesian).
      description: Comparison with a non-radiomic approach or proof of added clinical value
      number: 24
      name: NonRadiomicApproachComparison
      weight: 0.0293
    - condition: null
      comment: >-
        Whether a comparison with a simple baseline reference model (such
        as a Zero Rules/No Information Rate classifier) was performed. Use of machine
        learning methods should be justified by proof of increased performance. In any
        case, the comparison should be done with an appropriate statistical method (e.g.,
        DeLong's test for AUC comparison, Net Reclassification Index). Furthermore,
        in case of multiple comparisons, multiple testing correction methods (e.g.,
        Bonferroni, Benjamini-Hochberg, or Tukey) should be considered in order to reduce
        the false discovery rate provided that the statistical comparison is done with
        a frequentist approach (rather than Bayesian).
      description: Comparison with simple or classical statistical models
      number: 25
      name: SimpleBaselineComparison
      weight: 0.0176
  Testing:
    - condition: null
      comment: >-
        Whether the model is tested on an independent data set that is sampled
        from the same source as the training and/or validation sets.
      description: Internal testing
      number: 26
      name: InternalTesting
      weight: 0.0375
    - condition: null
      comment: >-
        Whether the model is tested with independent data from other institution(s).
        This also applies to the studies validating the previously published models
        trained at another institution.
      description: External testing
      number: 27
      name: ExternalTesting
      weight: 0.0749
  Open Science:
    - condition: null
      comment: >-
        Whether any imaging, segmentation, clinical, or radiomics analysis
        data is shared with the public.
      description: Data availability
      number: 28
      name: DataAvailability
      weight: 0.0075
    - condition: null
      comment: >-
        Whether all scripts related to automatic segmentation and/or modeling
        are shared with the public. These should include clear instructions for their
        implementation (e.g., accompanying documentation, tutorials).
      description: Code availability
      number: 29
      name: CodeAvailability
      weight: 0.0075
    - condition: null
      comment: >-
        Whether the final model is shared in the form of a raw model file
        or as a ready-to-use system. If automated segmentation was employed, the corresponding
        trained model should also be made available to allow replication. These should
        include clear instructions for their usage (e.g., accompanying documentation, tutorials).
      description: Model availability
      number: 30
      name: ModelAvailability
      weight: 0.0075

conditions:
  - comment: >-
      "Segmentation" refers to i) Fine delineation of a region or
      volume of interest; ii) Rough delineation with bounding boxes; or, iii) cropping
      the image around a region of interest.
    description: Does the study include segmentation?
    name: StudyIncludesSegmentation
    number: 1
  - comment: >-
      "Fully automated segmentation" refers to segmentation process
      without any human intervention.
    description: Does the study include fully automated segmentation?
    name: FullyAutomatedSegmentation
    number: 2
  - comment: >-
      "Hand-crafted radiomic features" (i.e., traditional radiomic
      features) are created in advance by human experts or mathematicians.
    description: Does the study include hand-crafted feature extraction?
    name: HandCraftedFeatureExtraction
    number: 3
  - comment: >-
      "Tabular data" refers to data that is organized in a table with
      rows and columns (i.e., numeric radiomic features in a tabulated format, which 
      is usually seen in hand-crafted and some deep learning-based studies as deep features).
    description: Does the study include tabular data?
    name: TabularData
    number: 4
  - comment: >-
      "End-to-end deep learning" refers to the use of deep learning
      to directly process the image data and produce a classification or regression
      model.
    description: Does the study include end-to-end deep learning?
    name: EndToEndDeepLearning
    number: 5
