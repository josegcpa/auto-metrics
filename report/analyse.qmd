---
title: "A short report on automating METRICS"
code-line-numbers: true
format: 
  html:
    df-print: paged
    toc: true
    toc-location: left
    fig-format: png
    fig-dpi: 300
editor: visual
bibliography: bibliography.bib
csl: "ieee.csl"
author: José Guilherme de Almeida
footnotes-hover: true
reference-location: margin
code-fold: true
code-overflow: wrap

include-in-header:
  - text: |
      <style>
        .cell-output-stdout code {
          word-break: break-wor !important;
          white-space: pre-wrap !important;
          max-height: 600px;
        }
        .cell-output-stdout {
          background-color: #f0eded;
          border-radius: 4px;
        }
      </style>
---

# TL;DR

It is possible --- to some extent --- to automate METRICS (a radiomics publication quality assessment tool) using large language models. You can test this [here](https://auto-metrics.netlify.app/) with your own Gemini API key.

# Context

Here we show a proof of concept on how it is possible to automate some aspects of scientific analysis. In particular, we will be doing this through the Methodological Radiomics Score (otherwise known as METRICS) @Kocak2024-wk. METRICS comprises a set of helpful guidelines to determine the quality of a scientific publication doing some form of radiomics analysis (i.e. the systematic extraction of imaging features from clinical images and the correlation of these features with patient-specific outcomes or image characteristics @Van_Timmeren2020-gy). These sorts of scores are not new, with the radiomics quality score (RQS) being the other main contender for scoring radiomics @Lambin2017-rn. Whether METRICS is more or less useful than RQS has been addressed in other publications and it is not the particular concern of this piece to clarify it @Kocak2024-sv. Instead, here we make use of a recent publication on the reproducibility of METRICS scores: Akinci D'Antonoli and others' "Reproducibility of methodological radiomics score (METRICS): an intra- and inter-rater reliability study endorsed by EuSoMII" @Akinci-D-Antonoli2025-ep.

## Short summary of *Akinci D'Antonoli and others*

In this work, a set of radiologists was divided into three separate experience levels --- from 1 (less experienced) to 3 (more experienced) - based on their experience not only as radiologists but also as academics. Two experimental conditions were tested --- no training on METRICS and with training with METRICS. The overall experimental design is sound and the conclusions are quite interesting: in essence, there is low-to-moderate inter-rater reliability, and this is particularly true when comparing between no training and with training [^1].

[^1]: The article also features repeatibility experiments, but that is not within the scope of this short report.

## Visualizing reproducibility results from Akinci D'Antonoli and others

Here we are providing only a recapitulation of what is provided as Supplementary Information 1. In particular, here we only use the Cohen's Kappa to analyse how agreement varies between different conditions and raters. As observable in @fig-cohen-raters, Cohen's Kappa (a measure of inter-rater agreement) is quite consistent and between 0.4 and 0.6. The one exception is the very high Kappa between experience level 3 (highest experience level) with and without training. This may highlight the effect of experience or it may be biased as the four radiologists annotated with experience level 3 are either editors/members of the editorial board of European Radiology (and are more exposed to this sort of analysis) or took part in developing METRICS (AP/AS/MK and LU, respectively); only three other authors are in this position of being parts of editorial teams at radiology journals/publications or having participated in METRICS. In any case it quite a stark difference in our opinion.

Further analyzing how this looks for items and conditions separetely (i.e. calculating the average item-specific Cohen's Kappa) further confirms the overall analysis as illustrated in @fig-cohen-raters B. Finally, it is interesting to see that there is little agreement as far as determining the different conditions is concerned @fig-cohen-raters C. This part is crucial as some items depend on specific conditions being met for grading, otherwise they are discarded.

```{r library-loading}
#| output: false
library(tidyverse)
library(rjson)
library(irr)
library(knitr)
library(kableExtra)
library(patchwork)
library(psych)
library(stats)

theme_classic_2 <- function(base_size = 11,
                            base_family = "",
                            base_line_size = 11/22,
                            base_rect_size = 11/22) {
  theme_classic(base_size = base_size,
                base_family = base_family,
                base_line_size = base_line_size,
                base_rect_size = base_rect_size) +
    theme(axis.text = element_text(size = base_size,
                                   colour = "black"),
          title = element_text(size = base_size + 1),
          axis.title = element_text(size = base_size),
          strip.background = element_blank(),
          strip.text = element_text(size = base_size,
                                    face = "bold",
                                    hjust = 0.0,
                                    margin = margin(b = 2)),
          legend.title = element_text(size = base_size, face = "bold"),
          panel.background = element_blank(),
          legend.text = element_text(size = base_size),
          axis.line = element_line(),
          axis.ticks = element_line(),
          panel.grid = element_blank()) %>%
    return
}

make_heatmap <- function(df) {
  plot_output <- ggplot(df, aes(x = rater1, y = rater2, fill = estimate)) + 
    geom_tile(colour = "black", linewidth = 0.5) +
    theme_classic_2(base_size = 8) +
    geom_label(aes(label = sprintf("%.2f", estimate)), 
               fill = "white",
               colour = "black", 
               size = 2.5,
               label.size = unit(0, "line"),
               label.padding = unit(0.3, "line"),
               vjust = 0,
               alpha = 0.7) + 
    geom_label(aes(label = sprintf("[%.2f, %.2f]", lower, upper)), 
               fill = "white",
               colour = "black", 
               size = 2.5,
               label.size = unit(0, "line"),
               label.padding = unit(0.3, "line"),
               vjust = 1.0,
               alpha = 0.7) + 
    scale_x_discrete(expand = c(0,0)) +
    scale_y_discrete(expand = c(0,0)) +
    theme(axis.title = element_blank(),
          legend.position = "bottom",
          legend.key.height = unit(0.5, "line"),
          legend.title = element_text(vjust = 1.0),
          legend.margin = margin())
  return(wrap_plots(plot_output))
}

skip_columns <- c(
  "elapsed_time", 
  "Summary",
  "error",
  "prompt",
  "article_text",
  "metadata")

match_class <- c(`n/a` = NA, no = 0, yes = 1)
key_order <- c(paste0("Item", 1:30), paste0("Condition", 1:30))
```

```{r data-loading}
#| warning: false
all_dfs <- list()
df_original <- read_csv(
  "../data/akinci-dantonoli/inter-rater.csv", col_types = c(rep("c", 40), "i", "i")) 
df <- df_original %>% 
  gather(key = "key", value = "value", 
         -group, -exp, -dois, -dois_clean, -title) %>% 
  subset(key != "Total METRICS score:") %>%
  subset(!grepl("Quality", key)) %>%
  mutate(key = gsub("#", "", key)) %>%
  mutate(group = ifelse(group == 1, "no training", "with training")) %>%
  mutate(value = match_class[value])

all_dfs$akinci_dantonoli <- df

df_original
```

```{r data-processing}
#| warning: false

all_raters <- c("no training_1","no training_2","no training_3",
                "with training_1","with training_2","with training_3")
all_rater_labels <- c(
  "Not\ntrained (1)", "Not\ntrained (2)", "Not\ntrained (3)",
  "Trained (1)", "Trained (2)", "Trained (3)")

long_df <- pivot_wider(df, values_from = value, names_from = c("group", "exp"))

rater_combinations <- combn(all_raters, 2)

all_cohens <- list()
for (idx in 1:ncol(rater_combinations)) {
  raters <- rater_combinations[, idx]
  sub_df <- subset(long_df, !grepl("Condition", key))[c(raters[1], raters[2])] %>%
    na.omit() %>%
    as.matrix()
  if ((nrow(sub_df) > 10) & (var(sub_df[,1]) > 0) & (var(sub_df[,2]) > 0)) { # guarantee that at least 10 articles were graded
    kk <- as.data.frame(cohen.kappa(sub_df)$confid)
    curr_df <- kk
    curr_df$rater1 <- raters[1]
    curr_df$rater2 <- raters[2]
    all_cohens[[length(all_cohens) + 1]] <- curr_df
  }
}

all_cohens_items <- list()
for (k in unique(long_df$key)) {
  for (idx in 1:ncol(rater_combinations)) {
    raters <- rater_combinations[, idx]
    sub_df <- subset(long_df, key == k)[c(raters[1], raters[2])] %>%
      na.omit() %>%
      as.matrix()
    if ((nrow(sub_df) > 10) & (var(sub_df[,1]) > 0) & (var(sub_df[,2]) > 0)) { # guarantee that at least 10 articles were graded
      kk <- as.data.frame(cohen.kappa(sub_df)$confid)
      curr_df <- kk
      curr_df$rater1 <- raters[1]
      curr_df$rater2 <- raters[2]
      curr_df$item <- k
      curr_df$n <- nrow(sub_df)
      all_cohens_items[[length(all_cohens_items) + 1]] <- curr_df 
    }
  }
}
```

```{r cohen-calculation}
cohens_overall <- do.call(rbind, all_cohens) %>% 
  mutate(rater1 = factor(rater1, all_raters, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_raters, all_rater_labels))

cohens_average_items <- do.call(rbind, all_cohens_items) %>% 
  subset(grepl("Item", item)) %>%
  mutate(rater1 = factor(rater1, all_raters, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_raters, all_rater_labels)) %>%
  group_by(rater1, rater2) %>%
  na.omit() %>%
  summarise(se = sd(estimate) / sqrt(length(estimate)),
            estimate = mean(estimate), 
            lower = max(estimate - abs(qnorm(0.025)) * se, 0),
            upper = min(estimate + abs(qnorm(0.025)) * se, 1),
            .groups = "drop")

cohens_average_conditions <- do.call(rbind, all_cohens_items) %>% 
  subset(grepl("Condition", item)) %>%
  mutate(rater1 = factor(rater1, all_raters, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_raters, all_rater_labels)) %>%
  group_by(rater1, rater2) %>%
  na.omit() %>%
  summarise(se = sd(estimate) / sqrt(length(estimate)),
            estimate = mean(estimate), 
            lower = max(estimate - abs(qnorm(0.025)) * se, 0),
            upper = min(estimate + abs(qnorm(0.025)) * se, 1),
            .groups = "drop")

cohens_overall_plot <- make_heatmap(cohens_overall) +
  scale_fill_distiller(palette = 3, name = "Cohen's Kappa")

cohens_average_items_plot <- make_heatmap(cohens_average_items) +
  scale_fill_distiller(palette = 3, name = "Average Cohen's\nKappa across items")

cohens_average_conditions_plot <- make_heatmap(cohens_average_conditions) +
  scale_fill_distiller(palette = 3, name = "Average Cohen's\nKappa across conditions")
```

```{r fig-cohen-raters}
#| fig.height: 10.0
#| fig.width: 5.0
#| label: fig-cohen-raters
#| fig-cap: Cohen's Kappa for Akinci D'Antonoli and others (2025).
(cohens_overall_plot / 
  cohens_average_items_plot /
  cohens_average_conditions_plot) +
  plot_annotation(tag_levels = c("A"))
```

# LLMs for scientific assessment --- replacement or sidekick?

Given this rich dataset --- which can be easily linked to the relevant literature --- we had a simple thought: how easily can this be performed automatically? With the advent of large language models (LLMs), there are ample opportunities for this. Indeed, combining LLMs with relatively deterministic structured generation can ensure that i) we always get the same results and ii) we always get structured results. The first can be achieved through 0 temperature sampling (i.e. no randomness given the same input) and constrained generation (i.e. the LLM can only generate tokens which force it to satisfy a set of constraints).

Here, we have a concrete hypothesis: is the average agreement between an LLM and human rater groups similar to that which is observed between human rater groups? If so, this puts LLMs as a possible second read as far as METRICS assessments are concerned.

## The protocol

We used Gemini 2.0 Flash because it is remarkably cheap for these sorts of experiments (and in general). As a prompt, we use the following (it is somewhat long so bear with us, it copies the guidelines made available in the original METRICS paper and in the [METRICS helper website](https://metricsscore.github.io/metrics/METRICS.html)):

```{r prompt-exposition}
read_file("prompt.txt") %>%
  cat
```

We start by detailing some instructions that the LLM should follow. Then, we provide clear evaluation metrics and a rating rubric which should be followed. Finally, we declare the input format and make space for the actual METRICS guidelines and output format. Where we wrote `{article}`, that is where the text is inserted. To get these article texts we went through the articles which we could access --- between publicly available and institutionally-accessible articles we were able to access 31 out of the total of 34 --- and simply copied the text between the abstract (incl.) and the references (excl.) for each.

To ensure structured outputs, we make use of a relatively strict --- but simple! --- data model for the output:

```{python ratings-schematic}
#| echo: true
#| eval: false
from enum import Enum
from pydantic import BaseModel


class RatingEnum(Enum):
    yes = "yes"
    no = "no"


class RatingWithNAEnum(Enum):
    yes = "yes"
    no = "no"
    na = "n/a"


class Rating(BaseModel):
    rating: RatingEnum
    reason: str


class RatingWithNA(BaseModel):
    rating: RatingWithNAEnum
    reason: str


class Metrics(BaseModel):
    Summary: str
    Condition1: Rating
    Condition2: Rating
    Condition3: Rating
    Condition4: Rating
    Condition5: Rating
    Item1: Rating
    Item2: Rating
    Item3: Rating
    Item4: Rating
    Item5: Rating
    Item6: Rating
    Item7: Rating
    Item8: RatingWithNA
    Item9: RatingWithNA
    Item10: RatingWithNA
    Item11: Rating
    Item12: RatingWithNA
    Item13: Rating
    Item14: RatingWithNA
    Item15: RatingWithNA
    Item16: RatingWithNA
    Item17: RatingWithNA
    Item18: Rating
    Item19: Rating
    Item20: Rating
    Item21: Rating
    Item22: Rating
    Item23: Rating
    Item24: Rating
    Item25: Rating
    Item26: Rating
    Item27: Rating
    Item28: Rating
    Item29: Rating
    Item30: Rating
```

## The tests

Upon loading, we were able to verify that all the LLM outputs have, as expected, generated a correctly formatted JSON. Below we present an example for one such article. Additionally, the flexibility of our specification also allows for a small reason to be appended to each rating --- this facilitates the verification of LLM claims. Indeed, this explanation ends up being helpful at a later stage of this analysis when we start getting into possible reasons for disagreement.

```{r loading-json-example}
llm_rating_files <- list.files(
  path = "../ratings/gemini/akinci-dantonoli", 
  pattern = "*json", 
  full.names = T)
names(llm_rating_files) <- lapply(
  llm_rating_files, function(x) gsub(".json", "", str_split(x, "/")[[1]][5])
)

file_to_display <- read_file(file = llm_rating_files[[1]]) %>%
  fromJSON
file_to_display <- file_to_display[names(file_to_display) != "metadata"]

cat(sprintf("Loading and printing the JSON for %s:\n\n%s", 
            names(llm_rating_files)[[1]],
            toJSON(file_to_display)))
```

Upon analysis of @fig-cohen-raters-llm we see quite clearly that, while there is some agreement between LLMs and raters, this is very much on the lower end of the agreement spectrum which is observed for human rater groups. Indeed, it can be risky to simply deployed this sans supervision. However, we note that this is still well within what would be acceptable had this been an additional reviewer. As noted earlier, it might be a good format to have an LLM perform these assessments as a second reader which ends up reinforcing some conclusions or reanalysing others.

```{r load-llm-ratings-ada2025}
all_llm_ratings <- list()
for (f in llm_rating_files) {
  title <- gsub(".json", "", str_split(f, "/")[[1]][5])
  json_data <- fromJSON(file = f)
  json_data <- json_data[!(names(json_data) %in% skip_columns)]
  json_df <- data.frame(
    key = names(json_data), 
    llm = unlist(lapply(json_data, function(x) x$rating)), title = title)
  all_llm_ratings[[length(all_llm_ratings) + 1]] <- json_df
}

llm_df <- do.call(rbind, all_llm_ratings) %>%
  mutate(llm = match_class[llm]) %>%
  mutate(title = gsub("_", "/", title))

# redefine rater vectors to include LLMs
all_raters <- c("no training_1","no training_2","no training_3",
                "with training_1","with training_2","with training_3",
                "llm")
all_rater_labels <- c(
  "Not\ntrained (1)", "Not\ntrained (2)", "Not\ntrained (3)",
  "Trained (1)", "Trained (2)", "Trained (3)",
  "LLM"
)

full_df <- merge(
  pivot_wider(df, values_from = value, names_from = c("group", "exp")), 
  llm_df, 
  by = c("key", "title"))
```

```{r calculate-cohens-llm-ada2025}
#| warning: false
rater_combinations <- combn(all_raters, 2)

all_cohens <- list()
for (idx in 1:ncol(rater_combinations)) {
  raters <- rater_combinations[, idx]
  sub_df <- subset(full_df, !grepl("Condition", key))[c(raters[1], raters[2])] %>%
    na.omit() %>%
    as.matrix()
  if ((nrow(sub_df) > 10) & (var(sub_df[,1]) > 0) & (var(sub_df[,2]) > 0)) { # guarantee that at least 10 articles were graded
    kk <- as.data.frame(cohen.kappa(sub_df)$confid)
    curr_df <- kk
    curr_df$rater1 <- raters[1]
    curr_df$rater2 <- raters[2]
    all_cohens[[length(all_cohens) + 1]] <- curr_df
  }
}

all_cohens_items <- list()
for (k in unique(full_df$key)) {
  for (idx in 1:ncol(rater_combinations)) {
    raters <- rater_combinations[, idx]
    sub_df <- subset(full_df, key == k)[c(raters[1], raters[2])] %>%
      na.omit() %>%
      as.matrix()
    if ((nrow(sub_df) > 10) & (var(sub_df[,1]) > 0) & (var(sub_df[,2]) > 0)) { # guarantee that at least 10 articles were graded
      kk <- as.data.frame(cohen.kappa(sub_df)$confid)
      curr_df <- kk
      curr_df$rater1 <- raters[1]
      curr_df$rater2 <- raters[2]
      curr_df$item <- k
      curr_df$n <- nrow(sub_df)
      all_cohens_items[[length(all_cohens_items) + 1]] <- curr_df 
    }
  }
}
```

```{r calculate-cohens-llm-averages}
cohens_overall <- do.call(rbind, all_cohens) %>% 
  mutate(rater1 = factor(rater1, all_raters, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_raters, all_rater_labels))

cohens_average_items <- do.call(rbind, all_cohens_items) %>% 
  subset(grepl("Item", item)) %>%
  mutate(rater1 = factor(rater1, all_raters, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_raters, all_rater_labels)) %>%
  group_by(rater1, rater2) %>%
  na.omit() %>%
  summarise(se = sd(estimate) / sqrt(length(estimate)),
            estimate = mean(estimate), 
            lower = max(estimate - abs(qnorm(0.025)) * se, 0),
            upper = min(estimate + abs(qnorm(0.025)) * se, 1),
            .groups = "drop")

cohens_average_conditions <- do.call(rbind, all_cohens_items) %>% 
  subset(grepl("Condition", item)) %>%
  mutate(rater1 = factor(rater1, all_raters, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_raters, all_rater_labels)) %>%
  group_by(rater1, rater2) %>%
  na.omit() %>%
  summarise(se = sd(estimate) / sqrt(length(estimate)),
            estimate = mean(estimate), 
            lower = max(estimate - abs(qnorm(0.025)) * se, 0),
            upper = min(estimate + abs(qnorm(0.025)) * se, 1),
            .groups = "drop")

cohens_overall_plot <- make_heatmap(cohens_overall) +
  scale_fill_distiller(palette = 3, name = "Cohen's Kappa")

cohens_average_items_plot <- make_heatmap(cohens_average_items) +
  scale_fill_distiller(palette = 3, name = "Average Cohen's\nKappa across items")

cohens_average_conditions_plot <- make_heatmap(cohens_average_conditions) +
  scale_fill_distiller(palette = 3, name = "Average Cohen's\nKappa across conditions")
```

```{r fig-cohen-raters}
#| fig.height: 9.0
#| fig.width: 5.0
#| label: fig-cohen-raters-llm
#| fig-cap: Cohen's Kappa for Akinci D'Antonoli and others (2025) with LLM.
(cohens_overall_plot / 
  cohens_average_items_plot /
  cohens_average_conditions_plot) +
  plot_annotation(tag_levels = c("A"))
```

In @fig-proportions we show how these scores are somewhat variable when analysed on a per-item basis. While some items are somewhat consistent (Items 1, 2, 20, 21, 22, 29 and 30 all have percent agreements[^2] \> 70%), most are relatively scattered.

[^2]: here we used percent agreements as opposed to Cohen's Kappa because the latter is not defined when data has 0 variance (some questions were always answered the same) and has unintuitive behavior when it gets too close to 0 or 1.

```{r calculate-readers-vs-llm-ada2025}
#| fig.height: 5
#| fig.width: 6
#| warning: false
#| fig-cap: Proportion of agreement between human readers and Auto-METRICS assessment stratified by training groups and experience levels.
#| label: fig-proportions

proportion_data <- merge(
  df,
  llm_df, 
  by = c("key", "title")) %>% 
  group_by(key, group, exp) %>% 
  summarise(capture = mean(value == llm, na.rm = T), .groups = "drop") %>%
  mutate(key = factor(key, rev(key_order))) %>%
  mutate(cond_or_item = ifelse(grepl("Condition", key), "Condition", "Item"))

proportion_data %>%
  ggplot(aes(x = capture, y = key, colour = as.factor(exp),
             shape = group, group = paste(exp, group))) + 
  geom_point() + 
  theme_classic_2(base_size = 8) +
  theme(panel.grid.major.y = element_line(color = "grey90"),
        legend.key.size = unit(0, "line"),
        strip.text.y = element_text(angle = 0)) +
  xlab("Proportion of agreements with LLM") +
  ylab("Item") +
  facet_grid(cond_or_item ~ ., scales = "free", space = "free", ) + 
  scale_colour_brewer(palette = "Set1",
                      name = "Experience level") +
  scale_shape_manual(values = c(2, 3), name = "Training") +
  guides(x = guide_axis(cap = T))
```

## Analysing some specific cases

My main point of interest now was getting to know how these differences are manifested. As such, we will focus on cases where the number of disagreements with human raters is relatively large --- in other words, we want to know if there are actual intuitive reasons behind LLM behavior. We consider here that agreements are worth 1 point, disagreements -1, and incompatibilities (i.e. LLM provided answer but answer for human raters is N/A) is 0.

A main point of this analysis is for us to think through of some possible improvements that can be added to the description of each item to make it as clear as possible for automated assessment.

```{r fig-disagreement}
#| fig.height: 5
#| fig.width: 4
#| fig.cap: Average agreement between the LLM and all rater groups. Negative values imply a higher proportion of disagreements, whereas higher values imply a higher proportion of agreements.
#| label: fig-disagreement
score_agreement <- function(a, b) {
  d <- case_when(
    is.na(a) | is.na(b) ~ NA,
    a != b ~ -1,
    a == b ~ 1
  )
  return(d)
}

sum_na_if_empty <- function(x) {
  if (length(x) == 0) {
    return(NA)
  }
  return(sum(x))
}

disagreement_df <- full_df %>%
  rowwise() %>%
  mutate(
    n_disagreements = c(
      score_agreement(`no training_1`, llm),
      score_agreement(`no training_2`, llm),
      score_agreement(`no training_3`, llm),
      score_agreement(`with training_1`,llm),
      score_agreement(`with training_2`, llm), 
      score_agreement(`with training_3`, llm)
    ) %>%
      Filter(f = function(x) !is.na(x)) %>%
      sum_na_if_empty
  ) %>%
  arrange(n_disagreements)

threshold <- 1.0

aggregated_disagreement_df <- disagreement_df %>%
  group_by(key) %>%
  summarise(average_disagreement_score = mean(n_disagreements, na.rm = T)) %>%
  arrange(average_disagreement_score) %>%
  mutate(Reassessed = ifelse(average_disagreement_score < threshold, "Yes", "No"))


aggregated_disagreement_df %>%
  ggplot(aes(x = average_disagreement_score,
             y = reorder(key, average_disagreement_score),
             colour = Reassessed)) + 
  geom_vline(xintercept = threshold, linetype = 2) +
  geom_point() +
  theme_classic_2(base_size = 8) +
  ylab("METRICS item/condition") +
  scale_color_manual(values = c(No = "black", Yes = "red3")) +
  xlab("Average agreement\n(negative = disagreement, positive = agreement)")

aggregated_disagreement_df
```

```{r disagreement}
prompt_improvement_items <- aggregated_disagreement_df$key[
  aggregated_disagreement_df$Reassessed == "Yes"]

cat(sprintf(
  "Items where prompting will be improved: %s", 
  paste(prompt_improvement_items, collapse = ", ")))
```

### Item23

*Use of uni-parametric imaging or proof of its inferiority*

As in the table above, we can see that the average disagreement score (the average of the scoring system noted above) is more severe for Item23 - whether uni-parametric imaging was used or proved to be inferior to multi-parametric imaging. Focusing on this, we take five papers at random where all reader groups said "yes" while the LLM said "no". We bring up the reasons provided by the LLM to further clarify this, and inspect each paper individually to better understand what the reason was. By and large, two dominant reasons appear to arise:

1.  The LLM mistakenly assumes that using multiple image transformations during radiomic feature extraction counts as "multi-parametric imaging"
2.  The LLM mistakenly interprets the paper and assumes that there are other categories apart from "uni-parametric imaging" and "multi-parametric imaging"

#### Potential improvement

Including the following in the item helper: "Uni-parametric imaging implies the acquisition of a single image for each study, as opposed to multi-parametric imaging, which involves the acquisition of multiple images for each study. If a study applies a radiomics-based method to multi-parametric imaging data, it should also apply that method to uni-parametric imaging data and compare both. If a single imaging modality was used, the answer should be "yes". Examples of uni-parametric imaging include a single MRI sequence rather than multiple, or a single phase in a dynamic contrast-enhanced scan."

```{r disagreement-23}
set.seed(42)

get_examples_of_disagreement <- function(item, n_examples = 5) {
  item_subset <- disagreement_df %>%
    subset(key == item) %>%
    arrange(n_disagreements) 
  
  if (nrow(item_subset) < n_examples) {
    n_examples <- nrow(item_subset)
  }
  relevant_titles <- item_subset$title[1:n_examples]
  
  output <- list()
  for (tt in relevant_titles) {
    reason <- fromJSON(file = llm_rating_files[[gsub("/", "_", tt)]])[[item]]
    reason_number <- match_class[reason$rating]
    human_raters <- subset(item_subset, title == tt)
    human_raters <- unlist(
      human_raters[, Filter(function(x) grepl("training", x), colnames(human_raters))]) %>%
      paste(collapse = ", ")
    output[[length(output) + 1]] <- tibble(
      Title = tt,
      `Human ratings` = human_raters,
      `LLM rating` = reason_number,
      `LLM reason` = unlist(reason[[2]])
    )
  }
  do.call(what = rbind, output)
}

get_examples_of_disagreement(prompt_improvement_items[1])
```

### Item7

*The interval between imaging used and reference standard*

Disagreements for this item are somewhat interesting --- upon closer inspection of any of the papers mentioned below, there are no errors from the LLM. However, the LLM considers these as "yes", but most radiologist groups considered this to be the opposite. We found that the interval between imaging and reference standard is acceptable (except for "Application of a comprehensive model based on CT radiomics and clinical features for postoperative recurrence risk prediction in non-small cell lung cancer", where this interval is not mentioned) and follows the helping criteria [^3]; however, there may be slight aspects of METRICS which we are not fully understanding.

[^3]: "Whether the time interval between the diagnostic imaging exams (used as an input for the radiomics analysis) and the outcome measure/reference standard acquisition is appropriate to validate the presence or absence of target conditions of the radiomics analysis at the moment of the diagnostic imaging exams."

#### Potential improvement

Inclusion of the following in the item helper: "If there is no mention of when the reference standard (i.e. the prediction target) was acquired relative to the diagnostic imaging exams, the answer should be"no"."

```{r disagreement-7}
prompt_improvement_items[2]
get_examples_of_disagreement(prompt_improvement_items[2])
```

### Item16

*Appropriateness of dimensionality compared to data size*

This is something which was somewhat expected --- the METRICS question referring to appropriate data dimensionality does not define this in exact terms. Consequently, Item16 is typically hard to understand @Akinci-D-Antonoli2025-ep. However, a consistent analysis of the LLM outputs showed that it tended to answer "n/a" when there was no available information, when this should only be the case when Condition 4 is "no".

#### Potential improvement

Inclusion of the following in the item helper: "Only when Condition 4 is negative should this item be "n/a". If the information is not specified, use the best available knowledge to provide a "yes" or "no" answer."

```{r disagreement-16}
prompt_improvement_items[3]

get_examples_of_disagreement(prompt_improvement_items[3])
```

### Item10

*Test set segmentation masks produced by a single reader or automated tool*

Here, the LLM considers a senior radiologist reviewing segmentations to be standard clinical practice, and given that the helper note[^4] does not specifically address what "clinical practice" is or isn't, the LLM (very wrongfully) assumes that segmentation reviews are a standard part of it.

[^4]: "Whether final segmentation in the test set is produced by a single reader (manually or with a semi-automated tool) or an entirely automated tool, to better reflect clinical practice."

#### Potential improvement

Inclusion of the following in the item helper: "A radiologist reviewing the results of the automated segmentation is not considered clinical practice. Additionally, both training and testing segmentation masks should be produced by a single reader or automated tool."

```{r disagreement-10}
prompt_improvement_items[4]
get_examples_of_disagreement(prompt_improvement_items[4])
```

### Item13

*Transparent reporting of feature extraction parameters, otherwise providing a default configuration statement*

Here, the disagreements stem from the LLM assuming that adequate reporting of feature extraction parameters can be simply the explanation of which packages were used. However, as radiomics researchers know, there are multiple possible configurations for these feature packages. Indeed, oftentimes the list of possible features *groups* may not be sufficient as multiple features can be activated or deactivated inside of each feature group.

#### Potential improvement

Inclusion of the following in the item helper: "Mention of features by name or feature groups is not sufficient. Explicitly check whether the scientific article provides a list of the radiomic features which are being extracted or if it mentions that the default set of parameters was used."

```{r disagreement-13}
prompt_improvement_items[5]
get_examples_of_disagreement(prompt_improvement_items[5])
```

### Item24

*Comparison with a non-radiomic approach or proof of added clinical value*

Here, the main point is for raters to determine whether the radiomics-based method can improve on a standard approach which is more likely to be already implemented [^5]. 

[^5]: "Whether a non-radiomic method that is representative of the clinical practice is included in the analysis for comparison purposes. Non-radiomic methods might include semantic features, RADS or RECIST scoring, and simple volume or size evaluations. If no non-radiomics method is available, proof of improved diagnostic accuracy (e.g., improved performance of a radiologist assisted by the model’s output) or patient outcome (e.g., decision analysis, overall survival) should be provided. In any case, the comparison should be done with an appropriate statistical method to evaluate the added practical and clinical value of the model (e.g., DeLong’s test for AUC comparison, decision curve analysis for net benefit comparison, Net Reclassification Index). Furthermore, in case of multiple comparisons, multiple testing correction methods (e.g., Bonferroni) should be considered in order to reduce the false discovery rate provided that the statistical comparison is done with a frequentist approach (rather than Bayesian)."

Large parts of the disagreement stem from the LLM considering that this is not the case whereas human raters believe this to be the case. As observed for examples 1 through four, we note that the LLM consistently considers models with _radiological_ features as models which make use of _radiomics_ features. This implies a deeply interesting question: is a radiological feature a radiomic feature? Depending on the source, radiomics can be as simple as quantitative features extracted from radiological images: this would place radiological features extracted by clinicians in the same field as radiomic features. However, we believe that the model operates under the definition that "quantitative radiological features" are synonyms with "radiomic features".

The 4 examples where the LLM provides a negative evaluation to an overall positive evaluation by radioligsts are:

1. "Development of a combined radiomics and CT feature-based model for differentiating malignant from benign subcentimeter solid pulmonary nodules" compares the radiomics approach with a clinical/radiological feature model (this clinical/radiologicl feature model included both features extracted manually from CT images by radiologists and sex, age, smoking history, and interval time between the CT scans and surgery)
2. "Preoperative prediction of early recurrence in resectable pancreatic cancer integrating clinical, radiologic, and CT radiomics features" compares the radiomics-based approach with a clinical-radiological model
3. "The value of LI-RADS and radiomic features from MRI for predicting microvascular invasion in hepatocellular carcinoma within 5 cm" compares LI-RADS (a standard radiological assessment for liver imaging) with radiomic features
4. "Comparative analysis of radiomics and deep-learning algorithms for survival prediction in hepatocellular carcinoma" compares their radiomics/CNN-based approach with a clinical model; however and interestingly, the LLM recognises this but does not consider this as it does not relate to radiological characteristics.

Finally, contrary to most human rater/rater groups, the LLM considers that "CT radiomics to differentiate between Wilms tumor and clear cell sarcoma of the kidney in children" did perform a comparison with a non-radiomics approach, but this comparison is not direct in the publication. Indeed, in the publication, authors rely instead on a statistical test between groups in the training data, showing that readily available radiological are not sufficiently discriminative.

#### Potential improvement

Inclusion of the following in the item helper: "Radiological features (features extracted by radiologists without the assistance of computer or software tools) are not radiomic features. Additionally, models incorporating exclusively clinical features without any radiological information can be considered comparisons with representative clinical standards."

```{r disagreement-24}
prompt_improvement_items[6]
get_examples_of_disagreement(prompt_improvement_items[6])
```

### Item8

*Transparent description of segmentation methodology*

This item seeks to elucidate whether there is a sufficiently good description of the segmentation methodology [^6], and the LLM consistently disagrees when the majority voted "no". Here, we find that the reason stems from there being relatively distinct definitions of what constitutes a good description of such a methodology: indeed, there is always at least one rater group which considers this to be "yes".  For "CNN-based multi-modal radiomics analysis of pseudo-CT utilization in MRI-only brain stereotactic radiotherapy: a feasibility study", 3 groups considered that there was no segmentation; this is incorrect as the study features CTV/OAR deliniation, despite this being only for the validation. Whether or not this counts as segmentation in the context of radiomics is hard to understand from current guidelines. For the remaining cases (where LLM considered "no" and rater groups considered "yes"):

[^6]: "Whether the rules or the method of the segmentation are defined (e.g., margin shrinkage, peri-tumoral sampling, details of segmentation regardless of whether manual, semi-automated or automated methods are used). In the case of DL-based radiomics, the segmentation can refer to the rough delineation with bounding boxes or cropping the image around a region of interest."

1. "Prediction of venous trans-stenotic pressure gradient using shape features derived from magnetic resonance venography in idiopathic intracranial hypertension patients" describes the segmentation only as having been performed by "neuroradiologists", which does not provide sufficient details.
2. "Comparative analysis of radiomics and deep-learning algorithms for survival prediction in hepatocellular carcinoma" describes the segmentation protocol as "[a] resident with two years of experience in liver imaging contoured the liver parenchyma and the largest HCC-lesion in both contrast phases using the open-source software 3D Slicer. All segmentations were verified by the same resident 4 weeks later"; in our opinion this stands as a sufficiently transparent description of segmentation, so without any further details it is in our opinion hard to understand why rater groups (but not all) did not consider this to be the case.
3. "Deep learning-radiomics integrated noninvasive detection of epidermal growth factor receptor mutations in non-small cell lung cancer patients" provide a short note that an automatic segmentation was used and provide a link to a paper describing this. Given that the description is not available in this paper, we agree with the rater group majority ("no" is the correct evaluation).
4. "Prediction of SBRT response in liver cancer by combining original and delta cone-beam CT radiomics: a pilot study" features a fairly complete description of the delineation method but does not mention the experience levels of each radiologist; this can be the reason why the majority considered this to be insufficient, whereas the LLM considers this to be sufficient.

#### Potential improvement

Inclusion of the following in the item helper: "The segmentation methodology should be detailed, including the years of experience of each radiologist and the utilized software program."

```{r disagreement-8}
prompt_improvement_items[7]
get_examples_of_disagreement(prompt_improvement_items[7])
```

### Item19

*Handling of confounding factors*

Here, the disagreements are, as far as we can tell, stemming from a difficulty we also face when considering questions such as this. The helper note for this item [^7] is somewhat hard to consider in strict terms. From our understanding and also based on the ratings provided by the different rater groups, we assume that the confounders should be associated with acquisition parameters or with patient characteristics. 

[^7]: "Whether potential confounding factors were analyzed, identified if present, and removed if necessary (e.g., if it has a strong influence on generalizability). These may include different distributions of patient characteristics (e.g., gender, lesion stage or grade) across sites or scanners."

* Cases where the LLM considers a "yes" for Item19 while the majority of rater groups consider a "no" stem from the LLM either considering that there was an analysis of confounders. For example, in "Prediction of SBRT response in liver cancer by combining original and delta cone-beam CT radiomics: a pilot study", the authors do not control for confounders. However, they analyse how pre-SBRT therapy is associated with response typ, which the LLM considers as an analysis of confounders.
* Cases where the LLM considers a "no" for Item19 while the majority of rater groups consider a "yes" stem from ambiguity in what controlling for confounders can be. For instance, in "The value of LI-RADS and radiomic features from MRI for predicting microvascular invasion in hepatocellular carcinoma within 5 cm", the authors include potential confounders into the model (mostly clinical and demographic variables), leading raters to largely consider a "yes" for Item19. However, no post-hoc analysis is performed, nor is an explicit analysis of the impact of confounders, leading the LLM to consider a "no" for Item19.

#### Potential improvement

Inclusion of the following in the item helper: "Be sure to focus on confounders stemming from the acquisition process, such as scanner type, manufacturer, or scanner parameters, or from the individual patient characteristics, such as age, weight, or comorbidities."

```{r disagreement-19}
prompt_improvement_items[8]
get_examples_of_disagreement(prompt_improvement_items[8])
```

### Overall table

```{r overall-table-disagreements}
prompt_improvement_items %>%
  lapply(function(x) {
    get_examples_of_disagreement(x) %>%
      mutate(Item = x)
  }) %>%
  do.call(what = rbind) %>%
  mutate(`Human ratings` = gsub("1", "Yes", `Human ratings`),
         `Human ratings` = gsub("0", "No", `Human ratings`),
         `LLM rating` = gsub("1", "Yes", `LLM rating`),
         `LLM rating` = gsub("0", "No", `LLM rating`)) %>%
  kable
```

### Reanalysis with improved prompt

Upon the analysis above, we use the same LLM with the data and the improved descriptions injected into the prompt. While not ideal, this will allow us to see --- to a very limited extent as we are lacking an external validation --- whether this can lead to an improvement. As highlighted in @fig-cohen-raters-llm-improved it is possible to see no improvements, hinting that these sorts of analysis may benefit from more refined prompt improvement methods. @fig-cohen-raters-conditions-llm-improved-subset, specific for the eight items above, shows that there are no systematic improvements. Additionally, an interesting effect non-linear effect here is that while the condition prompts where not altered, there was a notable improvement to the agreement between LLM and human rater groups for conditions.

```{r list-improved-prompt-results}
improved_llm_rating_files <- list.files(
  path = "../ratings_improved/gemini/akinci-dantonoli", 
  pattern = "*json", full.names = T)
names(improved_llm_rating_files) <- lapply(
  improved_llm_rating_files, function(x) gsub(".json", "", str_split(x, "/")[[1]][4])
)
```

```{r read-improved-prompt-results}
all_improved_llm_ratings <- list()
for (f in improved_llm_rating_files) {
  title <- gsub(".json", "", str_split(f, "/")[[1]][5])
  json_data <- fromJSON(file = f)
  json_data <- json_data[!(names(json_data) %in% skip_columns)]
  json_df <- data.frame(
    key = names(json_data), 
    llm_changes = unlist(lapply(json_data, function(x) x$rating)), title = title)
  all_improved_llm_ratings[[length(all_improved_llm_ratings) + 1]] <- json_df
}

improved_llm_df <- do.call(rbind, all_improved_llm_ratings) %>%
  mutate(llm_changes = match_class[llm_changes]) %>%
  mutate(title = gsub("_", "/", title))

# redefine rater vectors to include LLMs
all_raters <- c("no training_1","no training_2","no training_3",
                "with training_1","with training_2","with training_3",
                "llm", "llm_changes")
all_rater_labels <- c(
  "Not\ntrained (1)", "Not\ntrained (2)", "Not\ntrained (3)",
  "Trained (1)", "Trained (2)", "Trained (3)",
  "LLM", "LLM + changes"
)

fuller_df <- merge(
  full_df,
  improved_llm_df, 
  by = c("key", "title"))
```

```{r calculate-cohens-improved-ada2025}
#| warning: false
rater_combinations <- combn(all_raters, 2)

all_cohens <- list()
for (idx in 1:ncol(rater_combinations)) {
  raters <- rater_combinations[, idx]
  sub_df <- subset(fuller_df, !grepl("Condition", key))[c(raters[1], raters[2])] %>%
    na.omit() %>%
    as.matrix()
  if ((nrow(sub_df) > 10) & (var(sub_df[,1]) > 0) & (var(sub_df[,2]) > 0)) { # guarantee that at least 10 articles were graded
    kk <- as.data.frame(cohen.kappa(sub_df)$confid)
    curr_df <- kk
    curr_df$rater1 <- raters[1]
    curr_df$rater2 <- raters[2]
    all_cohens[[length(all_cohens) + 1]] <- curr_df
  }
}

all_cohens_items <- list()
for (k in unique(fuller_df$key)) {
  for (idx in 1:ncol(rater_combinations)) {
    raters <- rater_combinations[, idx]
    sub_df <- subset(fuller_df, key == k)[c(raters[1], raters[2])] %>%
      na.omit() %>%
      as.matrix()
    if ((nrow(sub_df) > 10) & (var(sub_df[,1]) > 0) & (var(sub_df[,2]) > 0)) { # guarantee that at least 10 articles were graded
      kk <- as.data.frame(cohen.kappa(sub_df)$confid)
      curr_df <- kk
      curr_df$rater1 <- raters[1]
      curr_df$rater2 <- raters[2]
      curr_df$item <- k
      curr_df$n <- nrow(sub_df)
      all_cohens_items[[length(all_cohens_items) + 1]] <- curr_df 
    }
  }
}
```

```{r calculate-cohens-improved-ada2025-subset}
disagreement_df <- fuller_df %>%
  rowwise() %>%
  mutate(
    n_disagreements = c(
      score_agreement(`no training_1`, llm),
      score_agreement(`no training_2`, llm),
      score_agreement(`no training_3`, llm),
      score_agreement(`with training_1`,llm),
      score_agreement(`with training_2`, llm), 
      score_agreement(`with training_3`, llm)
    ) %>%
      Filter(f = function(x) !is.na(x)) %>%
      sum_na_if_empty,
    n_disagreements_changes = c(
      score_agreement(`no training_1`, llm_changes),
      score_agreement(`no training_2`, llm_changes),
      score_agreement(`no training_3`, llm_changes),
      score_agreement(`with training_1`,llm_changes),
      score_agreement(`with training_2`, llm_changes), 
      score_agreement(`with training_3`, llm_changes)
    ) %>%
      Filter(f = function(x) !is.na(x)) %>%
      sum_na_if_empty
  ) %>%
  arrange(n_disagreements)

aggregated_disagreement_df <- disagreement_df %>%
  group_by(key) %>%
  summarise(average_disagreement_score = mean(n_disagreements, na.rm = T),
            average_disagreement_changes_score = mean(n_disagreements_changes, 
                                                      na.rm = T)) %>%
  arrange(average_disagreement_score) %>%
  mutate(Reassessed = ifelse(average_disagreement_score < threshold, "Yes", "No"))

aggregated_disagreement_df %>%
  ggplot(aes(x = average_disagreement_score,
             y = average_disagreement_changes_score,
             label = key,
             colour = Reassessed)) +
  geom_point() +
  geom_label() +
  geom_abline(slope = 1)
```

```{r calculate-cohens-improved-aggregate-ada2025}
#| warning: false
cohens_overall <- do.call(rbind, all_cohens) %>% 
  mutate(rater1 = factor(rater1, all_raters, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_raters, all_rater_labels))

cohens_average_items <- do.call(rbind, all_cohens_items) %>% 
  subset(grepl("Item", item)) %>%
  mutate(rater1 = factor(rater1, all_raters, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_raters, all_rater_labels)) %>%
  group_by(rater1, rater2) %>%
  na.omit() %>%
  summarise(se = sd(estimate) / sqrt(length(estimate)),
            estimate = mean(estimate), 
            lower = max(estimate - abs(qnorm(0.025)) * se, -1),
            upper = min(estimate + abs(qnorm(0.025)) * se, 1),
            .groups = "drop")

cohens_average_conditions <- do.call(rbind, all_cohens_items) %>% 
  subset(grepl("Condition", item)) %>%
  mutate(rater1 = factor(rater1, all_raters, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_raters, all_rater_labels)) %>%
  group_by(rater1, rater2) %>%
  na.omit() %>%
  summarise(se = sd(estimate) / sqrt(length(estimate)),
            estimate = mean(estimate), 
            lower = max(estimate - abs(qnorm(0.025)) * se, 0),
            upper = min(estimate + abs(qnorm(0.025)) * se, 1),
            .groups = "drop")

inside_legend_theme <- theme(
  legend.position = c(0.99, 0.05),
  legend.justification = c(1.0, 0.0),
  legend.direction = "horizontal",
  legend.title = element_text(hjust = 1.0))

colourbar_guides <- guides(fill = guide_colorbar(title.position = "top",
                                                 frame.colour = "black",
                                                 ticks.colour = "black",
                                                 ticks.size = 0.5,
                                                 frame.size = 0.5))

cohens_overall_plot <- make_heatmap(cohens_overall) +
  scale_fill_distiller(palette = 3, name = "Cohen's Kappa") +
  inside_legend_theme + 
  colourbar_guides

cohens_average_items_plot <- make_heatmap(cohens_average_items) +
  scale_fill_distiller(palette = 3, name = "Average Cohen's\nKappa across items") +
  inside_legend_theme + 
  colourbar_guides

cohens_average_conditions_plot <- make_heatmap(cohens_average_conditions) +
  scale_fill_distiller(palette = 3, name = "Average Cohen's\nKappa across conditions") +
  inside_legend_theme + 
  colourbar_guides

cohens_overall %>% 
  mutate(irg = case_when(
    grepl("LLM", rater1) & grepl("LLM", rater2) ~ "LLM-LLM",
    grepl("LLM", rater1) & !grepl("LLM", rater2) ~ "LLM-Human",
    grepl("LLM", rater2) & !grepl("LLM", rater1) ~ "LLM-Human",
    .default = "Human-Human"
  )) %>%
  subset(irg != "LLM-LLM") %>%
  summarise(average_llm_human = mean(estimate[irg == "LLM-Human"]),
            average_human_human = mean(estimate[irg == "Human-Human"]),
            p.val = wilcox.test(estimate ~ irg)$p.val)
```

```{r fig-cohen-raters-llm-improved}
#| fig.height: 8.5
#| fig.width: 6.0
#| label: fig-cohen-raters-llm-improved
#| fig-cap: Cohen's Kappa for Akinci D'Antonoli and others (2025) with LLM.
(cohens_overall_plot / 
  cohens_average_items_plot /
  cohens_average_conditions_plot) +
  plot_annotation(tag_levels = c("A"))
```

```{r fig-cohen-raters-conditions-llm-improved-subset}
#| fig.height: 3.0
#| fig.width: 6.0
#| label: fig-cohen-raters-conditions-llm-improved-subset
#| fig-cap: Average of Cohen's Kappa calculated between different groups of raters and the LLM for each specific condition and only for items 23, 7, 16, 10, 13, 24, 8 and 19.
do.call(rbind, all_cohens_items) %>% 
  subset(item %in% prompt_improvement_items) %>%
  mutate(rater1 = factor(rater1, all_raters, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_raters, all_rater_labels)) %>%
  group_by(rater1, rater2) %>%
  na.omit() %>%
  summarise(se = sd(estimate) / sqrt(length(estimate)),
            estimate = mean(estimate), 
            lower = max(estimate - abs(qnorm(0.025)) * se, -1),
            upper = min(estimate + abs(qnorm(0.025)) * se, 1),
            .groups = "drop") %>%
  make_heatmap() +
  scale_fill_distiller(palette = 3, 
                       name = "Average Cohen's Kappa\nacross items 23, 7, 16,\n10, 13, 24, 8 and 19") +
  inside_legend_theme + 
  colourbar_guides
```

## Analysing whether radiomic scores are similar between different rater groups and LLMs

Finally, we consider the final quantity of METRICS (i.e. a single score between 0 and 1 which quantifies how much a manuscript follows this rating). We can see that LLM-based assessments tend to over-estimate scores when relative to some readers (@fig-corr-matrix). However, when measuring correlations between all human rater groups and between LLMs and all human rater groups, these are relatively similar, hinting that there is similar variability between other reader groups (@fig-corr-matrix). Mean absolute errors are also not particularly higher than others (@fig-corr-matrix).

```{r calculate-metrics-scores-ada2025}
#| warning: false
SCORES <- c(
    Item1 = 0.0368,
    Item2 = 0.0735,
    Item3 = 0.0919,
    Item4 = 0.0438,
    Item5 = 0.0292,
    Item6 = 0.0438,
    Item7 = 0.0292,
    Item8 = 0.0337,
    Item9 = 0.0225,
    Item10 = 0.0112,
    Item11 = 0.0622,
    Item12 = 0.0311,
    Item13 = 0.0415,
    Item14 = 0.0200,
    Item15 = 0.0200,
    Item16 = 0.0300,
    Item17 = 0.0200,
    Item18 = 0.0599,
    Item19 = 0.0300,
    Item20 = 0.0352,
    Item21 = 0.0234,
    Item22 = 0.0176,
    Item23 = 0.0117,
    Item24 = 0.0293,
    Item25 = 0.0176,
    Item26 = 0.0375,
    Item27 = 0.0749,
    Item28 = 0.0075,
    Item29 = 0.0075,
    Item30 = 0.0075
)

scores_df_long <- fuller_df %>%
  gather(key = "group", value = "value", 
         `no training_1`, `no training_2`, `no training_3`,
         `with training_1`, `with training_2`, `with training_3`,
         llm, llm_changes) %>%
  subset(grepl("Item", key)) %>%
  mutate(multiplier = ifelse(is.na(value), NA, SCORES[key]),
         score = multiplier * value) %>%
  group_by(group, title) %>%
  summarise(score = sum(score, na.rm = T) / sum(multiplier, na.rm = T),
            .groups = "drop") %>%
  mutate(group = factor(group, all_raters, all_rater_labels))

scores_df <- scores_df_long %>%
  spread(key = "group", value = score) %>%
  gather(key = "group", value = "value", 
         `Not\ntrained (1)`, `Not\ntrained (2)`, `Not\ntrained (3)`,
         `Trained (1)`, `Trained (2)`, `Trained (3)`) 

scores_df_wide <- scores_df_long %>% 
  spread(key = "group", value = "score")

all_corr <- list()
rater_combinations_pretty <- combn(all_rater_labels, 2)
for (idx in 1:ncol(rater_combinations_pretty)) {
  raters <- rater_combinations_pretty[, idx]
  sub_df <- scores_df_wide[,c(raters[1], raters[2])] %>%
    na.omit() %>%
    as.matrix()
  corr_output <- cor.test(sub_df[,1], sub_df[,2])
  mae <- mean(abs(sub_df[,1] - sub_df[,2]))
  mae_half_interval = sd(abs(sub_df[,1] - sub_df[,2])) / sqrt(nrow(sub_df)) * qnorm(1 - 0.025)
  curr_df <- data.frame(estimate = corr_output$estimate,
                        lower = corr_output$conf.int[1],
                        upper = corr_output$conf.int[2],
                        mae = mae,
                        mae_lower = mae - mae_half_interval,
                        mae_upper = mae + mae_half_interval
                        )
  curr_df$rater1 <- raters[1]
  curr_df$rater2 <- raters[2]
  all_corr[[length(all_corr) + 1]] <- curr_df
}
```

```{r fig-corr}
#| fig.height: 2.7
#| fig.width: 4
#| warning: false
#| label: fig-corr
#| fig-cap: Association between LLM with and without changes and human rater groups.

scores_df %>%
  ggplot(aes(colour = gsub("\n", " ", group))) + 
  geom_abline(slope = 1) +
  geom_point(aes(x = value, y = LLM, shape = "LLM")) +
  geom_point(aes(x = value, y = `LLM + changes`, shape = "LLM + changes")) +
  geom_segment(aes(x = value, xend = value, 
                   y = `LLM`, yend = `LLM + changes`)) +
  theme_classic_2(base_size = 8) + 
  scale_shape_manual(values = c(1, 16), name = "LLM") +
  coord_cartesian(xlim = c(0, 1), ylim = c(0, 1)) +
  scale_colour_brewer(palette = "Set2", name = "Group") +
  theme(legend.key.size = unit(0, "lines")) + 
  ylab("METRICS score from LLM rater") +
  xlab("METRICS score from human rater")
```

```{r fig-corr-matrix-block}
#| fig.height: 6
#| fig.width: 5.5
#| warning: false
#| label: fig-corr-matrix
#| fig-cap: Correlation values (Pearson's R; higher is better) and mean absolute erorr (lower is better) between LLM with and without changes and human rater groups.
pearsons_r_plot <- do.call(rbind, all_corr) %>% 
  mutate(rater1 = factor(rater1, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_rater_labels)) %>%
  make_heatmap() +
  scale_fill_distiller(palette = 3, name = "Pearson's R") +
  inside_legend_theme + 
  colourbar_guides

mae_plot <- do.call(rbind, all_corr) %>% 
  mutate(rater1 = factor(rater1, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_rater_labels)) %>%
  mutate(estimate = mae,
         lower = mae_lower,
         upper = mae_upper) %>%
  make_heatmap() +
  scale_fill_distiller(palette = 3, name = "Mean absolute error") +
  inside_legend_theme + 
  colourbar_guides

do.call(rbind, all_corr) %>% 
  mutate(rater1 = factor(rater1, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_rater_labels)) %>% 
  mutate(irg = case_when(
    grepl("LLM", rater1) & grepl("LLM", rater2) ~ "LLM-LLM",
    grepl("LLM", rater1) & !grepl("LLM", rater2) ~ "LLM-Human",
    grepl("LLM", rater2) & !grepl("LLM", rater1) ~ "LLM-Human",
    .default = "Human-Human"
  )) %>%
  subset(irg != "LLM-LLM") %>%
  summarise(average_llm_human = mean(estimate[irg == "LLM-Human"]),
            average_human_human = mean(estimate[irg == "Human-Human"]),
            p.val = wilcox.test(estimate ~ irg)$p.val)

(pearsons_r_plot / mae_plot) +
  plot_annotation(tag_levels = c("A"))
```

However, and dispite some similarities, we show that there are more statistically significant differences when performing human-LLM comparisons than when comparing human-human comparisons. We do this through a post-hoc analysis with a Tukey HSD following a statistically significant ANOVA.

```{r lme-analysis-ada2025}
library(lme4)
library(emmeans)
library(multcomp)

select <- dplyr::select

model <- lmer(score ~ group + (1|title), data = scores_df_long)

# Get EMMs and do Tukey HSD
emm <- emmeans(model, ~ group)
pair_comparisons <- pairs(emm, adjust = "tukey")

tukey.hsd <- pair_comparisons %>%
  as.data.frame

tukey_df <- tukey.hsd %>%
  mutate(comparisons = case_when(
    !grepl("rained", contrast) ~ "LLM-LLM",
    !grepl("LLM", contrast) ~ "Human-Human",
    .default = "Human-LLM")) %>%
  select(-df)

tukey_df_clean <- tukey_df %>%
  mutate(value = sprintf("%.3f (%.3f)", estimate, SE),
         p.value = ifelse(p.value < 0.0001, "<0.0001", 
                          sprintf("%.4f", p.value))) %>%
  select(comparisons, contrast, value, p.value) %>%
  mutate(contrast = gsub("-", " vs. ", contrast))

tukey_df_clean %>%
  arrange(comparisons, contrast) %>% 
  kable(format = "html")

tukey_df %>%
  group_by(comparisons) %>%
  summarise(
    `Fraction of statistically significant Tukey HSD` = sum(p.value < 0.05) / length(estimate),
    `Number of stat. sig. comparisons` = sum(p.value < 0.05),
    `Total number of comparisons` = length(estimate),
    `Average absolute difference` = mean(abs(estimate[p.value < 0.05])))
```

## Validation of LLM observations above with *Kocak et. al (2025)*

Around the time we were developing this, Kocak and others published "Radiomics for differentiating radiation-induced brain injury from recurrence in gliomas: systematic review, meta-analysis, and methodological quality evaluation using METRICS and RQS" in European Radiology @Kocak2025-uj. Here, three different raters (identified as raters 1-3) were tasked with classifying 27 papers using METRICS. Here, we provide LLM-based METRICS scores for only 22 as the remaining 5 were behind a paywall. It should be noted that the first author (and one of the raters as far we understand it) in *Kocak et al. (2025)* was also a co-author in @Akinci-D-Antonoli2025-ep and the first author of the original METRICS definition paper @Kocak2024-wk.

As visible below across @fig-cohen-kocak, the inter-rater agreements are relatively stable between readers and between readers and LLMs. An interesting outcome of this is that the LLM + changes do not lead to such stark improvements as before, hinting that dataset-specific prompt tuning may not be the best way of improving scores across multiple publications.

```{r read-k2025}
df_original <- read_csv(
  "../data/kocak/inter-rater.csv", col_types = c(rep("c", 40), "i", "i")) 
df <- df_original %>% 
  gather(key = "key", value = "value", 
         -rater, -dois, -dois_clean, -title) %>% 
  subset(key != "Total METRICS score:") %>%
  subset(!grepl("Quality", key)) %>%
  mutate(key = gsub("#", "", key)) %>%
  mutate(value = match_class[value]) %>%
  mutate(rater = sprintf("Rater%s", rater))

all_dfs$kocak <- df

df_original
```

```{r load-files-k2025}
llm_rating_files <- list.files(
  path = "../ratings/gemini/kocak", 
  pattern = "*json", 
  full.names = T)
names(llm_rating_files) <- lapply(
  llm_rating_files, function(x) gsub(".json", "", str_split(x, "/")[[1]][5])
)

all_llm_ratings <- list()
for (f in llm_rating_files) {
  title <- gsub(".json", "", str_split(f, "/")[[1]][5])
  json_data <- fromJSON(file = f)
  json_data <- json_data[!(names(json_data) %in% skip_columns)]
  json_df <- data.frame(
    key = names(json_data), 
    llm = unlist(lapply(json_data, function(x) x$rating)), title = title)
  all_llm_ratings[[length(all_llm_ratings) + 1]] <- json_df
}

llm_df <- do.call(rbind, all_llm_ratings) %>%
  mutate(llm = match_class[llm]) %>%
  mutate(title = gsub("_", "/", title))

improved_llm_rating_files <- list.files(
  path = "../ratings_improved/gemini/kocak", 
  pattern = "*json", full.names = T)
names(improved_llm_rating_files) <- lapply(
  improved_llm_rating_files, function(x) gsub(".json", "", str_split(x, "/")[[1]][4])
)

all_improved_llm_ratings <- list()
for (f in improved_llm_rating_files) {
  title <- gsub(".json", "", str_split(f, "/")[[1]][5])
  json_data <- fromJSON(file = f)
  json_data <- json_data[!(names(json_data) %in% skip_columns)]
  json_df <- data.frame(
    key = names(json_data), 
    llm_changes = unlist(lapply(json_data, function(x) x$rating)), title = title)
  all_improved_llm_ratings[[length(all_improved_llm_ratings) + 1]] <- json_df
}

improved_llm_df <- do.call(rbind, all_improved_llm_ratings) %>%
  mutate(llm_changes = match_class[llm_changes]) %>%
  mutate(title = gsub("_", "/", title))

full_df <- merge(
  pivot_wider(df, values_from = value, names_from = c("rater")), 
  llm_df, 
  by = c("key", "title"))

fuller_df <- merge(
  full_df,
  improved_llm_df, 
  by = c("key", "title"))
```

```{r calculate-cohens-k2025}
#| warning: false
all_raters <- c("Rater1", "Rater2", "Rater3",
                "llm", "llm_changes")
all_rater_labels <- c(
  "Rater 1", "Rater 2", "Rater 3",
  "LLM", "LLM + changes"
)

rater_combinations <- combn(all_raters, 2)

all_cohens <- list()
for (idx in 1:ncol(rater_combinations)) {
  raters <- rater_combinations[, idx]
  sub_df <- subset(fuller_df, !grepl("Condition", key))[c(raters[1], raters[2])] %>%
    na.omit() %>%
    as.matrix()
  if ((nrow(sub_df) > 10) & (var(sub_df[,1]) > 0) & (var(sub_df[,2]) > 0)) { # guarantee that at least 10 articles were graded
    kk <- as.data.frame(cohen.kappa(sub_df)$confid)
    curr_df <- kk
    curr_df$rater1 <- raters[1]
    curr_df$rater2 <- raters[2]
    all_cohens[[length(all_cohens) + 1]] <- curr_df
  }
}

all_cohens_items <- list()
for (k in unique(fuller_df$key)) {
  for (idx in 1:ncol(rater_combinations)) {
    raters <- rater_combinations[, idx]
    sub_df <- subset(fuller_df, key == k)[c(raters[1], raters[2])] %>%
      na.omit() %>%
      as.matrix()
    if ((nrow(sub_df) > 10) & (var(sub_df[,1]) > 0) & (var(sub_df[,2]) > 0)) { # guarantee that at least 10 articles were graded
      kk <- as.data.frame(cohen.kappa(sub_df)$confid)
      curr_df <- kk
      curr_df$rater1 <- raters[1]
      curr_df$rater2 <- raters[2]
      curr_df$item <- k
      curr_df$n <- nrow(sub_df)
      all_cohens_items[[length(all_cohens_items) + 1]] <- curr_df 
    }
  }
}
```

```{r calculate-cohens-average-k2025}
#| warning: false
cohens_overall <- do.call(rbind, all_cohens) %>% 
  mutate(rater1 = factor(rater1, all_raters, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_raters, all_rater_labels))

cohens_average_items <- do.call(rbind, all_cohens_items) %>% 
  subset(grepl("Item", item)) %>%
  mutate(rater1 = factor(rater1, all_raters, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_raters, all_rater_labels)) %>%
  group_by(rater1, rater2) %>%
  na.omit() %>%
  summarise(se = sd(estimate) / sqrt(length(estimate)),
            estimate = mean(estimate), 
            lower = max(estimate - abs(qnorm(0.025)) * se, 0),
            upper = min(estimate + abs(qnorm(0.025)) * se, 1),
            .groups = "drop")

cohens_average_conditions <- do.call(rbind, all_cohens_items) %>% 
  subset(grepl("Condition", item)) %>%
  mutate(rater1 = factor(rater1, all_raters, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_raters, all_rater_labels)) %>%
  group_by(rater1, rater2) %>%
  na.omit() %>%
  summarise(se = sd(estimate) / sqrt(length(estimate)),
            estimate = mean(estimate), 
            lower = max(estimate - abs(qnorm(0.025)) * se, 0),
            upper = min(estimate + abs(qnorm(0.025)) * se, 1),
            .groups = "drop")

cohens_overall_plot <- make_heatmap(cohens_overall) +
  scale_fill_distiller(palette = 3, name = "Cohen's Kappa") +
  inside_legend_theme + 
  colourbar_guides

cohens_average_items_plot <- make_heatmap(cohens_average_items) +
  scale_fill_distiller(palette = 3, name = "Average Cohen's\nKappa across items") +
  inside_legend_theme + 
  colourbar_guides

cohens_average_conditions_plot <- make_heatmap(cohens_average_conditions) +
  scale_fill_distiller(palette = 3, name = "Average Cohen's\nKappa across conditions") +
  inside_legend_theme + 
  colourbar_guides

cohens_overall %>% 
  mutate(irg = case_when(
    grepl("LLM", rater1) & grepl("LLM", rater2) ~ "LLM-LLM",
    grepl("LLM", rater1) & !grepl("LLM", rater2) ~ "LLM-Human",
    grepl("LLM", rater2) & !grepl("LLM", rater1) ~ "LLM-Human",
    .default = "Human-Human"
  )) %>%
  subset(irg != "LLM-LLM") %>%
  summarise(average_llm_human = mean(estimate[irg == "LLM-Human"]),
            average_human_human = mean(estimate[irg == "Human-Human"]),
            p.val = wilcox.test(estimate ~ irg)$p.val)
```

```{r fig-cohen-kocak}
#| fig.height: 6.0
#| fig.width: 4.2
#| label: fig-cohen-kocak
#| fig-cap: Cohen's Kappa calculated between different groups of raters and the LLM for _Kocak et al. (2025)_.

(cohens_overall_plot / 
  cohens_average_items_plot /
  cohens_average_conditions_plot) +
  plot_annotation(tag_levels = c("A"))
```

```{r average-cohens-testing}
#| warning: false
scores_df_long <- fuller_df %>%
  gather(key = "group", value = "value", 
         Rater1, Rater2, Rater3,
         llm, llm_changes) %>%
  subset(grepl("Item", key)) %>%
  mutate(multiplier = ifelse(is.na(value), NA, SCORES[key]),
         score = multiplier * value) %>%
  group_by(group, title) %>%
  summarise(score = sum(score, na.rm = T) / sum(multiplier, na.rm = T),
            .groups = "drop") %>%
  mutate(group = factor(group, all_raters, all_rater_labels))

scores_df_wide <- scores_df_long %>% 
  spread(key = "group", value = "score")

all_corr <- list()
rater_combinations_pretty <- combn(all_rater_labels, 2)
for (idx in 1:ncol(rater_combinations_pretty)) {
    raters <- rater_combinations_pretty[, idx]
    sub_df <- scores_df_wide[,c(raters[1], raters[2])] %>%
        na.omit() %>%
        as.matrix()
    corr_output <- cor.test(sub_df[,1], sub_df[,2])
    mae <- mean(abs(sub_df[,1] - sub_df[,2]))
    mae_half_interval = sd(abs(sub_df[,1] - sub_df[,2])) / sqrt(nrow(sub_df)) * qnorm(1 - 0.025)
    curr_df <- data.frame(estimate = corr_output$estimate,
                          lower = corr_output$conf.int[1],
                          upper = corr_output$conf.int[2],
                          mae = mae,
                          mae_lower = mae - mae_half_interval,
                          mae_upper = mae + mae_half_interval
    )
    curr_df$rater1 <- raters[1]
    curr_df$rater2 <- raters[2]
    all_corr[[length(all_corr) + 1]] <- curr_df
}

do.call(rbind, all_corr) %>% 
  mutate(rater1 = factor(rater1, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_rater_labels)) %>% 
  mutate(irg = case_when(
    grepl("LLM", rater1) & grepl("LLM", rater2) ~ "LLM-LLM",
    grepl("LLM", rater1) & !grepl("LLM", rater2) ~ "LLM-Human",
    grepl("LLM", rater2) & !grepl("LLM", rater1) ~ "LLM-Human",
    .default = "Human-Human"
  )) %>%
  subset(irg != "LLM-LLM") %>%
  summarise(average_llm_human = mean(estimate[irg == "LLM-Human"]),
            average_human_human = mean(estimate[irg == "Human-Human"]),
            p.val = wilcox.test(estimate ~ irg)$p.val)
```

```{r lme-k2025}
scores_df_long <- fuller_df %>%
  gather(key = "group", value = "value", 
         Rater1, Rater2, Rater3,
         llm, llm_changes) %>%
  subset(grepl("Item", key)) %>%
  mutate(multiplier = ifelse(is.na(value), NA, SCORES[key]),
         score = multiplier * value) %>%
  group_by(group, title) %>%
  summarise(score = sum(score, na.rm = T) / sum(multiplier, na.rm = T),
            .groups = "drop") %>%
  mutate(group = factor(group, all_raters, all_rater_labels))

model <- lmer(score ~ group + (1|title), data = scores_df_long)

# Get EMMs and do Tukey HSD
emm <- emmeans(model, ~ group)
pair_comparisons <- pairs(emm, adjust = "tukey")

tukey.hsd <- pair_comparisons %>%
  as.data.frame

tukey_df <- tukey.hsd %>%
  mutate(comparisons = case_when(
    !grepl("ater", contrast) ~ "LLM-LLM",
    !grepl("LLM", contrast) ~ "Human-Human",
    .default = "Human-LLM")) %>%
  select(-df)

tukey_df_clean <- tukey_df %>%
  mutate(value = sprintf("%.3f (%.3f)", estimate, SE),
         p.value = ifelse(p.value < 0.0001, "<0.0001", 
                          sprintf("%.4f", p.value))) %>%
  select(comparisons, contrast, value, p.value) %>%
  mutate(contrast = gsub("-", " vs. ", contrast))

tukey_df_clean %>%
  arrange(comparisons, contrast) %>% 
  kable(format = "html")

tukey_df %>%
  group_by(comparisons) %>%
  summarise(
    `Fraction of statistically significant Tukey HSD` = sum(p.value < 0.05) / length(estimate),
    `Number of stat. sig. comparisons` = sum(p.value < 0.05),
    `Total number of comparisons` = length(estimate),
    `Average absolute difference` = mean(abs(estimate[p.value < 0.05])))
```

## Analysis with local LLMs

To better understand how local LLMs can perform this task, we make use of a small panel of local LLMs and LRMs as provided by Ollama and with their default quantizations.

```{r read-local-llms-results}
all_llm_ratings <- list()
for (rating_id in c("ratings", "ratings_improved")) {
  for (model in list.files(sprintf("../%s", rating_id), pattern = "*")) {
    for (file in list.files(sprintf("../%s/%s", rating_id, model), 
                            pattern = "*json", recursive = T,
                            full.names = T)) {
      information <- str_split(file, pattern = "/")[[1]]
      model <- information[3]
      paper_group <- ifelse(information[4] == "kocak", 
                            "Kocak (2025)", "Akinci D'Antonoli (2025)")
      paper <- gsub("\\.json", "", information[5])
      json_data <- fromJSON(file = file)
      time_elapsed <- json_data$metadata$elapsed_time
      if (is.null(json_data$metadata$error)) {
        json_data <- json_data[!(names(json_data) %in% skip_columns)]
        json_df <- data.frame(
          key = names(json_data), 
          value_llm = unlist(lapply(json_data, function(x) x$rating)), 
          title = paper,
          rater_llm = ifelse(rating_id == "ratings", "llm", "llm_changes"),
          set = paper_group, 
          model = model,
          time_elapsed = time_elapsed,
          error = NA)
        all_llm_ratings[[length(all_llm_ratings) + 1]] <- json_df
      } else {
        worked <- F
        try(
          {
            json_data <- fromJSON(paste0(json_data$raw_output, '"}'))
            json_data <- json_data[!(names(json_data) %in% skip_columns)]
            worked <- T
          },
          silent = T
        )
        if (worked == T) {
          json_df <- data.frame(
            key = names(json_data), 
            value_llm = unlist(lapply(json_data, function(x) x$rating)), 
            title = paper,
            rater_llm = ifelse(rating_id == "ratings", "llm", "llm_changes"),
            set = paper_group, 
            model = model,
            time_elapsed = time_elapsed,
            error = "Recoverable")
          
        } else {
          json_df <- data.frame(
            key = NA,
            value_llm = NA,
            title = paper,
            rater_llm = ifelse(rating_id == "ratings", "llm", "llm_changes"),
            set = paper_group,
            model = model,
            time_elapsed = time_elapsed,
            error = "Irrecoverable"
          )
        }
        all_llm_ratings[[length(all_llm_ratings) + 1]] <- json_df
      }
    }
  }
}

all_llm_df <- do.call(rbind, all_llm_ratings) %>%
  mutate(value_llm = match_class[value_llm]) %>%
  mutate(title = gsub("_", "/", title)) %>%
  mutate(key = gsub("_[A-Za-z0-9]+", "", key))
rownames(all_llm_df) <- NULL

all_raters <- c("no training_1","no training_2","no training_3",
                "with training_1","with training_2","with training_3",
                "Rater1", "Rater2", "Rater3",
                "llm", "llm_changes")
all_rater_labels <- c(
  "Not\ntrained (1)", "Not\ntrained (2)", "Not\ntrained (3)",
  "Trained (1)", "Trained (2)", "Trained (3)",
  "Rater 1", "Rater 2", "Rater 3",
  "LLM", "LLM + changes"
)

all_human_raters <- rbind(
  all_dfs$akinci_dantonoli %>%
    mutate(rater = paste(group, exp, sep = "_"), 
           set = "Akinci D'Antonoli (2025)") %>%
    select(rater, title, key, value, set),
  all_dfs$kocak %>%
    mutate(set = "Kocak (2025)") %>%
    select(rater, title, key, value, set)
)

complete_comparisons <- merge(
  all_human_raters, 
  all_llm_df, 
  by = c("title", "key", "set")) 

complete_comparisons
```

```{r local-llms-cohens}
cohens_for_all <- complete_comparisons %>%
  group_by(model, rater, rater_llm, set) %>%
  summarise(
    CK = list(cohen.kappa(data.frame(value, value_llm))),
    .groups = "drop") %>%
  rowwise() %>%
  mutate(lower = CK$confid[1, 1],
         estimate = CK$confid[1, 2],
         upper = CK$confid[1, 3]) %>%
  select(-CK) %>%
  rowwise() %>%
  mutate(model_name = gsub("-[0-9]+b.*", "", model),
         param_count = str_match(model, "[0-9]+(?=b)")[[1]]) %>%
  mutate(
    model_label = ifelse(
      model != "gemini",
      sprintf("%s (%sb)", str_to_title(model_name), param_count),
      "Gemini")) %>%
  mutate(model_label = gsub("Qwq", "QwQ", model_label)) 

times_for_all <- complete_comparisons %>%
  group_by(model, rater_llm) %>%
  summarise(
    average_time = mean(time_elapsed),
    min_time = min(time_elapsed),
    max_time = max(time_elapsed),
    .groups = "drop") %>%
  rowwise() %>%
  rowwise() %>%
  mutate(model_name = gsub("-[0-9]+b.*", "", model),
         param_count = str_match(model, "[0-9]+(?=b)")[[1]]) %>%
  mutate(
    model_label = ifelse(
      model != "gemini",
      sprintf("%s (%sb)", str_to_title(model_name), param_count),
      "Gemini")) %>%
  mutate(model_label = gsub("Qwq", "QwQ", model_label)) 

model_label_order <- str_sort(unique(cohens_for_all$model_label), numeric = T)
model_label_order <- model_label_order[model_label_order != "Gemini"]
model_label_order <- c(model_label_order, "Gemini")

cohens_for_all$model_label <- factor(
  cohens_for_all$model_label,
  model_label_order)

times_for_all$model_label <- factor(
  times_for_all$model_label,
  model_label_order)

cohens_for_all <- cohens_for_all %>%
  mutate(
    model_provider = gsub(
      "[0-9\\.]+", "", 
      gsub("-Small", "", gsub(" \\(.*\\)", "", model_label)))) %>%
  mutate(model_provider = case_when(
    model_provider == "Deepseek-R" ~ "Deepseek",
    grepl("Phi", model_provider) ~ "Microsoft",
    grepl("QwQ|Qwen", model_provider) ~ "Alibaba",
    grepl("Llama", model_provider) ~ "Meta",
    grepl("Gemma|Gemini", model_label) ~ "Google",
    .default = model_provider)) %>%
  mutate(
    model_type = case_when(
      grepl("QwQ|R1|Reasoning", model_label) ~ "Open LRM",
      grepl("Gemini", model_label) ~ "Commercial",
      .default = "Open LLM"
    ) 
  ) %>%
  group_by(rater) %>%
  mutate(ranking = rank(-estimate))

times_for_all <- times_for_all %>%
  mutate(
    model_provider = gsub(
      "[0-9\\.]+", "", 
      gsub("-Small", "", gsub(" \\(.*\\)", "", model_label)))) %>%
  mutate(model_provider = case_when(
    model_provider == "Deepseek-R" ~ "Deepseek",
    grepl("Phi", model_provider) ~ "Microsoft",
    grepl("QwQ|Qwen", model_provider) ~ "Alibaba",
    grepl("Llama", model_provider) ~ "Meta",
    grepl("Gemma|Gemini", model_label) ~ "Google",
    .default = model_provider)) %>%
  mutate(
    model_type = case_when(
      grepl("QwQ|R1|Reasoning", model_label) ~ "Open LRM",
      grepl("Gemini", model_label) ~ "Commercial",
      .default = "Open LLM"
    ) 
  )
```

```{r local-llms-cohens-subset}
cohens_for_all_plot_df <- cohens_for_all %>%
  subset(rater_llm == "llm_changes")

times_for_all_plot_df <- times_for_all %>%
  subset(rater_llm == "llm_changes")
```

```{r calculate-metrics-df}
open_llm_cohens_plot <- cohens_for_all_plot_df %>%
  ggplot(aes(x = estimate, y = model_label, colour = set, shape = model_type)) +
  geom_vline(xintercept = 0, alpha = 0.5) +
  geom_point(position = position_dodge(width = 0.10)) +
  theme_classic_2(base_size = 8) +
  scale_colour_brewer(palette = "Set1", name = "Ratings source") +
  xlab("Cohen's Kappa\n(higher = better)") +
  ylab("") + 
  facet_grid(model_provider ~ ., scales = "free", space = "free") +
  theme(legend.key.size = unit(0.2, "line"),
        strip.text.y = element_text(angle = 0)) +
  scale_shape_manual(values = c(3, 2, 17), name = "Model type") +
  guides(colour = guide_legend(ncol = 1,
                               direction = "vertical"),
         shape = guide_legend(ncol = 1,
                              direction = "vertical"))

open_llm_ranking_plot <- cohens_for_all_plot_df %>%
  group_by(model_label, model_type, model_provider) %>%
  summarise(min_ranking = min(ranking),
            max_ranking = max(ranking),
            ranking = median(ranking)) %>%
  ggplot(aes(x = ranking, y = model_label, 
             xmin = min_ranking, xmax = max_ranking, 
             shape = model_type)) +
  geom_vline(xintercept = 1, alpha = 0.5) +
  geom_point(position = position_dodge(width = 0.10)) +
  geom_errorbarh(position = position_dodge(width = 0.10),
                 height = 0) +
  theme_classic_2(base_size = 8) +
  scale_colour_brewer(palette = "Set1", name = "Ratings source") +
  xlab("Ranking\n(lower = better)") +
  ylab("") + 
  facet_grid(model_provider ~ ., scales = "free", space = "free") +
  theme(legend.key.size = unit(0.2, "line"),
        strip.text.y = element_text(angle = 0)) +
  scale_shape_manual(values = c(3, 2, 17), name = "Model type") +
  guides(shape = guide_legend(ncol = 1, direction = "vertical"))

open_llm_time_plot <- times_for_all_plot_df %>%
  ggplot(aes(x = average_time, 
             y = model_label,
             xmin = min_time,
             xmax = max_time)) +
  geom_bar(stat = "identity", fill = "grey90", colour = "black") +
  geom_errorbarh(height = 0) +
  theme_classic_2(base_size = 8) +
  xlab("Time elapsed (s)") +
  ylab("") + 
  facet_grid(model_provider ~ ., scales = "free", space = "free") +
  theme(legend.key.size = unit(0.2, "line"),
        strip.text.y = element_text(angle = 0)) +
  coord_cartesian(expand = F) +
  scale_x_continuous(breaks = c(0, 150, 300))

times_for_all_plot_df
```

```{r}
#| fig.height: 3
#| fig.width: 5
#| fig.cap: Comparison of Cohen's kappa for LLM and LLM + changes.
#| label: fig-local-comparison
cohens_comparison_llm_prompt <- cohens_for_all %>% 
  select(-lower, -upper, -ranking) %>% 
  spread(key = rater_llm, value = estimate) %>%
  subset(model != "gemini")

wilcox.test(
  cohens_comparison_llm_prompt$llm,
  cohens_comparison_llm_prompt$llm_changes,
  paired = T)

cohens_comparison_llm_prompt %>% 
  ggplot(aes(x = llm, y = llm_changes, colour = set, shape = model_type)) + 
  geom_point() +
  geom_abline(slope = 1) + 
  xlab("LLM") + ylab("LLM + changes") + 
  theme_classic_2(base_size = 8) + 
  theme(legend.key.height = unit(0, "line")) + 
  scale_color_brewer(palette = "Set1", name = "Ratings source") + 
  scale_shape_manual(values = c(2, 17), name = "Model type")
```

### Error codes

```{r fig-local-errors}
#| fig.cap: Number of recoverable or irrecoverable errors stratified by LLM.
#| label: fig-local-errors
open_llm_error_plot_df <- all_llm_df %>%
  select(title, rater_llm, error, model) %>%
  distinct() %>%
  group_by(model, rater_llm, error) %>%
  summarise(N = length(unique(title))) %>%
  group_by(model, rater_llm) %>%
  mutate(total = sum(N)) %>%
  mutate(error = ifelse(is.na(error), "No error", error)) %>%
  mutate(model_name = gsub("-[0-9]+b.*", "", model),
         param_count = str_match(model, "[0-9]+(?=b)")[[1]]) %>%
  mutate(
    model_label = ifelse(
      model != "gemini",
      sprintf("%s (%sb)", str_to_title(model_name), param_count),
      "Gemini")) %>%
  mutate(model_label = gsub("Qwq", "QwQ", model_label)) %>%
  mutate(rater_llm = ifelse(rater_llm == "llm", "LLM", "LLM + changes"))

open_llm_error_plot <- open_llm_error_plot_df %>%
  ggplot(
    aes(x = N / total, y = model_label, fill = error)
  ) + 
  geom_bar(stat = "identity", position = "stack", colour = "black") + 
  xlab("Fraction of cases") + 
  ylab("Model") + 
  facet_wrap(~ rater_llm) + 
  theme_classic_2(base_size = 8) +
  scale_fill_brewer(palette = "Set3",
                    breaks = c("No error", "Recoverable", "Irrecoverable"),
                    name = "Error status") + 
  coord_cartesian(expand = F) + 
  theme(panel.spacing.x = unit(2.0, "line"))

open_llm_error_plot_df %>%
  subset(error != "No error") %>%
  select(-model_label, -param_count, -model_name)
open_llm_error_plot
```

### Overall plot

```{r make-local-overall-figure}
open_llm_ranking_cohens_plot <- 
  (
    open_llm_ranking_plot + 
      theme(strip.text.y = element_blank())
    ) + 
  (
    open_llm_cohens_plot + 
      theme(legend.position = "bottom",
            strip.text.y = element_blank(),
            axis.text.y = element_blank())
    ) + 
  (
    open_llm_time_plot + 
      theme(axis.text.y = element_blank())
  ) +
  plot_layout(guides = 'collect', widths = c(0.8, 1.0, 0.4)) &
  theme(legend.position = "bottom")
```

```{r fig-local-llm}
#| fig.height: 5
#| fig.width: 6.5
#| fig.cap: Comparison of Cohen’s Kappa with different raters/rater groups and elapsed time for open large language models (LLMs), large reasoning models (LRMs) and Gemini (commercial alternative). A – Median ranking (triangles or crosses) and minimum/maximum rankings (horizontal line) across all rater/rater groups. B – Human-LLM Cohen’s kappa for each rater/rater group. C – Average (bars) and minimum/maximum (horizontal lines) elapsed time in seconds for all models. For A and B, shapes relate to whether the model was commercial, an open LLM or an open LRM, while colours (red and blue) refer to the paper group. The size of all open models is specified between parentheses after the name of each model.
#| label: fig-local-llm

#| fig.height: 5
#| fig.width: 6.5
open_llm_ranking_cohens_plot +
  plot_annotation(tag_levels = "A")
```

# Conclusion and where to go from here

This was a somewhat quick analysis prompted by a concrete question --- can we facilitate what are oftentimes cumbersome analyses by getting the assistance of an LLM? The answer to this --- at least what this analysis shows --- is that:

1.  There is a reasonable amount of agreement between LLMs and human rater groups, but there is also a reasonable amount of differences between human rater groups and LLMs. This is not absurd as this system in particular --- METRICS --- relies quite heavily on expert knowledge. The paper that originated this short report highlights, to an extent, the differences in subjective evaluation associated with human raters @Akinci-D-Antonoli2025-ep. While more objective, LLM-based assessments may end up being too different. However, our confirmation using @Kocak2025-uj does show that these findings --- that there is a good number of similarities between human raters and LLMs --- generalise to other datasets.
2.  Improvements are possible from relatively simple analyses which place the human in the loop, suggesting that more data and further optimization can lead to a robust automated METRICS assessment system. While automated prompt engineering could be an alternative, there is a significant amount of work which goes into the annotation of these manuscripts, making these approaches --- which require a good number of examples --- quite tricky to handle when sample sizes are small. Our analysis on @Kocak2025-uj showed that prompt tuning using small sample sizes is likely to result in observable but limited improvements However, both the tuning and its validation used relatively small dataset sizes. Perhaps automatic prompt tuning can improve this.
3. Open LLMs can pave the way for a more privacy-preserving way of using Auto-METRICS. Phi4-Reasoning, in particular, is remarkably powerful at this, out-ranking Gemini Flash 2.0. The only drawback is in terms of compute access (not everyone has access to a GPU) and time - this solutions takes significantly longer (a little over 10 seconds becomes over 2 minutes on average). 

```{r session-info}
sessionInfo()
```
