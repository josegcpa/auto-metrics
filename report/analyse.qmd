---
title: "A short report on automating METRICS"
code-line-numbers: true
format: 
  html:
    df-print: paged
    toc: true
    toc-location: left
editor: visual
bibliography: bibliography.bib
csl: "ieee.csl"
author: Jos√© Guilherme de Almeida
footnotes-hover: true
reference-location: margin
code-fold: true
---

# TL;DR

It is possible --- to some extent --- to automate METRICS (a radiomics publication quality assessment tool) using large language models. You can test this [here](https://auto-metrics.netlify.app/) with your own Gemini API key.

# Context

Here we show a proof of concept on how it is possible to automate some aspects of scientific analysis. In particular, we will be doing this through the Methodological Radiomics Score (otherwise known as METRICS) @Kocak2024-wk. METRICS comprises a set of helpful guidelines to determine the quality of a scientific publication doing some form of radiomics analysis (i.e. the systematic extraction of imaging features from clinical images and the correlation of these features with patient-specific outcomes or image characteristics @Van_Timmeren2020-gy). These sorts of scores are not new, with the radiomics quality score (RQS) being the other main contender for scoring radiomics @Lambin2017-rn. Whether METRICS is more or less useful than RQS has been addressed in other publications and it is not the particular concern of this piece to clarify it @Kocak2024-sv. Instead, here we make use of a recent publication on the reproducibility of METRICS scores: Akinci D'Antonoli and others' "Reproducibility of methodological radiomics score (METRICS): an intra- and inter-rater reliability study endorsed by EuSoMII" @Akinci-D-Antonoli2025-ep.

## Short summary of _Akinci D'Antonoli and others_

In this work, a set of radiologists was divided into three separate experience levels --- from 1 (less experienced) to 3 (more experienced) - based on their experience not only as radiologists but also as academics. Two experimental conditions were tested --- no training on METRICS and with training with METRICS. The overall experimental design is sound and the conclusions are quite interesting: in essence, there is low-to-moderate inter-rater reliability, and this is particularly true when comparing between no training and with training [^repeatibility]. 

[^repeatibility]: The article also features repeatibility experiments, but that is not within the scope of this short report.

## Visualizing reproducibility results from Akinci D'Antonoli and others

Here we are providing only a recapitulation of what is provided as Supplementary Information 1. In particular, here we only use the Cohen's Kappa to analyse how agreement varies between different conditions and raters. As observable in @fig-cohen-raters, Cohen's Kappa (a measure of inter-rater agreement) is quite consistent and between 0.4 and 0.6. The one exception is the very high Kappa between experience level 3 (highest experience level) with and without training. This may highlight the effect of experience or it may be biased as the four radiologists annotated with experience level 3 are either editors/members of the editorial board of European Radiology (and are more exposed to this sort of analysis) or took part in developing METRICS (AP/AS/MK and LU, respectively); only three other authors are in this position of being parts of editorial teams at radiology journals/publications or having participated in METRICS. In any case it quite a stark difference in my opinion.

Further analyzing how this looks for items and conditions separetely (i.e. calculating the average item-specific Cohen's Kappa) further confirms the overall analysis as illustrated in @fig-cohen-raters-items. Finally, it is interesting to see that there is little agreement as far as determining the different conditions is concerned @fig-cohen-raters-conditions. This part is crucial as some items depend on specific conditions being met for grading, otherwise they are discarded.

```{r}
#| output: false
library(tidyverse)
library(rjson)
library(irr)
library(knitr)
library(kableExtra)

theme_classic_2 <- function(base_size = 11,
                            base_family = "",
                            base_line_size = 11/22,
                            base_rect_size = 11/22) {
  theme_classic(base_size = base_size,
                base_family = base_family,
                base_line_size = base_line_size,
                base_rect_size = base_rect_size) %>%
    theme(axis.text = element_text(size = base_size,
                                   colour = "black"),
          axis.title = element_text(size = base_size),
          strip.background = element_blank(),
          strip.text = element_text(size = base_size,
                                    face = "bold",
                                    hjust = 0.0,
                                    margin = margin(b = 2)),
          legend.title = element_text(size = base_size, face = "bold"),
          panel.background = element_blank(),
          legend.text = element_text(size = base_size),
          axis.line = element_line(),
          axis.ticks = element_line(),
          panel.grid = element_blank()) %>%
    return
}

match_class <- c(`n/a` = NA, no = 0, yes = 1)
key_order <- c(paste0("Item", 1:30), paste0("Condition", 1:30))
```

```{r data-loading}
#| warning: false
df_original <- read_csv(
  "../data/akinci-dantonoli/inter-rater.csv", col_types = c(rep("c", 40), "i", "i")) 
df <- df_original %>% 
  gather(key = "key", value = "value", 
         -group, -exp, -dois, -dois_clean, -title) %>% 
  subset(key != "Total METRICS score:") %>%
  subset(!grepl("Quality", key)) %>%
  mutate(key = gsub("#", "", key)) %>%
  mutate(group = ifelse(group == 1, "no training", "with training")) %>%
  mutate(value = match_class[value])

df_original
```

```{r data-processing}
#| warning: false
all_raters <- c("no training_1","no training_2","no training_3",
                "with training_1","with training_2","with training_3")
all_rater_labels <- c(
  "No training (1)", "No training (2)", "No training (3)",
  "With training (1)", "With training (2)", "With training (3)")

long_df <- pivot_wider(df, values_from = value, names_from = c("group", "exp"))

rater_combinations <- combn(all_raters, 2)

all_cohens <- list()
for (idx in 1:ncol(rater_combinations)) {
  raters <- rater_combinations[, idx]
  sub_df <- subset(long_df, !grepl("Condition", key))[c(raters[1], raters[2])] %>%
    na.omit() %>%
    as.matrix()
  if ((nrow(sub_df) > 10) & (var(sub_df[,1]) > 0) & (var(sub_df[,2]) > 0)) { # guarantee that at least 10 articles were graded
    curr_df <- data.frame(estimate = kappa2(sub_df)$value)
    curr_df$rater1 <- raters[1]
    curr_df$rater2 <- raters[2]
    all_cohens[[length(all_cohens) + 1]] <- curr_df
  }
}

all_cohens_items <- list()
for (k in unique(long_df$key)) {
  for (idx in 1:ncol(rater_combinations)) {
    raters <- rater_combinations[, idx]
    sub_df <- subset(long_df, key == k)[c(raters[1], raters[2])] %>%
      na.omit() %>%
      as.matrix()
    if ((nrow(sub_df) > 10) & (var(sub_df[,1]) > 0) & (var(sub_df[,2]) > 0)) { # guarantee that at least 10 articles were graded
      curr_df <- data.frame(estimate = kappa2(sub_df)$value)
      curr_df$rater1 <- raters[1]
      curr_df$rater2 <- raters[2]
      curr_df$item <- k
      curr_df$n <- nrow(sub_df)
      all_cohens_items[[length(all_cohens_items) + 1]] <- curr_df 
    }
  }
}
```

```{r fig-cohen-raters}
#| fig.height: 3
#| fig.width: 4
#| label: fig-cohen-raters
#| fig-cap: Cohen's Kappa calculated across all items (1-30) between different groups of raters.
do.call(rbind, all_cohens) %>% 
  mutate(rater1 = factor(rater1, all_raters, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_raters, all_rater_labels)) %>%
  ggplot(aes(x = rater1, y = rater2, fill = estimate)) + 
  geom_tile(colour = "black", linewidth = 0.5) +
  theme_classic_2(base_size = 8) +
  geom_label(aes(label = sprintf("%.2f", estimate)), fill = "white",
             colour = "black", size = 2.5,
             label.size = unit(0, "line"),
             label.padding = unit(0.2, "line")) + 
  scale_fill_distiller(palette = 3, name = "Cohen's\nkappa") +
  scale_x_discrete(expand = c(0,0)) +
  scale_y_discrete(expand = c(0,0)) +
  theme(axis.text.x = element_text(angle = 30, hjust = 1.0, vjust = 1.0),
        axis.title = element_blank())
```

```{r fig-cohen-raters-items}
#| fig.height: 2.5
#| fig.width: 4
#| label: fig-cohen-raters-items
#| fig-cap: Average of Cohen's Kappa calculated between different groups of raters for each specific item.
do.call(rbind, all_cohens_items) %>% 
  subset(grepl("Item", item)) %>%
  mutate(rater1 = factor(rater1, all_raters, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_raters, all_rater_labels)) %>%
  group_by(rater1, rater2) %>%
  na.omit() %>%
  summarise(estimate = sum(estimate * n) / sum(n), .groups = "drop") %>%
  ggplot(aes(x = rater1, y = rater2, fill = estimate)) + 
  geom_tile(colour = "black", linewidth = 0.5) +
  theme_classic_2(base_size = 8) +
  geom_label(aes(label = sprintf("%.2f", estimate)), fill = "white",
             colour = "black", size = 2.5,
             label.size = unit(0, "line"),
             label.padding = unit(0.2, "line")) + 
  scale_fill_distiller(palette = 3, name = "Average cohen's\nkappa across\nitems") +
  scale_x_discrete(expand = c(0,0)) +
  scale_y_discrete(expand = c(0,0)) +
  theme(axis.text.x = element_text(angle = 30, hjust = 1.0, vjust = 1.0),
        axis.title = element_blank())
```

```{r fig-cohen-raters-conditions}
#| fig.height: 2.5
#| fig.width: 4
#| label: fig-cohen-raters-conditions
#| fig-cap: Average of Cohen's Kappa calculated between different groups of raters for each specific condition.
do.call(rbind, all_cohens_items) %>% 
  subset(grepl("Condition", item)) %>%
  mutate(rater1 = factor(rater1, all_raters, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_raters, all_rater_labels)) %>%
  group_by(rater1, rater2) %>%
  na.omit() %>%
  summarise(estimate = sum(estimate * n) / sum(n), .groups = "drop") %>%
  ggplot(aes(x = rater1, y = rater2, fill = estimate)) + 
  geom_tile(colour = "black", linewidth = 0.5) +
  theme_classic_2(base_size = 8) +
  geom_label(aes(label = sprintf("%.2f", estimate)), fill = "white",
             colour = "black", size = 2.5,
             label.size = unit(0, "line"),
             label.padding = unit(0.2, "line")) + 
  scale_fill_distiller(palette = 3, name = "Average cohen's\nkappa across\nconditions") +
  scale_x_discrete(expand = c(0,0)) +
  scale_y_discrete(expand = c(0,0)) +
  theme(axis.text.x = element_text(angle = 30, hjust = 1.0, vjust = 1.0),
        axis.title = element_blank())
```

# LLMs for scientific assessment --- replacement or sidekick?

Given this rich dataset --- which can be easily linked to the relevant literature --- we had a simple thought: how easily can this be performed automatically? With the advent of large language models (LLMs), there are ample opportunities for this. Indeed, combining LLMs with relatively deterministic structured generation can ensure that i) we always get the same results and ii) we always get structured results. The first can be achieved through 0 temperature sampling (i.e. no randomness given the same input) and constrained generation (i.e. the LLM can only generate tokens which force it to satisfy a set of constraints).

Here, we have a concrete hypothesis: is the average agreement between an LLM and human rater groups similar to that which is observed between human rater groups? If so, this puts LLMs as a possible second read as far as METRICS assessments are concerned.

## The protocol

We used Gemini 2.0 Flash because it is remarkably cheap for these sorts of experiments (and in general). As a prompt, we use the following (it is somewhat long so bear with us, it copies the guidelines made available in the original METRICS paper and in the [METRICS helper website](https://metricsscore.github.io/metrics/METRICS.html)):

```{}
# Instructions
You are an expert radiologist with decades of experience in developing and implementing clinical artificial intelligence.
You have to offer ratings to a scientific article of clinical importance at the end of this prompt according to a set of criteria.
You must answer carefully and thoughtfully, considering the context of the article and your expertise.
Be extremely thorough and conservative with your answers as these tools are supposed to be deployed in the clinic.

# Evaluation
## Metrics definition
There are 30 criteria in total and each criterion is grouped into one of 9 categories.
There are, additionally, 5 conditions which define whether some criterion should be filled or not.
The 5 Conditions are defined below under "## Conditions". A short explanation is provided for each.
The 30 criteria are defined below under "## Criteria". Each is grouped under its respective category. A short explanation is provided for each.

## Rating Rubric
No: no evidence that this criterion is being followed in this publication
Yes: evidence that this criterion is being followed in this publication
n/a: not applicable
Reason: a short explanation for each ranking

# Input format
The article text is provided below under "# Article text". Anything outside of this text should not be evaluated.

# Output format
You have to output:
- a summary of the article accurately representing the main conclusion of the article
- the answers for conditions (Yes, No) and a short explanation for your decision
- the ratings (Yes, No or n/a) for each criterion and a short explanation for your decision

## Conditions

* Condition 1: Does the study include segmentation? - "Segmentation" refers to i) Fine delineation of a region or volume of interest; ii) Rough delineation with bounding boxes; or, iii) cropping the image around a region of interest.
* Condition 2: Does the study include fully automated segmentation? - "Fully automated segmentation" refers to segmentation process without any human intervention.
* Condition 3: Does the study include hand-crafted feature extraction? - "Hand-crafted radiomic features" (i.e., traditional radiomic features) are created in advance by human experts or mathematicians.
* Condition 4: Does the study include tabular data? - "Tabular data" refers to data that is organized in a table with rows and columns (i.e., numeric radiomic features in a tabulated format, which  is usually seen in hand-crafted and some deep learning-based studies as deep features).
* Condition 5: Does the study include end-to-end deep learning? - "End-to-end deep learning" refers to the use of deep learning to directly process the image data and produce a classification or regression model.

## Criteria

### Study Design
* Item 1: Adherence to radiomics and/or machine learning-specific checklists or guidelines - Whether any guideline or checklist, e.g., CLEAR checklist, is used in designing and reporting, as appropriate for the study design (e.g., handcrafted radiomics or deep learning pipeline).
* Item 2: Eligibility criteria that describe a representative study population - Whether inclusion and exclusion criteria are explicitly defined. These should lead to a representative study sample that matches the general population of interest for the study aim.
* Item 3: High-quality reference standard with a clear definition - Whether the reference standard or outcome measure is representative of the current  clinical practice and robust. Examples of high-quality reference standards are preferably  histopathology, well-established clinical and genomic markers, the latest version of the prognostic  tools, guideline-based follow-up or consensus-based expert opinions. Examples of poor quality  reference standards are those based on qualitative image evaluation, images that are later  used for feature extraction, or outdated versions of prognostic tools.

### Imaging Data
* Item 4: Multi-center - Whether more than one institution is involved as a diagnostic imaging data source for radiomics analysis.
* Item 5: Clinical translatability of the imaging data source for radiomics analysis - Whether the source of the radiomics data is an imaging technique that reflects established standardization approaches, such as acquisition protocol guidelines (e.g., PI-RADS specifications).
* Item 6: Imaging protocol with acquisition parameters - Whether the image acquisition protocol is clearly reported to ensure the replicability of the method.
* Item 7: The interval between imaging used and reference standard - Whether the time interval between the diagnostic imaging exams (used as an input for the radiomics analysis) and the outcome measure/reference standard acquisition is appropriate to validate the presence or absence of target conditions of the radiomics analysis at the moment of the diagnostic imaging exams.

### Segmentation
* Item 8: Transparent description of segmentation methodology - Whether the rules or the method of the segmentation are defined (e.g., margin shrinkage, peri-tumoral sampling, details of segmentation regardless of whether manual, semi-automated or automated methods are used). In the case of DL-based radiomics, the segmentation can refer to the rough delineation with bounding boxes or cropping the image around a region of interest. Answer only if Condition 1 are "yes".
* Item 9: Formal evaluation of fully automated segmentation - If a segmentation technique that does not require any sort of human intervention is used, examples of the results should be presented and a formal assessment of its accuracy compared to domain expert annotations included in the study (e.g., DICE score or Jaccard index compared with a radiologist's semantic annotation). Answer only if Condition 1 and 2 are "yes".
* Item 10: Test set segmentation masks produced by a single reader or automated tool - Whether final segmentation in the test set is produced by a single reader (manually or with a semi-automated tool) or an entirely automated tool, to better reflect clinical practice. Answer only if Condition 1 are "yes".

### Image Processing and Feature Extraction
* Item 11: Appropriate use of image preprocessing techniques with transparent description - Whether preprocessing of the images is appropriately performed  considering the imaging modality (e.g., gray level normalization for MRI, image registration in case of multiple contrasts or modalities) and feature extraction techniques (i.e., 2D or 3D) that are used. For instance, in the case of large slice thickness (e.g., u22655 mm), extreme upsampling (e.g., 1 x 1 x 1 mm3) of the volume might be inappropriate. In such a case, 2D feature extraction could be preferable, ensuring in-plane isotropy of the pixels. On the other hand, achieving isotropic voxel values should be targeted in 3D feature extraction, to allow for texture feature rotational invariance. Also, whether gray level discretization parameters (bin width, along with resulting gray level range, or bin count) are described in full detail. Description of different image types used (e.g., original, filtered) should also be included (e.g., high and low pass filter combinations for wavelet decomposition, sigma values for Laplacian of Gaussian edge enhancement filtering). If the image window is fixed, it should be clarified.
* Item 12: Use of standardized feature extraction software - Whether a standardized software (e.g., compliant with IBSI) was used for feature extraction, including information on the version number. Answer only if Condition 3 are "yes".
* Item 13: Transparent reporting of feature extraction parameters, otherwise providing a default configuration statement - Whether feature types (e.g., hand-crafted, deep features) and feature classes (for hand-crafted) are described. Also, if a default configuration statement is provided for the remaining feature extraction parameters. A file presenting the complete configuration of these settings should be included in the study materials (e.g., parameter file such as in YAML format, screenshot if a dedicated file for this is not available for the software). In the case of DL, neural network architecture along with all image operations should be described.

### Feature Processing
* Item 14: Removal of non-robust features - Whether unstable features are removed via test-retest, reproducibility analysis by analysis of different segmentations, or stability analysis [i.e., image perturbations]. Instability may be due to random noise introduced by manual or even automated image segmentation or exposed in a scan-rescan setting. The specific methods used should be clearly presented, with specific results for each component in multi-step feature removal pipelines. Answer only if Condition 4 are "yes".
* Item 15: Removal of redundant features - Whether dimensionality is reduced by selecting the more informative features such as with algorithm-based feature selection (e.g., LASSO coefficients, Random Forest feature importance), univariate correlation, collinearity, or variance analysis. The specific methods used should be clearly presented, with specific results for each component in multi-step feature removal pipelines. Answer only if Condition 4 are "yes".
* Item 16: Appropriateness of dimensionality compared to data size - Whether the number of instances and features in the final training data set is appropriate according to the research question and modeling algorithm. This should be demonstrated by statistical means, empirically through consistency of performance in validation and testing, or based on previous evidence in the literature. Answer only if Condition 4 are "yes".
* Item 17: Robustness assessment of end-to-end deep learning pipelines - Whether DL pipeline consistency of performance has been assessed in a test-retest setting, for example by a scan-rescan approach, use of segmentations by different readers, or stability analysis [i.e., image perturbations]. Answer only if Condition 5 are "yes".

### Preparation for Modeling
* Item 18: Proper data partitioning process - Whether the training-validation-test data split is done at the very beginning of the analysis pipeline, prior to any processing step. Data split should be random but reproducible (e.g., fixed random seed), preferably without altering outcome variable distribution in the test set (e.g., using a stratified data split). Moreover, the data split should be on the patient level, not the scan level (i.e., different scans of the same patient should be in the same set). Proper data partitioning should guarantee that all data processing (e.g., scaling, missing value imputation, oversampling or undersampling) is done blinded to the test set data. These techniques should be exclusively fitted on training (or development) data sets and then used to transform test data at the time of inference. If a single training-validation data split is not done and a resampling technique (e.g., cross-validation) is used instead, test data should always be handled separately from this.
* Item 19: Handling of confounding factors - Whether potential confounding factors were analyzed, identified if present, and removed if necessary (e.g., if it has a strong influence on generalizability). These may include different distributions of patient characteristics (e.g., gender, lesion stage or grade) across sites or scanners.

### Metrics and Comparison
* Item 20: Use of appropriate performance evaluation metrics for task - Whether appropriate accuracy metrics are reported, such as AUC for Receiver Operating Characteristics (ROC) or Precision-Recall (PRC) curves and confusion matrix-derived accuracy metrics (e.g., specificity, sensitivity, precision, F1 score) for classification tasks; MSE, RMSE, and MAE for regression tasks. For classification tasks, the confusion matrix should always be included, to allow the calculation of additional metrics. If training a DL network, loss curves should be presented.
* Item 21: Consideration of uncertainty - Whether uncertainty measures are included in the analysis, such as 95% confidence interval (CI), standard deviation (SD), or standard error (SE). Report on methodology to derive that distribution (ie. bootstrapping with replacement, etc).
* Item 22: Calibration assessment - Whether the final model's calibration is assessed.
* Item 23: Use of uni-parametric imaging or proof of its inferiority - Use of a single imaging set (such as a single MRI sequence rather than multiple, or a single phase in a dynamic contrast-enhanced scan) should be preferred, as multi-parametric imaging may unnecessarily increase data dimensionality and risk of overfitting. Therefore, in the case of multi-parametric studies, uni-parametric evaluations should also be performed to justify the need for a multi-parametric approach by formally comparing their performance (e.g., DeLong's or McNemar's tests). This item is also intended to reward studies that experimentally justify the use of more complex models compared to simpler alternatives, in regard to input data type.
* Item 24: Comparison with a non-radiomic approach or proof of added clinical value - Whether a non-radiomic method that is representative of the clinical practice is included in the analysis for comparison purposes. Non-radiomic methods might include semantic features,  RADS or RECIST scoring, and simple volume or size evaluations. If no non-radiomics method is available,  proof of improved diagnostic accuracy (e.g., improved performance of a radiologist assisted by the  model's output) or patient outcome (e.g., decision analysis, overall survival) should be provided.  In any case, the comparison should be done with an appropriate statistical method to evaluate the  added practical and clinical value of the model (e.g., DeLong‚Äôs test for AUC comparison, decision  curve analysis for net benefit comparison, Net Reclassification Index). Furthermore, in case of  multiple comparisons, multiple testing correction methods (e.g., Bonferroni) should be considered  in order to reduce the false discovery rate provided that the statistical comparison is done with  a frequentist approach (rather than Bayesian).
* Item 25: Comparison with simple or classical statistical models - Whether a comparison with a simple baseline reference model (such as a Zero Rules/No Information Rate classifier) was performed. Use of machine learning methods should be justified by proof of increased performance. In any case, the comparison should be done with an appropriate statistical method (e.g., DeLong's test for AUC comparison, Net Reclassification Index). Furthermore, in case of multiple comparisons, multiple testing correction methods (e.g., Bonferroni, Benjamini-Hochberg, or Tukey) should be considered in order to reduce the false discovery rate provided that the statistical comparison is done with a frequentist approach (rather than Bayesian).

### Testing
* Item 26: Internal testing - Whether the model is tested on an independent data set that is sampled from the same source as the training and/or validation sets.
* Item 27: External testing - Whether the model is tested with independent data from other institution(s). This also applies to the studies validating the previously published models trained at another institution.

### Open Science
* Item 28: Data availability - Whether any imaging, segmentation, clinical, or radiomics analysis data is shared with the public.
* Item 29: Code availability - Whether all scripts related to automatic segmentation and/or modeling are shared with the public. These should include clear instructions for their implementation (e.g., accompanying documentation, tutorials).
* Item 30: Model availability - Whether the final model is shared in the form of a raw model file or as a ready-to-use system. If automated segmentation was employed, the corresponding trained model should also be made available to allow replication. These should include clear instructions for their usage (e.g., accompanying documentation, tutorials).


# Article text

{article}
```

We start by detailing some instructions that the LLM should follow. Then, we provide clear evaluation metrics and a rating rubric which should be followed. Finally, we declare the input format and make space for the actual METRICS guidelines and output format. Where we wrote `{article}`, that is where the text is inserted. To get these article texts we went through the articles which we could access --- between publicly available and institutionally-accessible articles we were able to access 31 out of the total of 34 --- and simply copied the text between the abstract (incl.) and the references (excl.) for each.

To ensure structured outputs, we make use of a relatively strict --- but simple! --- data model for the output:

```{python}
#| echo: true
#| eval: false
from enum import Enum
from pydantic import BaseModel


class RatingEnum(Enum):
    yes = "yes"
    no = "no"


class RatingWithNAEnum(Enum):
    yes = "yes"
    no = "no"
    na = "n/a"


class Rating(BaseModel):
    rating: RatingEnum
    reason: str


class RatingWithNA(BaseModel):
    rating: RatingWithNAEnum
    reason: str


class Metrics(BaseModel):
    Summary: str
    Condition1: Rating
    Condition2: Rating
    Condition3: Rating
    Condition4: Rating
    Condition5: Rating
    Item1: Rating
    Item2: Rating
    Item3: Rating
    Item4: Rating
    Item5: Rating
    Item6: Rating
    Item7: Rating
    Item8: RatingWithNA
    Item9: RatingWithNA
    Item10: RatingWithNA
    Item11: Rating
    Item12: RatingWithNA
    Item13: Rating
    Item14: RatingWithNA
    Item15: RatingWithNA
    Item16: RatingWithNA
    Item17: RatingWithNA
    Item18: Rating
    Item19: Rating
    Item20: Rating
    Item21: Rating
    Item22: Rating
    Item23: Rating
    Item24: Rating
    Item25: Rating
    Item26: Rating
    Item27: Rating
    Item28: Rating
    Item29: Rating
    Item30: Rating
```

## The tests

Upon loading, we were able to verify that all the LLM outputs have, as expected, generated a correctly formatted JSON. Below we present an example for one such article. Additionally, the flexibility of our specification also allows for a small reason to be appended to each rating --- this facilitates the verification of LLM claims. Indeed, this explanation ends up being helpful at a later stage of this analysis when we start getting into possible reasons for disagreement.

```{r}
llm_rating_files <- list.files(path = "../ratings/akinci-dantonoli", pattern = "*json", full.names = T)
names(llm_rating_files) <- lapply(
  llm_rating_files, function(x) gsub(".json", "", str_split(x, "/")[[1]][4])
)
cat(sprintf("Loading and printing the JSON for %s:\n\n", names(llm_rating_files)[[1]]))

read_file(file = llm_rating_files[[1]]) %>%
  cat
```

Upon analysis of @fig-cohen-raters-llm, @fig-cohen-raters-items-llm and @fig-cohen-raters-conditions-llm, we see quite clearly that, while there is some agreement between LLMs and raters, this is very much on the lower end of the agreement spectrum which is observed for human rater groups. Indeed, it can be risky to simply deployed this sans supervision. However, we note that this is still well within what would be acceptable had this been an additional reviewer. As noted earlier, it might be a good format to have an LLM perform these assessments as a second reader which ends up reinforcing some conclusions or reanalysing others.

```{r}
all_llm_ratings <- list()
for (f in llm_rating_files) {
  title <- gsub(".json", "", str_split(f, "/")[[1]][4])
  json_data <- fromJSON(file = f)
  json_df <- data.frame(
    key = names(json_data[-1]), 
    llm = unlist(lapply(json_data[-1], function(x) x$rating)), title = title)
  all_llm_ratings[[length(all_llm_ratings) + 1]] <- json_df
}

llm_df <- do.call(rbind, all_llm_ratings) %>%
  mutate(llm = match_class[llm]) %>%
  mutate(title = gsub("_", "/", title))

# redefine rater vectors to include LLMs
all_raters <- c("no training_1","no training_2","no training_3",
                "with training_1","with training_2","with training_3",
                "llm")
all_rater_labels <- c(
  "No training (1)", "No training (2)", "No training (3)",
  "With training (1)", "With training (2)", "With training (3)",
  "LLM"
)

full_df <- merge(
  pivot_wider(df, values_from = value, names_from = c("group", "exp")), 
  llm_df, 
  by = c("key", "title"))
```

```{r}
#| warning: false
rater_combinations <- combn(all_raters, 2)

all_cohens <- list()
for (idx in 1:ncol(rater_combinations)) {
  raters <- rater_combinations[, idx]
  sub_df <- subset(full_df, !grepl("Condition", key))[c(raters[1], raters[2])] %>%
    na.omit() %>%
    as.matrix()
  if ((nrow(sub_df) > 10) & (var(sub_df[,1]) > 0) & (var(sub_df[,2]) > 0)) { # guarantee that at least 10 articles were graded
    curr_df <- data.frame(estimate = kappa2(sub_df)$value)
    curr_df$rater1 <- raters[1]
    curr_df$rater2 <- raters[2]
    all_cohens[[length(all_cohens) + 1]] <- curr_df
  }
}

all_cohens_items <- list()
for (k in unique(full_df$key)) {
  for (idx in 1:ncol(rater_combinations)) {
    raters <- rater_combinations[, idx]
    sub_df <- subset(full_df, key == k)[c(raters[1], raters[2])] %>%
      na.omit() %>%
      as.matrix()
    if ((nrow(sub_df) > 10) & (var(sub_df[,1]) > 0) & (var(sub_df[,2]) > 0)) { # guarantee that at least 10 articles were graded
      curr_df <- data.frame(
        estimate = kappam.fleiss(sub_df)$value)
      curr_df$rater1 <- raters[1]
      curr_df$rater2 <- raters[2]
      curr_df$item <- k
      curr_df$n <- nrow(sub_df)
      all_cohens_items[[length(all_cohens_items) + 1]] <- curr_df 
    }
  }
}
```

```{r fig-cohen-raters-llm}
#| fig.height: 3
#| fig.width: 4
#| label: fig-cohen-raters-llm
#| fig-cap: Cohen's Kappa calculated across all items (1-30) between different groups of raters and the LLM.
do.call(rbind, all_cohens) %>% 
  mutate(rater1 = factor(rater1, all_raters, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_raters, all_rater_labels)) %>%
  ggplot(aes(x = rater1, y = rater2, fill = estimate)) + 
  geom_tile(colour = "black", linewidth = 0.5) +
  theme_classic_2(base_size = 8) +
  geom_label(aes(label = sprintf("%.2f", estimate)), fill = "white",
             colour = "black", size = 2.5,
             label.size = unit(0, "line"),
             label.padding = unit(0.2, "line")) + 
  scale_fill_distiller(palette = 3, name = "Cohen's\nkappa") +
  scale_x_discrete(expand = c(0,0)) +
  scale_y_discrete(expand = c(0,0)) +
  theme(axis.text.x = element_text(angle = 30, hjust = 1.0, vjust = 1.0),
        axis.title = element_blank())
```

```{r fig-cohen-raters-items-llm}
#| fig.height: 2.5
#| fig.width: 4
#| label: fig-cohen-raters-items-llm
#| fig-cap: Average of Cohen's Kappa calculated between different groups of raters and the LLM for each specific item.
do.call(rbind, all_cohens_items) %>% 
  subset(grepl("Item", item)) %>%
  mutate(rater1 = factor(rater1, all_raters, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_raters, all_rater_labels)) %>%
  group_by(rater1, rater2) %>%
  na.omit() %>%
  summarise(estimate = sum(estimate * n) / sum(n), .groups = "drop") %>%
  ggplot(aes(x = rater1, y = rater2, fill = estimate)) + 
  geom_tile(colour = "black", linewidth = 0.5) +
  theme_classic_2(base_size = 8) +
  geom_label(aes(label = sprintf("%.2f", estimate)), fill = "white",
             colour = "black", size = 2.5,
             label.size = unit(0, "line"),
             label.padding = unit(0.2, "line")) + 
  scale_fill_distiller(palette = 3, name = "Average cohen's\nkappa across\nitems") +
  scale_x_discrete(expand = c(0,0)) +
  scale_y_discrete(expand = c(0,0)) +
  theme(axis.text.x = element_text(angle = 30, hjust = 1.0, vjust = 1.0),
        axis.title = element_blank())
```

```{r fig-cohen-raters-conditions-llm}
#| fig.height: 2.5
#| fig.width: 4
#| label: fig-cohen-raters-conditions-llm
#| fig-cap: Average of Cohen's Kappa calculated between different groups of raters and the LLM for each specific condition.
do.call(rbind, all_cohens_items) %>% 
  subset(grepl("Condition", item)) %>%
  mutate(rater1 = factor(rater1, all_raters, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_raters, all_rater_labels)) %>%
  group_by(rater1, rater2) %>%
  na.omit() %>%
  summarise(estimate = sum(estimate * n) / sum(n), .groups = "drop") %>%
  ggplot(aes(x = rater1, y = rater2, fill = estimate)) + 
  geom_tile(colour = "black", linewidth = 0.5) +
  theme_classic_2(base_size = 8) +
  geom_label(aes(label = sprintf("%.2f", estimate)), fill = "white",
             colour = "black", size = 2.5,
             label.size = unit(0, "line"),
             label.padding = unit(0.2, "line")) + 
  scale_fill_distiller(palette = 3, name = "Average cohen's\nkappa across\nconditions") +
  scale_x_discrete(expand = c(0,0)) +
  scale_y_discrete(expand = c(0,0)) +
  theme(axis.text.x = element_text(angle = 30, hjust = 1.0, vjust = 1.0),
        axis.title = element_blank())
```

In @fig-proportions we show how these scores are somewhat variable when analysed on a per-item basis. While some items are somewhat consistent (Items 1, 2, 20, 21, 22, 29 and 30 all have percent agreements[^pct] > 70%), most are relatively scattered.

[^pct]: here we used percent agreements as opposed to Cohen's Kappa because the latter is not defined when data has 0 variance (some questions were always answered the same) and has unintuitive behavior when it gets too close to 0 or 1.

```{r}
#| fig.height: 5
#| fig.width: 6
#| warning: false
#| fig-cap: Proportion of agreement between human readers and the LLM METRICS assessment stratified by training groups and experience levels.
#| label: fig-proportions
proportion_data <- merge(
  df,
  llm_df, 
  by = c("key", "title")) %>% 
  group_by(key, group, exp) %>% 
  summarise(capture = mean(value == llm, na.rm = T), .groups = "drop") %>%
  mutate(key = factor(key, rev(key_order))) %>%
  mutate(cond_or_item = ifelse(grepl("Condition", key), "Condition", "Item"))

proportion_data %>%
  ggplot(aes(x = capture, y = key, colour = as.factor(exp),
             shape = group, group = paste(exp, group))) + 
  geom_point() + 
  theme_classic_2(base_size = 8) +
  theme(panel.grid.major.y = element_line(color = "grey90"),
        legend.key.size = unit(0, "line"),
        strip.text.y = element_text(angle = 0)) +
  xlab("Proportion of agreements with LLM") +
  ylab("Item") +
  facet_grid(cond_or_item ~ ., scales = "free", space = "free", ) + 
  scale_colour_brewer(palette = "Set1",
                      name = "Experience level") +
  scale_shape_manual(values = c(2, 3), name = "Training") +
  guides(x = guide_axis(cap = T))
```

## Analysing some specific cases

My main point of interest now was getting to know how these differences are manifested. As such, we will focus on cases where the number of disagreements with human raters is relatively large --- in other words, we want to know if there are actual intuitive reasons behind LLM behavior. We consider here that agreements are worth 1 point, disagreements -1, and incompatibilities (i.e. LLM provided answer but answer for human raters is N/A) is 0.

A main point of this analysis is for us to think through of some possible improvements that can be added to the description of each item to make it as clear as possible for automated assessment.

```{r}
score_agreement <- function(a, b) {
  d <- case_when(
    is.na(a) | is.na(b) ~ NA,
    a != b ~ -1,
    a == b ~ 1
  )
  return(d)
}

sum_na_if_empty <- function(x) {
  if (length(x) == 0) {
    return(NA)
  }
  return(sum(x))
}

disagreement_df <- full_df %>%
  rowwise() %>%
  mutate(
    n_disagreements = c(
      score_agreement(`no training_1`, llm),
      score_agreement(`no training_2`, llm),
      score_agreement(`no training_3`, llm),
      score_agreement(`with training_1`,llm),
      score_agreement(`with training_2`, llm), 
      score_agreement(`with training_3`, llm)
    ) %>%
      Filter(f = function(x) !is.na(x)) %>%
      sum_na_if_empty
  ) %>%
  arrange(n_disagreements)

disagreement_df %>%
  group_by(key) %>%
  summarise(average_disagreement_score = mean(n_disagreements, na.rm = T)) %>%
  arrange(average_disagreement_score)
```

### Item23

*Use of uni-parametric imaging or proof of its inferiority*

As in the table above, we can see that the average disagreement score (the average of the scoring system noted above) is more severe for Item23 - whether uni-parametric imaging was used or proved to be inferior to multi-parametric imaging. Focusing on this, we take five papers at random where all reader groups said "yes" while the LLM said "no". We bring up the reasons provided by the LLM to further clarify this, and inspect each paper individually to better understand what the reason was. By and large, two dominant reasons appear to arise:

1. The LLM mistakenly assumes that using multiple image transformations during radiomic feature extraction counts as "multi-parametric imaging"
2. The LLM mistakenly interprets the paper and assumes that there are other categories apart from "uni-parametric imaging" and "multi-parametric imaging"

#### Potential improvement

Including the following in the item helper: "Uni-parametric imaging implies the acquisition of a single imaging modality, as opposed to multi-parametric imaging, which involves the acquisition of multiple  imaging modalities."

```{r}
set.seed(42)

get_examples_of_disagreement <- function(item_number, n_examples = 5) {
  item_subset <- disagreement_df %>%
    subset(key == sprintf("Item%s", item_number)) %>%
    arrange(n_disagreements) 
  
  if (nrow(item_subset) < n_examples) {
    n_examples <- nrow(item_subset)
  }
  relevant_titles <- item_subset$title[1:n_examples]
  
  for (tt in relevant_titles) {
    reason <- fromJSON(file = llm_rating_files[[gsub("/", "_", tt)]])[[sprintf("Item%s", item_number)]]
    reason_number <- match_class[reason$rating]
    human_raters <- subset(item_subset, title == tt)
    human_raters <- unlist(
      human_raters[, Filter(function(x) grepl("training", x), colnames(human_raters))]) %>%
      paste(collapse = ", ")
    cat(sprintf("Title: %s\nReason (human raters: %s; LLM: %s): %s\n\n", 
                tt, 
                human_raters,
                reason_number, 
                reason$reason))
  }
}

get_examples_of_disagreement(23)
```

### Item7

*The interval between imaging used and reference standard*

Disagreements for this item are somewhat interesting --- upon closer inspection of any of the papers mentioned below, there are no errors from the LLM. However, the LLM considers these as "yes", but most radiologist groups considered this to be the opposite. We found that the interval between imaging and reference standard is acceptable (except for "Application of a comprehensive model based on CT radiomics and clinical features for postoperative recurrence risk prediction in non-small cell lung cancer", where this interval is not mentioned) and follows the helping criteria [^item7help]; however, there may be slight aspects of METRICS which we are not fully understanding.

[^item7help]: "Whether the time interval between the diagnostic imaging exams (used as an input for the radiomics analysis) and the outcome measure/reference standard acquisition is appropriate to validate the presence or absence of target conditions of the radiomics analysis at the moment of the diagnostic imaging exams."

#### Potential improvement

Inclusion of the following in the item helper: "If there is no mention of when the reference standard was acquired relative to the diagnostic imaging  exams, the answer should be "no"."

```{r}
get_examples_of_disagreement(7)
```

### Item16

*Appropriateness of dimensionality compared to data size*

This is something which was somewhat expected --- the METRICS question referring to appropriate data dimensionality does not define this in exact terms. Consequently, Item16 is typically hard to understand @Akinci-D-Antonoli2025-ep.

#### Potential improvement

N/A

```{r}
get_examples_of_disagreement(16)
```

### Item10

*Test set segmentation masks produced by a single reader or automated tool*

Here, the LLM considers a senior radiologist reviewing segmentations to be standard clinical practice, and given that the helper note[^item10helper] does not specifically address what "clinical practice" is or isn't, the LLM (very wrongfully) assumes that segmentation reviews are a standard part of it. 

[^item10helper]: "Whether final segmentation in the test set is produced by a single reader (manually or with a semi-automated tool) or an entirely automated tool, to better reflect clinical practice."

#### Potential improvement

Inclusion of the following in the item helper: "Keep in mind that having a radiologist review the results of the automated segmentation is not considered clinical practice."

```{r}
get_examples_of_disagreement(10)
```

### Item19

*Handling of confounding factors*

Here, the disagreements are, as far as I can tell, stemming from a difficulty I myself face when considering questions such as this. The helper note for this item [^item19helper] is somewhat hard to consider in strict terms. For instance: 

- In 'A Radiomic ‚ÄúWarning Sign‚Äù of Progression on Brain MRI in Individuals with MS', the authors offer an analysis of a pipeline-specific confounder which could affect how predictions were obtained or are interpreted. However, this confounder does not represent anything inherent to the patient or to the acquisition process, but rather to the algorithm developed during this work
- In "CNN-based multi-modal radiomics analysis of pseudo-CT utilization in MRI-only brain stereotactic radiotherapy: a feasibility study", the authors do offer an analysis of how MRI contrast presence affects model performance, but then again this is not really a confounder in the traditional sense
- In the remaining articles were the LLM answered "yes" and reader groups tended towards "no", there are some more confounding factors which _could_ be considered confounders but it is understandable that they are not _necessarily_ regarded as such by most people.

[^item19helper]: "Whether potential confounding factors were analyzed, identified if present, and removed if necessary (e.g., if it has a strong influence on generalizability). These may include different distributions of patient characteristics (e.g., gender, lesion stage or grade) across sites or scanners."

#### Potential improvement

Inclusion of the following in the item helper: "Be sure to focus on confounders stemming from the acquisition process, such as scanner type, manufacturer, or scanner parameters, or from the individual patient characteristics, such as age, weight, or comorbidities."

```{r}
get_examples_of_disagreement(19)
```

### Reanalysis with improved prompt

Upon the analysis above, we use the same LLM with the data and the improved descriptions injected into the prompt. While not ideal, this will allow us to see --- to a very limited extent as we are lacking an external validation --- whether this can lead to an improvement. As highlighted in @fig-cohen-raters-llm-improved, @fig-cohen-raters-items-llm-improved and @fig-cohen-raters-conditions-llm-improved it is possible to see a clear improvement in performance. @fig-cohen-raters-conditions-llm-improved-subset, specific for the five items with suggestions above, this process can lead to systematic improvements. Additionally, an interesting effect non-linear effect here is that while the condition prompts where not altered, there was a notable improvement to the agreement between LLM and human rater groups for conditions (@fig-cohen-raters-conditions-llm-improved).

```{r}
improved_llm_rating_files <- list.files(path = "../ratings_improved/akinci-dantonoli", 
                                        pattern = "*json", full.names = T)
names(improved_llm_rating_files) <- lapply(
  improved_llm_rating_files, function(x) gsub(".json", "", str_split(x, "/")[[1]][4])
)
```

```{r}
all_improved_llm_ratings <- list()
for (f in improved_llm_rating_files) {
  title <- gsub(".json", "", str_split(f, "/")[[1]][4])
  json_data <- fromJSON(file = f)
  json_df <- data.frame(
    key = names(json_data[-1]), 
    llm_changes = unlist(lapply(json_data[-1], function(x) x$rating)), title = title)
  all_improved_llm_ratings[[length(all_improved_llm_ratings) + 1]] <- json_df
}

improved_llm_df <- do.call(rbind, all_improved_llm_ratings) %>%
  mutate(llm_changes = match_class[llm_changes]) %>%
  mutate(title = gsub("_", "/", title))

# redefine rater vectors to include LLMs
all_raters <- c("no training_1","no training_2","no training_3",
                "with training_1","with training_2","with training_3",
                "llm", "llm_changes")
all_rater_labels <- c(
  "No training (1)", "No training (2)", "No training (3)",
  "With training (1)", "With training (2)", "With training (3)",
  "LLM", "LLM + changes"
)

fuller_df <- merge(
  full_df,
  improved_llm_df, 
  by = c("key", "title"))
```

```{r}
#| warning: false
rater_combinations <- combn(all_raters, 2)

all_cohens <- list()
for (idx in 1:ncol(rater_combinations)) {
  raters <- rater_combinations[, idx]
  sub_df <- subset(fuller_df, !grepl("Condition", key))[c(raters[1], raters[2])] %>%
    na.omit() %>%
    as.matrix()
  if ((nrow(sub_df) > 10) & (var(sub_df[,1]) > 0) & (var(sub_df[,2]) > 0)) { # guarantee that at least 10 articles were graded
    curr_df <- data.frame(estimate = kappa2(sub_df)$value)
    curr_df$rater1 <- raters[1]
    curr_df$rater2 <- raters[2]
    all_cohens[[length(all_cohens) + 1]] <- curr_df
  }
}

all_cohens_items <- list()
for (k in unique(fuller_df$key)) {
  for (idx in 1:ncol(rater_combinations)) {
    raters <- rater_combinations[, idx]
    sub_df <- subset(fuller_df, key == k)[c(raters[1], raters[2])] %>%
      na.omit() %>%
      as.matrix()
    if ((nrow(sub_df) > 10) & (var(sub_df[,1]) > 0) & (var(sub_df[,2]) > 0)) { # guarantee that at least 10 articles were graded
      curr_df <- data.frame(
        estimate = kappam.fleiss(sub_df)$value)
      curr_df$rater1 <- raters[1]
      curr_df$rater2 <- raters[2]
      curr_df$item <- k
      curr_df$n <- nrow(sub_df)
      all_cohens_items[[length(all_cohens_items) + 1]] <- curr_df 
    }
  }
}
```

```{r fig-cohen-raters-llm}
#| fig.height: 3.0
#| fig.width: 4.5
#| label: fig-cohen-raters-llm-improved
#| fig-cap: Cohen's Kappa calculated across all items (1-30) between different groups of raters and the LLM.
do.call(rbind, all_cohens) %>% 
  mutate(rater1 = factor(rater1, all_raters, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_raters, all_rater_labels)) %>%
  ggplot(aes(x = rater1, y = rater2, fill = estimate)) + 
  geom_tile(colour = "black", linewidth = 0.5) +
  theme_classic_2(base_size = 8) +
  geom_label(aes(label = sprintf("%.2f", estimate)), fill = "white",
             colour = "black", size = 2.5,
             label.size = unit(0, "line"),
             label.padding = unit(0.2, "line")) + 
  scale_fill_distiller(palette = 3, name = "Cohen's\nkappa") +
  scale_x_discrete(expand = c(0,0)) +
  scale_y_discrete(expand = c(0,0)) +
  theme(axis.text.x = element_text(angle = 30, hjust = 1.0, vjust = 1.0),
        axis.title = element_blank())
```

```{r fig-cohen-raters-items-llm}
#| fig.height: 3.0
#| fig.width: 4.5
#| label: fig-cohen-raters-items-llm-improved
#| fig-cap: Average of Cohen's Kappa calculated between different groups of raters and the LLM for each specific item.
do.call(rbind, all_cohens_items) %>% 
  subset(grepl("Item", item)) %>%
  mutate(rater1 = factor(rater1, all_raters, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_raters, all_rater_labels)) %>%
  group_by(rater1, rater2) %>%
  na.omit() %>%
  summarise(estimate = sum(estimate * n) / sum(n), .groups = "drop") %>%
  ggplot(aes(x = rater1, y = rater2, fill = estimate)) + 
  geom_tile(colour = "black", linewidth = 0.5) +
  theme_classic_2(base_size = 8) +
  geom_label(aes(label = sprintf("%.2f", estimate)), fill = "white",
             colour = "black", size = 2.5,
             label.size = unit(0, "line"),
             label.padding = unit(0.2, "line")) + 
  scale_fill_distiller(palette = 3, name = "Average cohen's\nkappa across\nitems") +
  scale_x_discrete(expand = c(0,0)) +
  scale_y_discrete(expand = c(0,0)) +
  theme(axis.text.x = element_text(angle = 30, hjust = 1.0, vjust = 1.0),
        axis.title = element_blank())
```

```{r fig-cohen-raters-conditions-llm}
#| fig.height: 3.0
#| fig.width: 4.5
#| label: fig-cohen-raters-conditions-llm-improved
#| fig-cap: Average of Cohen's Kappa calculated between different groups of raters and the LLM for each specific condition.
do.call(rbind, all_cohens_items) %>% 
  subset(grepl("Condition", item)) %>%
  mutate(rater1 = factor(rater1, all_raters, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_raters, all_rater_labels)) %>%
  group_by(rater1, rater2) %>%
  na.omit() %>%
  summarise(estimate = sum(estimate * n) / sum(n), .groups = "drop") %>%
  ggplot(aes(x = rater1, y = rater2, fill = estimate)) + 
  geom_tile(colour = "black", linewidth = 0.5) +
  theme_classic_2(base_size = 8) +
  geom_label(aes(label = sprintf("%.2f", estimate)), fill = "white",
             colour = "black", size = 2.5,
             label.size = unit(0, "line"),
             label.padding = unit(0.2, "line")) + 
  scale_fill_distiller(palette = 3, name = "Average cohen's\nkappa across\nconditions") +
  scale_x_discrete(expand = c(0,0)) +
  scale_y_discrete(expand = c(0,0)) +
  theme(axis.text.x = element_text(angle = 30, hjust = 1.0, vjust = 1.0),
        axis.title = element_blank())
```

```{r}
#| fig.height: 3.0
#| fig.width: 4.5
#| label: fig-cohen-raters-conditions-llm-improved-subset
#| fig-cap: Average of Cohen's Kappa calculated between different groups of raters and the LLM for each specific condition and only for items 23, 7, 16, 10 and 19.
do.call(rbind, all_cohens_items) %>% 
  subset(item %in% c("Item23", "Item7", "Item10", "Item19")) %>%
  mutate(rater1 = factor(rater1, all_raters, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_raters, all_rater_labels)) %>%
  group_by(rater1, rater2) %>%
  na.omit() %>%
  summarise(estimate = sum(estimate * n) / sum(n), .groups = "drop") %>%
  ggplot(aes(x = rater1, y = rater2, fill = estimate)) + 
  geom_tile(colour = "black", linewidth = 0.5) +
  theme_classic_2(base_size = 8) +
  geom_label(aes(label = sprintf("%.2f", estimate)), fill = "white",
             colour = "black", size = 2.5,
             label.size = unit(0, "line"),
             label.padding = unit(0.2, "line")) + 
  scale_fill_distiller(palette = 3, name = "Average cohen's\nkappa across\nitems") +
  scale_x_discrete(expand = c(0,0)) +
  scale_y_discrete(expand = c(0,0)) +
  theme(axis.text.x = element_text(angle = 30, hjust = 1.0, vjust = 1.0),
        axis.title = element_blank())
```

## Analysing whether radiomic scores are similar between different rater groups and LLMs

Finally, we consider the final quantity of METRICS (i.e. a single score between 0 and 1 which quantifies how much a manuscript follows this rating). We can see that LLM-based assessments tend to over-estimate scores when relative to some readers (@fig-corr-matrix). However, when measuring correlations between all human rater groups and between LLMs and all human rater groups, these are relatively similar, hinting that there is similar variability between other reader groups (@fig-corr-matrix). Mean absolute errors are also not particularly higher than others (@fig-mae).

```{r}
#| warning: false
SCORES <- c(
    Item1 = 0.0368,
    Item2 = 0.0735,
    Item3 = 0.0919,
    Item4 = 0.0438,
    Item5 = 0.0292,
    Item6 = 0.0438,
    Item7 = 0.0292,
    Item8 = 0.0337,
    Item9 = 0.0225,
    Item10 = 0.0112,
    Item11 = 0.0622,
    Item12 = 0.0311,
    Item13 = 0.0415,
    Item14 = 0.0200,
    Item15 = 0.0200,
    Item16 = 0.0300,
    Item17 = 0.0200,
    Item18 = 0.0599,
    Item19 = 0.0300,
    Item20 = 0.0352,
    Item21 = 0.0234,
    Item22 = 0.0176,
    Item23 = 0.0117,
    Item24 = 0.0293,
    Item25 = 0.0176,
    Item26 = 0.0375,
    Item27 = 0.0749,
    Item28 = 0.0075,
    Item29 = 0.0075,
    Item30 = 0.0075
)

scores_df_long <- fuller_df %>%
  gather(key = "group", value = "value", 
         `no training_1`, `no training_2`, `no training_3`,
         `with training_1`, `with training_2`, `with training_3`,
         llm, llm_changes) %>%
  subset(grepl("Item", key)) %>%
  mutate(multiplier = ifelse(is.na(value), NA, SCORES[key]),
         score = multiplier * value) %>%
  group_by(group, title) %>%
  summarise(score = sum(score, na.rm = T) / sum(multiplier, na.rm = T),
            .groups = "drop") %>%
  mutate(group = factor(group, all_raters, all_rater_labels))

scores_df <- scores_df_long %>%
  spread(key = "group", value = score) %>%
  gather(key = "group", value = "value", 
         `No training (1)`, `No training (2)`, `No training (3)`,
         `With training (1)`, `With training (2)`, `With training (3)`) 

scores_df_wide <- scores_df_long %>% 
  spread(key = "group", value = "score")

all_corr <- list()
rater_combinations_pretty <- combn(all_rater_labels, 2)
for (idx in 1:ncol(rater_combinations_pretty)) {
  raters <- rater_combinations_pretty[, idx]
  sub_df <- scores_df_wide[,c(raters[1], raters[2])] %>%
    na.omit() %>%
    as.matrix()
  curr_df <- data.frame(estimate = cor.test(sub_df[,1], sub_df[,2])$estimate,
                        mae = mean(abs(sub_df[,1] - sub_df[,2])))
  curr_df$rater1 <- raters[1]
  curr_df$rater2 <- raters[2]
  all_corr[[length(all_corr) + 1]] <- curr_df
}
```

```{r}
#| fig.height: 4.5
#| fig.width: 6
#| warning: false
#| label: fig-corr
#| fig-cap: Association between LLM with and without changes and human rater groups.

scores_df %>%
  ggplot(aes(colour = group)) + 
  geom_abline(slope = 1) +
  geom_point(aes(x = value, y = LLM, shape = "LLM")) +
  geom_point(aes(x = value, y = `LLM + changes`, shape = "LLM + changes")) +
  geom_segment(aes(x = value, xend = value, 
                   y = `LLM`, yend = `LLM + changes`)) +
  theme_classic_2(base_size = 8) + 
  scale_shape_manual(values = c(1, 16)) +
  coord_cartesian(xlim = c(0, 1), ylim = c(0, 1)) +
  scale_colour_brewer(palette = "Set2") +
  theme(legend.key.size = unit(0, "lines"))
```

```{r}
#| fig.height: 3
#| fig.width: 4
#| warning: false
#| label: fig-corr-matrix
#| fig-cap: Correlation values (R) between LLM with and without changes and human rater groups.
do.call(rbind, all_corr) %>% 
  mutate(rater1 = factor(rater1, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_rater_labels)) %>%
  ggplot(aes(x = rater1, y = rater2, fill = estimate)) + 
  geom_tile(colour = "black", linewidth = 0.5) +
  theme_classic_2(base_size = 8) +
  geom_label(aes(label = sprintf("%.2f", estimate)), fill = "white",
             colour = "black", size = 2.5,
             label.size = unit(0, "line"),
             label.padding = unit(0.2, "line")) + 
  scale_fill_distiller(palette = 3, name = "Cohen's\nkappa") +
  scale_x_discrete(expand = c(0,0)) +
  scale_y_discrete(expand = c(0,0)) +
  theme(axis.text.x = element_text(angle = 30, hjust = 1.0, vjust = 1.0),
        axis.title = element_blank())
```

```{r}
#| fig.height: 3
#| fig.width: 4
#| warning: false
#| label: fig-mae
#| fig-cap: Correlation values (R) between LLM with and without changes and human rater groups.
do.call(rbind, all_corr) %>% 
  mutate(rater1 = factor(rater1, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_rater_labels)) %>%
  ggplot(aes(x = rater1, y = rater2, fill = mae)) + 
  geom_tile(colour = "black", linewidth = 0.5) +
  theme_classic_2(base_size = 8) +
  geom_label(aes(label = sprintf("%.2f", mae)), fill = "white",
             colour = "black", size = 2.5,
             label.size = unit(0, "line"),
             label.padding = unit(0.2, "line")) + 
  scale_fill_distiller(palette = 3, name = "Cohen's\nkappa") +
  scale_x_discrete(expand = c(0,0)) +
  scale_y_discrete(expand = c(0,0)) +
  theme(axis.text.x = element_text(angle = 30, hjust = 1.0, vjust = 1.0),
        axis.title = element_blank())
```

However, and dispite some similarities, we show that there are more statistically significant differences when performing human-LLM comparisons than when comparing human-human comparisons. We do this through a post-hoc analysis with a Tukey HSD following a statistically significant ANOVA.

```{r}
library(stats)

scores.aov <- aov(lm(score ~ group, data = scores_df_long))

print(summary(scores.aov))

tukey.hsd <- scores.aov %>%
  TukeyHSD()

tukey.hsd <- tukey.hsd$group %>%
  as.data.frame
comparisons <- rownames(tukey.hsd)

tukey.hsd %>%
  mutate(comparisons = case_when(
    !grepl("training", comparisons) ~ "LLM-LLM",
    !grepl("LLM", comparisons) ~ "Human-Human",
    .default = "Human-LLM")) %>%
  group_by(comparisons) %>%
  summarise(
    `Fraction of statistically significant Tukey HSD` = sum(`p adj` < 0.05) / length(diff),
    `Average absolute difference` = mean(abs(diff[`p adj` < 0.05])))
```

## Validation of LLM observations above with _Kocak et. al (2025)_

Around the time we were developing this, Kocak and others published "Radiomics for differentiating radiation-induced brain injury from recurrence in gliomas: systematic review, meta-analysis, and methodological quality evaluation using METRICS and RQS" in European Radiology @Kocak2025-uj. Here, three different raters (identified as raters 1-3) were tasked with classifying 27 papers using METRICS. Here, we provide LLM-based METRICS scores for only 22 as the remaining 5 were behind a paywall. It should be noted that the first author (and one of the raters as far we understand it) in _Kocak et al. (2025)_ was also a co-author in @Akinci-D-Antonoli2025-ep and the first author of the original METRICS definition paper @Kocak2024-wk.

As visible below across @fig-cohen-kocak, @fig-cohen-kocak-items and @fig-cohen-kocak-conditions, the inter-rater agreements are relatively stable between readers and between readers and LLMs. An interesting outcome of this is that the LLM + changes do not lead to such stark improvements as before, hinting that dataset-specific prompt tuning may not be the best way of improving scores across multiple publications.

```{r}
df_original <- read_csv(
  "../data/kocak/inter-rater.csv", col_types = c(rep("c", 40), "i", "i")) 
df <- df_original %>% 
  gather(key = "key", value = "value", 
         -rater, -dois, -dois_clean, -title) %>% 
  subset(key != "Total METRICS score:") %>%
  subset(!grepl("Quality", key)) %>%
  mutate(key = gsub("#", "", key)) %>%
  mutate(value = match_class[value]) %>%
  mutate(rater = sprintf("Rater%s", rater))

df_original
```

```{r}
llm_rating_files <- list.files(path = "../ratings/kocak", pattern = "*json", full.names = T)
names(llm_rating_files) <- lapply(
  llm_rating_files, function(x) gsub(".json", "", str_split(x, "/")[[1]][4])
)

all_llm_ratings <- list()
for (f in llm_rating_files) {
  title <- gsub(".json", "", str_split(f, "/")[[1]][4])
  json_data <- fromJSON(file = f)
  json_df <- data.frame(
    key = names(json_data[-1]), 
    llm = unlist(lapply(json_data[-1], function(x) x$rating)), title = title)
  all_llm_ratings[[length(all_llm_ratings) + 1]] <- json_df
}

llm_df <- do.call(rbind, all_llm_ratings) %>%
  mutate(llm = match_class[llm]) %>%
  mutate(title = gsub("_", "/", title))

improved_llm_rating_files <- list.files(path = "../ratings_improved/kocak", 
                                        pattern = "*json", full.names = T)
names(improved_llm_rating_files) <- lapply(
  improved_llm_rating_files, function(x) gsub(".json", "", str_split(x, "/")[[1]][4])
)

all_improved_llm_ratings <- list()
for (f in improved_llm_rating_files) {
  title <- gsub(".json", "", str_split(f, "/")[[1]][4])
  json_data <- fromJSON(file = f)
  json_df <- data.frame(
    key = names(json_data[-1]), 
    llm_changes = unlist(lapply(json_data[-1], function(x) x$rating)), title = title)
  all_improved_llm_ratings[[length(all_improved_llm_ratings) + 1]] <- json_df
}

improved_llm_df <- do.call(rbind, all_improved_llm_ratings) %>%
  mutate(llm_changes = match_class[llm_changes]) %>%
  mutate(title = gsub("_", "/", title))

full_df <- merge(
  pivot_wider(df, values_from = value, names_from = c("rater")), 
  llm_df, 
  by = c("key", "title"))

fuller_df <- merge(
  full_df,
  improved_llm_df, 
  by = c("key", "title"))
```

```{r}
#| warning: false
all_raters <- c("Rater1", "Rater2", "Rater3",
                "llm", "llm_changes")
all_rater_labels <- c(
  "Rater 1", "Rater 2", "Rater 3",
  "LLM", "LLM + changes"
)

rater_combinations <- combn(all_raters, 2)

all_cohens <- list()
for (idx in 1:ncol(rater_combinations)) {
  raters <- rater_combinations[, idx]
  sub_df <- subset(fuller_df, !grepl("Condition", key))[c(raters[1], raters[2])] %>%
    na.omit() %>%
    as.matrix()
  if ((nrow(sub_df) > 10) & (var(sub_df[,1]) > 0) & (var(sub_df[,2]) > 0)) { # guarantee that at least 10 articles were graded
    curr_df <- data.frame(estimate = kappa2(sub_df)$value)
    curr_df$rater1 <- raters[1]
    curr_df$rater2 <- raters[2]
    all_cohens[[length(all_cohens) + 1]] <- curr_df
  }
}

all_cohens_items <- list()
for (k in unique(fuller_df$key)) {
  for (idx in 1:ncol(rater_combinations)) {
    raters <- rater_combinations[, idx]
    sub_df <- subset(fuller_df, key == k)[c(raters[1], raters[2])] %>%
      na.omit() %>%
      as.matrix()
    if ((nrow(sub_df) > 10) & (var(sub_df[,1]) > 0) & (var(sub_df[,2]) > 0)) { # guarantee that at least 10 articles were graded
      curr_df <- data.frame(
        estimate = kappam.fleiss(sub_df)$value)
      curr_df$rater1 <- raters[1]
      curr_df$rater2 <- raters[2]
      curr_df$item <- k
      curr_df$n <- nrow(sub_df)
      all_cohens_items[[length(all_cohens_items) + 1]] <- curr_df 
    }
  }
}
```

```{r fig-cohen-raters-llm}
#| fig.height: 3.0
#| fig.width: 4.5
#| label: fig-cohen-kocak
#| fig-cap: Cohen's Kappa calculated across all items (1-30) between different groups of raters and the LLM for _Kocak et al. (2025)_.
do.call(rbind, all_cohens) %>% 
  mutate(rater1 = factor(rater1, all_raters, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_raters, all_rater_labels)) %>%
  ggplot(aes(x = rater1, y = rater2, fill = estimate)) + 
  geom_tile(colour = "black", linewidth = 0.5) +
  theme_classic_2(base_size = 8) +
  geom_label(aes(label = sprintf("%.2f", estimate)), fill = "white",
             colour = "black", size = 2.5,
             label.size = unit(0, "line"),
             label.padding = unit(0.2, "line")) + 
  scale_fill_distiller(palette = 3, name = "Cohen's\nkappa") +
  scale_x_discrete(expand = c(0,0)) +
  scale_y_discrete(expand = c(0,0)) +
  theme(axis.text.x = element_text(angle = 30, hjust = 1.0, vjust = 1.0),
        axis.title = element_blank())
```

```{r fig-cohen-raters-items-llm}
#| fig.height: 3.0
#| fig.width: 4.5
#| label: fig-cohen-kocak-items
#| fig-cap: Average of Cohen's Kappa calculated between different groups of raters and the LLM for each specific item for _Kocak et al. (2025)_.
do.call(rbind, all_cohens_items) %>% 
  subset(grepl("Item", item)) %>%
  mutate(rater1 = factor(rater1, all_raters, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_raters, all_rater_labels)) %>%
  group_by(rater1, rater2) %>%
  na.omit() %>%
  summarise(estimate = sum(estimate * n) / sum(n), .groups = "drop") %>%
  ggplot(aes(x = rater1, y = rater2, fill = estimate)) + 
  geom_tile(colour = "black", linewidth = 0.5) +
  theme_classic_2(base_size = 8) +
  geom_label(aes(label = sprintf("%.2f", estimate)), fill = "white",
             colour = "black", size = 2.5,
             label.size = unit(0, "line"),
             label.padding = unit(0.2, "line")) + 
  scale_fill_distiller(palette = 3, name = "Average cohen's\nkappa across\nitems") +
  scale_x_discrete(expand = c(0,0)) +
  scale_y_discrete(expand = c(0,0)) +
  theme(axis.text.x = element_text(angle = 30, hjust = 1.0, vjust = 1.0),
        axis.title = element_blank())
```

```{r fig-cohen-raters-conditions-llm}
#| fig.height: 3.0
#| fig.width: 4.5
#| label: fig-cohen-kocak-conditions
#| fig-cap: Average of Cohen's Kappa calculated between different groups of raters and the LLM for each specific condition for _Kocak et al. (2025)_.
do.call(rbind, all_cohens_items) %>% 
  subset(grepl("Condition", item)) %>%
  mutate(rater1 = factor(rater1, all_raters, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_raters, all_rater_labels)) %>%
  group_by(rater1, rater2) %>%
  na.omit() %>%
  summarise(estimate = sum(estimate * n) / sum(n), .groups = "drop") %>%
  ggplot(aes(x = rater1, y = rater2, fill = estimate)) + 
  geom_tile(colour = "black", linewidth = 0.5) +
  theme_classic_2(base_size = 8) +
  geom_label(aes(label = sprintf("%.2f", estimate)), fill = "white",
             colour = "black", size = 2.5,
             label.size = unit(0, "line"),
             label.padding = unit(0.2, "line")) + 
  scale_fill_distiller(palette = 3, name = "Average cohen's\nkappa across\nconditions") +
  scale_x_discrete(expand = c(0,0)) +
  scale_y_discrete(expand = c(0,0)) +
  theme(axis.text.x = element_text(angle = 30, hjust = 1.0, vjust = 1.0),
        axis.title = element_blank())
```

# Conclusion and where to go from here

This was a somewhat quick analysis prompted by a concrete question --- can we facilitate what are oftentimes cumbersome analyses by getting the assistance of an LLM? The answer to this --- at least what this analysis shows --- is that:

1. There is a reasonable amount of agreement between LLMs and human rater groups, but there is also a reasonable amount of differences between human rater groups and LLMs. This is not absurd as this system in particular --- METRICS --- relies quite heavily on expert knowledge, and the paper that originated this short report highlights, to an extent, the differences in subjective evaluation associated with human raters @Akinci-D-Antonoli2025-ep. While more objective, LLM-based assessments may end up being too different. However, our further confirmation using @Kocak2025-uj does show that these findings --- that there is a good number of similarities between human raters and LLMs --- generalise to other datasets.
2. Improvements are possible from relatively simple analyses which place the human in the loop, suggesting that more data and further optimization can lead to a robust automated METRICS assessment system. While automated prompt engineering could be an alternative, there is a significant amount of work which goes into the annotation of these manuscripts, making these approaches --- which require a good number of examples --- quite tricky to handle when sample sizes are small. Our analysis on @Kocak2025-uj showed that prompt tuning using small sample sizes is likely to result in improvements which do not generalise. However, both the tuning and its validation used relatively small dataset sizes.