---
title: "A short report on automating METRICS"
code-line-numbers: true
format: 
  html:
    df-print: paged
    toc: true
    toc-location: left
    fig-format: png
    fig-dpi: 300
editor: visual
bibliography: bibliography.bib
csl: "ieee.csl"
author: Jos√© Guilherme de Almeida
footnotes-hover: true
reference-location: margin
code-fold: true
code-overflow: wrap

include-in-header:
  - text: |
      <style>
        .cell-output-stdout code {
          word-break: break-wor !important;
          white-space: pre-wrap !important;
          max-height: 600px;
        }
        .cell-output-stdout {
          background-color: #f0eded;
          border-radius: 4px;
        }
      </style>
---

# TL;DR

It is possible --- to some extent --- to automate METRICS (a radiomics publication quality assessment tool) using large language models. You can test this [here](https://auto-metrics.netlify.app/) with your own Gemini API key.

# Context

Here we show a proof of concept on how it is possible to automate some aspects of scientific analysis. In particular, we will be doing this through the Methodological Radiomics Score (otherwise known as METRICS) @Kocak2024-wk. METRICS comprises a set of helpful guidelines to determine the quality of a scientific publication doing some form of radiomics analysis (i.e. the systematic extraction of imaging features from clinical images and the correlation of these features with patient-specific outcomes or image characteristics @Van_Timmeren2020-gy). These sorts of scores are not new, with the radiomics quality score (RQS) being the other main contender for scoring radiomics @Lambin2017-rn. Whether METRICS is more or less useful than RQS has been addressed in other publications and it is not the particular concern of this piece to clarify it @Kocak2024-sv. Instead, here we make use of a recent publication on the reproducibility of METRICS scores: Akinci D'Antonoli and others' "Reproducibility of methodological radiomics score (METRICS): an intra- and inter-rater reliability study endorsed by EuSoMII" @Akinci-D-Antonoli2025-ep.

## Short summary of *Akinci D'Antonoli and others*

In this work, a set of radiologists was divided into three separate experience levels --- from 1 (less experienced) to 3 (more experienced) - based on their experience not only as radiologists but also as academics. Two experimental conditions were tested --- no training on METRICS and with training with METRICS. The overall experimental design is sound and the conclusions are quite interesting: in essence, there is low-to-moderate inter-rater reliability, and this is particularly true when comparing between no training and with training [^1].

[^1]: The article also features repeatibility experiments, but that is not within the scope of this short report.

## Visualizing reproducibility results from Akinci D'Antonoli and others

Here we are providing only a recapitulation of what is provided as Supplementary Information 1. In particular, here we only use the Cohen's Kappa to analyse how agreement varies between different conditions and raters. As observable in @fig-cohen-raters, Cohen's Kappa (a measure of inter-rater agreement) is quite consistent and between 0.4 and 0.6. The one exception is the very high Kappa between experience level 3 (highest experience level) with and without training. This may highlight the effect of experience or it may be biased as the four radiologists annotated with experience level 3 are either editors/members of the editorial board of European Radiology (and are more exposed to this sort of analysis) or took part in developing METRICS (AP/AS/MK and LU, respectively); only three other authors are in this position of being parts of editorial teams at radiology journals/publications or having participated in METRICS. In any case it quite a stark difference in my opinion.

Further analyzing how this looks for items and conditions separetely (i.e. calculating the average item-specific Cohen's Kappa) further confirms the overall analysis as illustrated in @fig-cohen-raters B. Finally, it is interesting to see that there is little agreement as far as determining the different conditions is concerned @fig-cohen-raters C. This part is crucial as some items depend on specific conditions being met for grading, otherwise they are discarded.

```{r}
#| output: false
library(tidyverse)
library(rjson)
library(irr)
library(knitr)
library(kableExtra)
library(patchwork)
library(psych)
library(stats)

theme_classic_2 <- function(base_size = 11,
                            base_family = "",
                            base_line_size = 11/22,
                            base_rect_size = 11/22) {
  theme_classic(base_size = base_size,
                base_family = base_family,
                base_line_size = base_line_size,
                base_rect_size = base_rect_size) %>%
    theme(axis.text = element_text(size = base_size,
                                   colour = "black"),
          title = element_text(size = base_size + 1),
          axis.title = element_text(size = base_size),
          strip.background = element_blank(),
          strip.text = element_text(size = base_size,
                                    face = "bold",
                                    hjust = 0.0,
                                    margin = margin(b = 2)),
          legend.title = element_text(size = base_size, face = "bold"),
          panel.background = element_blank(),
          legend.text = element_text(size = base_size),
          axis.line = element_line(),
          axis.ticks = element_line(),
          panel.grid = element_blank()) %>%
    return
}

make_heatmap <- function(df) {
  plot_output <- ggplot(df, aes(x = rater1, y = rater2, fill = estimate)) + 
    geom_tile(colour = "black", linewidth = 0.5) +
    theme_classic_2(base_size = 8) +
    geom_label(aes(label = sprintf("%.2f", estimate)), 
               fill = "white",
               colour = "black", 
               size = 2.5,
               label.size = unit(0, "line"),
               label.padding = unit(0.3, "line"),
               vjust = 0,
               alpha = 0.7) + 
    geom_label(aes(label = sprintf("[%.2f, %.2f]", lower, upper)), 
               fill = "white",
               colour = "black", 
               size = 2.5,
               label.size = unit(0, "line"),
               label.padding = unit(0.3, "line"),
               vjust = 1.0,
               alpha = 0.7) + 
    scale_x_discrete(expand = c(0,0)) +
    scale_y_discrete(expand = c(0,0)) +
    theme(axis.title = element_blank(),
          legend.position = "bottom",
          legend.key.height = unit(0.5, "line"),
          legend.title = element_text(vjust = 1.0),
          legend.margin = margin())
  return(wrap_plots(plot_output))
}

skip_columns <- c(
  "elapsed_time", 
  "Summary",
  "error",
  "prompt",
  "article_text",
  "metadata")

match_class <- c(`n/a` = NA, no = 0, yes = 1)
key_order <- c(paste0("Item", 1:30), paste0("Condition", 1:30))
```

```{r data-loading}
#| warning: false
all_dfs <- list()
df_original <- read_csv(
  "../data/akinci-dantonoli/inter-rater.csv", col_types = c(rep("c", 40), "i", "i")) 
df <- df_original %>% 
  gather(key = "key", value = "value", 
         -group, -exp, -dois, -dois_clean, -title) %>% 
  subset(key != "Total METRICS score:") %>%
  subset(!grepl("Quality", key)) %>%
  mutate(key = gsub("#", "", key)) %>%
  mutate(group = ifelse(group == 1, "no training", "with training")) %>%
  mutate(value = match_class[value])

all_dfs$akinci_dantonoli <- df

df_original
```

```{r data-processing}
#| warning: false

all_raters <- c("no training_1","no training_2","no training_3",
                "with training_1","with training_2","with training_3")
all_rater_labels <- c(
  "Not\ntrained (1)", "Not\ntrained (2)", "Not\ntrained (3)",
  "Trained (1)", "Trained (2)", "Trained (3)")

long_df <- pivot_wider(df, values_from = value, names_from = c("group", "exp"))

rater_combinations <- combn(all_raters, 2)

all_cohens <- list()
for (idx in 1:ncol(rater_combinations)) {
  raters <- rater_combinations[, idx]
  sub_df <- subset(long_df, !grepl("Condition", key))[c(raters[1], raters[2])] %>%
    na.omit() %>%
    as.matrix()
  if ((nrow(sub_df) > 10) & (var(sub_df[,1]) > 0) & (var(sub_df[,2]) > 0)) { # guarantee that at least 10 articles were graded
    kk <- as.data.frame(cohen.kappa(sub_df)$confid)
    curr_df <- kk
    curr_df$rater1 <- raters[1]
    curr_df$rater2 <- raters[2]
    all_cohens[[length(all_cohens) + 1]] <- curr_df
  }
}

all_cohens_items <- list()
for (k in unique(long_df$key)) {
  for (idx in 1:ncol(rater_combinations)) {
    raters <- rater_combinations[, idx]
    sub_df <- subset(long_df, key == k)[c(raters[1], raters[2])] %>%
      na.omit() %>%
      as.matrix()
    if ((nrow(sub_df) > 10) & (var(sub_df[,1]) > 0) & (var(sub_df[,2]) > 0)) { # guarantee that at least 10 articles were graded
      kk <- as.data.frame(cohen.kappa(sub_df)$confid)
      curr_df <- kk
      curr_df$rater1 <- raters[1]
      curr_df$rater2 <- raters[2]
      curr_df$item <- k
      curr_df$n <- nrow(sub_df)
      all_cohens_items[[length(all_cohens_items) + 1]] <- curr_df 
    }
  }
}
```

```{r}
cohens_overall <- do.call(rbind, all_cohens) %>% 
  mutate(rater1 = factor(rater1, all_raters, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_raters, all_rater_labels))

cohens_average_items <- do.call(rbind, all_cohens_items) %>% 
  subset(grepl("Item", item)) %>%
  mutate(rater1 = factor(rater1, all_raters, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_raters, all_rater_labels)) %>%
  group_by(rater1, rater2) %>%
  na.omit() %>%
  summarise(se = sd(estimate) / sqrt(length(estimate)),
            estimate = mean(estimate), 
            lower = max(estimate - abs(qnorm(0.025)) * se, 0),
            upper = min(estimate + abs(qnorm(0.025)) * se, 1),
            .groups = "drop")

cohens_average_conditions <- do.call(rbind, all_cohens_items) %>% 
  subset(grepl("Condition", item)) %>%
  mutate(rater1 = factor(rater1, all_raters, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_raters, all_rater_labels)) %>%
  group_by(rater1, rater2) %>%
  na.omit() %>%
  summarise(se = sd(estimate) / sqrt(length(estimate)),
            estimate = mean(estimate), 
            lower = max(estimate - abs(qnorm(0.025)) * se, 0),
            upper = min(estimate + abs(qnorm(0.025)) * se, 1),
            .groups = "drop")

cohens_overall_plot <- make_heatmap(cohens_overall) +
  scale_fill_distiller(palette = 3, name = "Cohen's Kappa")

cohens_average_items_plot <- make_heatmap(cohens_average_items) +
  scale_fill_distiller(palette = 3, name = "Average Cohen's\nKappa across items")

cohens_average_conditions_plot <- make_heatmap(cohens_average_conditions) +
  scale_fill_distiller(palette = 3, name = "Average Cohen's\nKappa across conditions")
```

```{r fig-cohen-raters}
#| fig.height: 10.0
#| fig.width: 5.0
#| label: fig-cohen-raters
#| fig-cap: Cohen's Kappa for Akinci D'Antonoli and others (2025).
(cohens_overall_plot / 
  cohens_average_items_plot /
  cohens_average_conditions_plot) +
  plot_annotation(tag_levels = c("A"))
```

# LLMs for scientific assessment --- replacement or sidekick?

Given this rich dataset --- which can be easily linked to the relevant literature --- we had a simple thought: how easily can this be performed automatically? With the advent of large language models (LLMs), there are ample opportunities for this. Indeed, combining LLMs with relatively deterministic structured generation can ensure that i) we always get the same results and ii) we always get structured results. The first can be achieved through 0 temperature sampling (i.e. no randomness given the same input) and constrained generation (i.e. the LLM can only generate tokens which force it to satisfy a set of constraints).

Here, we have a concrete hypothesis: is the average agreement between an LLM and human rater groups similar to that which is observed between human rater groups? If so, this puts LLMs as a possible second read as far as METRICS assessments are concerned.

## The protocol

We used Gemini 2.0 Flash because it is remarkably cheap for these sorts of experiments (and in general). As a prompt, we use the following (it is somewhat long so bear with us, it copies the guidelines made available in the original METRICS paper and in the [METRICS helper website](https://metricsscore.github.io/metrics/METRICS.html)):

```{r}
read_file("prompt.txt") %>%
  cat
```

We start by detailing some instructions that the LLM should follow. Then, we provide clear evaluation metrics and a rating rubric which should be followed. Finally, we declare the input format and make space for the actual METRICS guidelines and output format. Where we wrote `{article}`, that is where the text is inserted. To get these article texts we went through the articles which we could access --- between publicly available and institutionally-accessible articles we were able to access 31 out of the total of 34 --- and simply copied the text between the abstract (incl.) and the references (excl.) for each.

To ensure structured outputs, we make use of a relatively strict --- but simple! --- data model for the output:

```{python}
#| echo: true
#| eval: false
from enum import Enum
from pydantic import BaseModel


class RatingEnum(Enum):
    yes = "yes"
    no = "no"


class RatingWithNAEnum(Enum):
    yes = "yes"
    no = "no"
    na = "n/a"


class Rating(BaseModel):
    rating: RatingEnum
    reason: str


class RatingWithNA(BaseModel):
    rating: RatingWithNAEnum
    reason: str


class Metrics(BaseModel):
    Summary: str
    Condition1: Rating
    Condition2: Rating
    Condition3: Rating
    Condition4: Rating
    Condition5: Rating
    Item1: Rating
    Item2: Rating
    Item3: Rating
    Item4: Rating
    Item5: Rating
    Item6: Rating
    Item7: Rating
    Item8: RatingWithNA
    Item9: RatingWithNA
    Item10: RatingWithNA
    Item11: Rating
    Item12: RatingWithNA
    Item13: Rating
    Item14: RatingWithNA
    Item15: RatingWithNA
    Item16: RatingWithNA
    Item17: RatingWithNA
    Item18: Rating
    Item19: Rating
    Item20: Rating
    Item21: Rating
    Item22: Rating
    Item23: Rating
    Item24: Rating
    Item25: Rating
    Item26: Rating
    Item27: Rating
    Item28: Rating
    Item29: Rating
    Item30: Rating
```

## The tests

Upon loading, we were able to verify that all the LLM outputs have, as expected, generated a correctly formatted JSON. Below we present an example for one such article. Additionally, the flexibility of our specification also allows for a small reason to be appended to each rating --- this facilitates the verification of LLM claims. Indeed, this explanation ends up being helpful at a later stage of this analysis when we start getting into possible reasons for disagreement.

```{r}
llm_rating_files <- list.files(path = "../ratings/gemini/akinci-dantonoli", pattern = "*json", full.names = T)
names(llm_rating_files) <- lapply(
  llm_rating_files, function(x) gsub(".json", "", str_split(x, "/")[[1]][5])
)

file_to_display <- read_file(file = llm_rating_files[[1]]) %>%
  fromJSON
file_to_display <- file_to_display[names(file_to_display) != "metadata"]

cat(sprintf("Loading and printing the JSON for %s:\n\n%s", 
            names(llm_rating_files)[[1]],
            toJSON(file_to_display)))
```

Upon analysis of @fig-cohen-raters-llm we see quite clearly that, while there is some agreement between LLMs and raters, this is very much on the lower end of the agreement spectrum which is observed for human rater groups. Indeed, it can be risky to simply deployed this sans supervision. However, we note that this is still well within what would be acceptable had this been an additional reviewer. As noted earlier, it might be a good format to have an LLM perform these assessments as a second reader which ends up reinforcing some conclusions or reanalysing others.

```{r}
all_llm_ratings <- list()
for (f in llm_rating_files) {
  title <- gsub(".json", "", str_split(f, "/")[[1]][5])
  json_data <- fromJSON(file = f)
  json_data <- json_data[!(names(json_data) %in% skip_columns)]
  json_df <- data.frame(
    key = names(json_data[-1]), 
    llm = unlist(lapply(json_data[-1], function(x) x$rating)), title = title)
  all_llm_ratings[[length(all_llm_ratings) + 1]] <- json_df
}

llm_df <- do.call(rbind, all_llm_ratings) %>%
  mutate(llm = match_class[llm]) %>%
  mutate(title = gsub("_", "/", title))

# redefine rater vectors to include LLMs
all_raters <- c("no training_1","no training_2","no training_3",
                "with training_1","with training_2","with training_3",
                "llm")
all_rater_labels <- c(
  "Not\ntrained (1)", "Not\ntrained (2)", "Not\ntrained (3)",
  "Trained (1)", "Trained (2)", "Trained (3)",
  "LLM"
)

full_df <- merge(
  pivot_wider(df, values_from = value, names_from = c("group", "exp")), 
  llm_df, 
  by = c("key", "title"))
```

```{r}
#| warning: false
rater_combinations <- combn(all_raters, 2)

all_cohens <- list()
for (idx in 1:ncol(rater_combinations)) {
  raters <- rater_combinations[, idx]
  sub_df <- subset(full_df, !grepl("Condition", key))[c(raters[1], raters[2])] %>%
    na.omit() %>%
    as.matrix()
  if ((nrow(sub_df) > 10) & (var(sub_df[,1]) > 0) & (var(sub_df[,2]) > 0)) { # guarantee that at least 10 articles were graded
    kk <- as.data.frame(cohen.kappa(sub_df)$confid)
    curr_df <- kk
    curr_df$rater1 <- raters[1]
    curr_df$rater2 <- raters[2]
    all_cohens[[length(all_cohens) + 1]] <- curr_df
  }
}

all_cohens_items <- list()
for (k in unique(full_df$key)) {
  for (idx in 1:ncol(rater_combinations)) {
    raters <- rater_combinations[, idx]
    sub_df <- subset(full_df, key == k)[c(raters[1], raters[2])] %>%
      na.omit() %>%
      as.matrix()
    if ((nrow(sub_df) > 10) & (var(sub_df[,1]) > 0) & (var(sub_df[,2]) > 0)) { # guarantee that at least 10 articles were graded
      kk <- as.data.frame(cohen.kappa(sub_df)$confid)
      curr_df <- kk
      curr_df$rater1 <- raters[1]
      curr_df$rater2 <- raters[2]
      curr_df$item <- k
      curr_df$n <- nrow(sub_df)
      all_cohens_items[[length(all_cohens_items) + 1]] <- curr_df 
    }
  }
}
```

```{r}
cohens_overall <- do.call(rbind, all_cohens) %>% 
  mutate(rater1 = factor(rater1, all_raters, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_raters, all_rater_labels))

cohens_average_items <- do.call(rbind, all_cohens_items) %>% 
  subset(grepl("Item", item)) %>%
  mutate(rater1 = factor(rater1, all_raters, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_raters, all_rater_labels)) %>%
  group_by(rater1, rater2) %>%
  na.omit() %>%
  summarise(se = sd(estimate) / sqrt(length(estimate)),
            estimate = mean(estimate), 
            lower = max(estimate - abs(qnorm(0.025)) * se, 0),
            upper = min(estimate + abs(qnorm(0.025)) * se, 1),
            .groups = "drop")

cohens_average_conditions <- do.call(rbind, all_cohens_items) %>% 
  subset(grepl("Condition", item)) %>%
  mutate(rater1 = factor(rater1, all_raters, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_raters, all_rater_labels)) %>%
  group_by(rater1, rater2) %>%
  na.omit() %>%
  summarise(se = sd(estimate) / sqrt(length(estimate)),
            estimate = mean(estimate), 
            lower = max(estimate - abs(qnorm(0.025)) * se, 0),
            upper = min(estimate + abs(qnorm(0.025)) * se, 1),
            .groups = "drop")

cohens_overall_plot <- make_heatmap(cohens_overall) +
  scale_fill_distiller(palette = 3, name = "Cohen's Kappa")

cohens_average_items_plot <- make_heatmap(cohens_average_items) +
  scale_fill_distiller(palette = 3, name = "Average Cohen's\nKappa across items")

cohens_average_conditions_plot <- make_heatmap(cohens_average_conditions) +
  scale_fill_distiller(palette = 3, name = "Average Cohen's\nKappa across conditions")
```

```{r fig-cohen-raters}
#| fig.height: 9.0
#| fig.width: 5.0
#| label: fig-cohen-raters-llm
#| fig-cap: Cohen's Kappa for Akinci D'Antonoli and others (2025) with LLM.
(cohens_overall_plot / 
  cohens_average_items_plot /
  cohens_average_conditions_plot) +
  plot_annotation(tag_levels = c("A"))
```

In @fig-proportions we show how these scores are somewhat variable when analysed on a per-item basis. While some items are somewhat consistent (Items 1, 2, 20, 21, 22, 29 and 30 all have percent agreements[^2] \> 70%), most are relatively scattered.

[^2]: here we used percent agreements as opposed to Cohen's Kappa because the latter is not defined when data has 0 variance (some questions were always answered the same) and has unintuitive behavior when it gets too close to 0 or 1.

```{r}
#| fig.height: 5
#| fig.width: 6
#| warning: false
#| fig-cap: Proportion of agreement between human readers and the LLM METRICS assessment stratified by training groups and experience levels.
#| label: fig-proportions
proportion_data <- merge(
  df,
  llm_df, 
  by = c("key", "title")) %>% 
  group_by(key, group, exp) %>% 
  summarise(capture = mean(value == llm, na.rm = T), .groups = "drop") %>%
  mutate(key = factor(key, rev(key_order))) %>%
  mutate(cond_or_item = ifelse(grepl("Condition", key), "Condition", "Item"))

proportion_data %>%
  ggplot(aes(x = capture, y = key, colour = as.factor(exp),
             shape = group, group = paste(exp, group))) + 
  geom_point() + 
  theme_classic_2(base_size = 8) +
  theme(panel.grid.major.y = element_line(color = "grey90"),
        legend.key.size = unit(0, "line"),
        strip.text.y = element_text(angle = 0)) +
  xlab("Proportion of agreements with LLM") +
  ylab("Item") +
  facet_grid(cond_or_item ~ ., scales = "free", space = "free", ) + 
  scale_colour_brewer(palette = "Set1",
                      name = "Experience level") +
  scale_shape_manual(values = c(2, 3), name = "Training") +
  guides(x = guide_axis(cap = T))
```

## Analysing some specific cases

My main point of interest now was getting to know how these differences are manifested. As such, we will focus on cases where the number of disagreements with human raters is relatively large --- in other words, we want to know if there are actual intuitive reasons behind LLM behavior. We consider here that agreements are worth 1 point, disagreements -1, and incompatibilities (i.e. LLM provided answer but answer for human raters is N/A) is 0.

A main point of this analysis is for us to think through of some possible improvements that can be added to the description of each item to make it as clear as possible for automated assessment.

```{r}
#| fig.height: 5
#| fig.width: 4
score_agreement <- function(a, b) {
  d <- case_when(
    is.na(a) | is.na(b) ~ NA,
    a != b ~ -1,
    a == b ~ 1
  )
  return(d)
}

sum_na_if_empty <- function(x) {
  if (length(x) == 0) {
    return(NA)
  }
  return(sum(x))
}

disagreement_df <- full_df %>%
  rowwise() %>%
  mutate(
    n_disagreements = c(
      score_agreement(`no training_1`, llm),
      score_agreement(`no training_2`, llm),
      score_agreement(`no training_3`, llm),
      score_agreement(`with training_1`,llm),
      score_agreement(`with training_2`, llm), 
      score_agreement(`with training_3`, llm)
    ) %>%
      Filter(f = function(x) !is.na(x)) %>%
      sum_na_if_empty
  ) %>%
  arrange(n_disagreements)

disagreement_df %>%
  group_by(key) %>%
  summarise(average_disagreement_score = mean(n_disagreements, na.rm = T)) %>%
  arrange(average_disagreement_score) %>%
  ggplot(aes(x = average_disagreement_score,
             y = reorder(key, average_disagreement_score))) + 
  geom_point() +
  theme_classic_2(base_size = 8) +
  ylab("METRICS item/condition") +
  xlab("Average agreement\n(negative = disagreement, positive = agreement)")
```

### Item23

*Use of uni-parametric imaging or proof of its inferiority*

As in the table above, we can see that the average disagreement score (the average of the scoring system noted above) is more severe for Item23 - whether uni-parametric imaging was used or proved to be inferior to multi-parametric imaging. Focusing on this, we take five papers at random where all reader groups said "yes" while the LLM said "no". We bring up the reasons provided by the LLM to further clarify this, and inspect each paper individually to better understand what the reason was. By and large, two dominant reasons appear to arise:

1.  The LLM mistakenly assumes that using multiple image transformations during radiomic feature extraction counts as "multi-parametric imaging"
2.  The LLM mistakenly interprets the paper and assumes that there are other categories apart from "uni-parametric imaging" and "multi-parametric imaging"

#### Potential improvement

Including the following in the item helper: "Uni-parametric imaging implies the acquisition of a single imaging modality, as opposed to multi-parametric imaging, which involves the acquisition of multiple imaging modalities."

```{r}
set.seed(42)

get_examples_of_disagreement <- function(item_number, n_examples = 5) {
  item_subset <- disagreement_df %>%
    subset(key == sprintf("Item%s", item_number)) %>%
    arrange(n_disagreements) 
  
  if (nrow(item_subset) < n_examples) {
    n_examples <- nrow(item_subset)
  }
  relevant_titles <- item_subset$title[1:n_examples]
  
  output <- list()
  for (tt in relevant_titles) {
    reason <- fromJSON(file = llm_rating_files[[gsub("/", "_", tt)]])[[sprintf("Item%s", item_number)]]
    reason_number <- match_class[reason$rating]
    human_raters <- subset(item_subset, title == tt)
    human_raters <- unlist(
      human_raters[, Filter(function(x) grepl("training", x), colnames(human_raters))]) %>%
      paste(collapse = ", ")
    output[[length(output) + 1]] <- tibble(
      Title = tt,
      `Human ratings` = human_raters,
      `LLM rating` = reason_number,
      `LLM reason` = unlist(reason[[2]])
    )
  }
  do.call(what = rbind, output)
}

get_examples_of_disagreement(23)
```

### Item16

*Appropriateness of dimensionality compared to data size*

This is something which was somewhat expected --- the METRICS question referring to appropriate data dimensionality does not define this in exact terms. Consequently, Item16 is typically hard to understand @Akinci-D-Antonoli2025-ep.

#### Potential improvement

N/A

```{r}
get_examples_of_disagreement(16)
```

### Item7

*The interval between imaging used and reference standard*

Disagreements for this item are somewhat interesting --- upon closer inspection of any of the papers mentioned below, there are no errors from the LLM. However, the LLM considers these as "yes", but most radiologist groups considered this to be the opposite. We found that the interval between imaging and reference standard is acceptable (except for "Application of a comprehensive model based on CT radiomics and clinical features for postoperative recurrence risk prediction in non-small cell lung cancer", where this interval is not mentioned) and follows the helping criteria [^3]; however, there may be slight aspects of METRICS which we are not fully understanding.

[^3]: "Whether the time interval between the diagnostic imaging exams (used as an input for the radiomics analysis) and the outcome measure/reference standard acquisition is appropriate to validate the presence or absence of target conditions of the radiomics analysis at the moment of the diagnostic imaging exams."

#### Potential improvement

Inclusion of the following in the item helper: "If there is no mention of when the reference standard was acquired relative to the diagnostic imaging exams, the answer should be"no"."

```{r}
get_examples_of_disagreement(7)
```

### Item10

*Test set segmentation masks produced by a single reader or automated tool*

Here, the LLM considers a senior radiologist reviewing segmentations to be standard clinical practice, and given that the helper note[^4] does not specifically address what "clinical practice" is or isn't, the LLM (very wrongfully) assumes that segmentation reviews are a standard part of it.

[^4]: "Whether final segmentation in the test set is produced by a single reader (manually or with a semi-automated tool) or an entirely automated tool, to better reflect clinical practice."

#### Potential improvement

Inclusion of the following in the item helper: "Keep in mind that having a radiologist review the results of the automated segmentation is not considered clinical practice."

```{r}
get_examples_of_disagreement(10)
```

### Item19

*Handling of confounding factors*

Here, the disagreements are, as far as I can tell, stemming from a difficulty I myself face when considering questions such as this. The helper note for this item [^5] is somewhat hard to consider in strict terms. For instance:

[^5]: "Whether potential confounding factors were analyzed, identified if present, and removed if necessary (e.g., if it has a strong influence on generalizability). These may include different distributions of patient characteristics (e.g., gender, lesion stage or grade) across sites or scanners."

-   In 'A Radiomic "Warning Sign" of Progression on Brain MRI in Individuals with MS', the authors offer an analysis of a pipeline-specific confounder which could affect how predictions were obtained or are interpreted. However, this confounder does not represent anything inherent to the patient or to the acquisition process, but rather to the algorithm developed during this work
-   In "CNN-based multi-modal radiomics analysis of pseudo-CT utilization in MRI-only brain stereotactic radiotherapy: a feasibility study", the authors do offer an analysis of how MRI contrast presence affects model performance, but then again this is not really a confounder in the traditional sense
-   In the remaining articles were the LLM answered "yes" and reader groups tended towards "no", there are some more confounding factors which *could* be considered confounders but it is understandable that they are not *necessarily* regarded as such by most people.

#### Potential improvement

Inclusion of the following in the item helper: "Be sure to focus on confounders stemming from the acquisition process, such as scanner type, manufacturer, or scanner parameters, or from the individual patient characteristics, such as age, weight, or comorbidities."

```{r}
get_examples_of_disagreement(19)
```

### Item13

*Transparent reporting of feature extraction parameters, otherwise providing a default configuration statement*

Here, the disagreements stem from the LLM assuming that adequate reporting of feature extraction parameters can be simply the explanation of which packages were used. However, as radiomics researchers know, there are multiple possible configurations for these feature packages. Indeed, oftentimes the list of possible features _groups_ may not be sufficient as multiple features can be activated or deactivated inside of each feature group.

#### Potential improvement

Inclusion of the following in the item helper: "Mention of features by name or feature groups is not sufficient. Explicitly check whether the scientific article provides a list of the radiomic features which are being extracted or if it mentions that the default set of parameters was used."

```{r}
get_examples_of_disagreement(13)
```

### Reanalysis with improved prompt

Upon the analysis above, we use the same LLM with the data and the improved descriptions injected into the prompt. While not ideal, this will allow us to see --- to a very limited extent as we are lacking an external validation --- whether this can lead to an improvement. As highlighted in @fig-cohen-raters-llm-improved it is possible to see a clear improvement in performance. @fig-cohen-raters-conditions-llm-improved-subset, specific for the five items with suggestions above, this process can lead to systematic improvements. Additionally, an interesting effect non-linear effect here is that while the condition prompts where not altered, there was a notable improvement to the agreement between LLM and human rater groups for conditions.

```{r}
improved_llm_rating_files <- list.files(
  path = "../ratings_improved/gemini/akinci-dantonoli", 
  pattern = "*json", full.names = T)
names(improved_llm_rating_files) <- lapply(
  improved_llm_rating_files, function(x) gsub(".json", "", str_split(x, "/")[[1]][4])
)
```

```{r}
all_improved_llm_ratings <- list()
for (f in improved_llm_rating_files) {
  title <- gsub(".json", "", str_split(f, "/")[[1]][5])
  json_data <- fromJSON(file = f)
  json_data <- json_data[!(names(json_data) %in% skip_columns)]
  json_df <- data.frame(
    key = names(json_data[-1]), 
    llm_changes = unlist(lapply(json_data[-1], function(x) x$rating)), title = title)
  all_improved_llm_ratings[[length(all_improved_llm_ratings) + 1]] <- json_df
}

improved_llm_df <- do.call(rbind, all_improved_llm_ratings) %>%
  mutate(llm_changes = match_class[llm_changes]) %>%
  mutate(title = gsub("_", "/", title))

# redefine rater vectors to include LLMs
all_raters <- c("no training_1","no training_2","no training_3",
                "with training_1","with training_2","with training_3",
                "llm", "llm_changes")
all_rater_labels <- c(
  "Not\ntrained (1)", "Not\ntrained (2)", "Not\ntrained (3)",
  "Trained (1)", "Trained (2)", "Trained (3)",
  "LLM", "LLM + changes"
)

fuller_df <- merge(
  full_df,
  improved_llm_df, 
  by = c("key", "title"))
```

```{r}
#| warning: false
rater_combinations <- combn(all_raters, 2)

all_cohens <- list()
for (idx in 1:ncol(rater_combinations)) {
  raters <- rater_combinations[, idx]
  sub_df <- subset(fuller_df, !grepl("Condition", key))[c(raters[1], raters[2])] %>%
    na.omit() %>%
    as.matrix()
  if ((nrow(sub_df) > 10) & (var(sub_df[,1]) > 0) & (var(sub_df[,2]) > 0)) { # guarantee that at least 10 articles were graded
    kk <- as.data.frame(cohen.kappa(sub_df)$confid)
    curr_df <- kk
    curr_df$rater1 <- raters[1]
    curr_df$rater2 <- raters[2]
    all_cohens[[length(all_cohens) + 1]] <- curr_df
  }
}

all_cohens_items <- list()
for (k in unique(fuller_df$key)) {
  for (idx in 1:ncol(rater_combinations)) {
    raters <- rater_combinations[, idx]
    sub_df <- subset(fuller_df, key == k)[c(raters[1], raters[2])] %>%
      na.omit() %>%
      as.matrix()
    if ((nrow(sub_df) > 10) & (var(sub_df[,1]) > 0) & (var(sub_df[,2]) > 0)) { # guarantee that at least 10 articles were graded
      kk <- as.data.frame(cohen.kappa(sub_df)$confid)
      curr_df <- kk
      curr_df$rater1 <- raters[1]
      curr_df$rater2 <- raters[2]
      curr_df$item <- k
      curr_df$n <- nrow(sub_df)
      all_cohens_items[[length(all_cohens_items) + 1]] <- curr_df 
    }
  }
}
```

```{r}
cohens_overall <- do.call(rbind, all_cohens) %>% 
  mutate(rater1 = factor(rater1, all_raters, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_raters, all_rater_labels))

cohens_average_items <- do.call(rbind, all_cohens_items) %>% 
  subset(grepl("Item", item)) %>%
  mutate(rater1 = factor(rater1, all_raters, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_raters, all_rater_labels)) %>%
  group_by(rater1, rater2) %>%
  na.omit() %>%
  summarise(se = sd(estimate) / sqrt(length(estimate)),
            estimate = mean(estimate), 
            lower = max(estimate - abs(qnorm(0.025)) * se, -1),
            upper = min(estimate + abs(qnorm(0.025)) * se, 1),
            .groups = "drop")

cohens_average_conditions <- do.call(rbind, all_cohens_items) %>% 
  subset(grepl("Condition", item)) %>%
  mutate(rater1 = factor(rater1, all_raters, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_raters, all_rater_labels)) %>%
  group_by(rater1, rater2) %>%
  na.omit() %>%
  summarise(se = sd(estimate) / sqrt(length(estimate)),
            estimate = mean(estimate), 
            lower = max(estimate - abs(qnorm(0.025)) * se, 0),
            upper = min(estimate + abs(qnorm(0.025)) * se, 1),
            .groups = "drop")

inside_legend_theme <- theme(
  legend.position = c(0.99, 0.05),
  legend.justification = c(1.0, 0.0),
  legend.direction = "horizontal",
  legend.title = element_text(hjust = 1.0))

colourbar_guides <- guides(fill = guide_colorbar(title.position = "top",
                                                 frame.colour = "black",
                                                 ticks.colour = "black",
                                                 ticks.size = 0.5,
                                                 frame.size = 0.5))

cohens_overall_plot <- make_heatmap(cohens_overall) +
  scale_fill_distiller(palette = 3, name = "Cohen's Kappa") +
  inside_legend_theme + 
  colourbar_guides

cohens_average_items_plot <- make_heatmap(cohens_average_items) +
  scale_fill_distiller(palette = 3, name = "Average Cohen's\nKappa across items") +
  inside_legend_theme + 
  colourbar_guides

cohens_average_conditions_plot <- make_heatmap(cohens_average_conditions) +
  scale_fill_distiller(palette = 3, name = "Average Cohen's\nKappa across conditions") +
  inside_legend_theme + 
  colourbar_guides
```

```{r fig-cohen-raters-llm-improved}
#| fig.height: 8.5
#| fig.width: 6.0
#| label: fig-cohen-raters-llm-improved
#| fig-cap: Cohen's Kappa for Akinci D'Antonoli and others (2025) with LLM.
(cohens_overall_plot / 
  cohens_average_items_plot /
  cohens_average_conditions_plot) +
  plot_annotation(tag_levels = c("A"))
```

```{r}
#| fig.height: 3.0
#| fig.width: 6.0
#| label: fig-cohen-raters-conditions-llm-improved-subset
#| fig-cap: Average of Cohen's Kappa calculated between different groups of raters and the LLM for each specific condition and only for items 23, 16, 7, 19, 13 and 10.
do.call(rbind, all_cohens_items) %>% 
  subset(item %in% c("Item23", "Item16", "Item7", "Item19", "Item13", "Item10")) %>%
  mutate(rater1 = factor(rater1, all_raters, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_raters, all_rater_labels)) %>%
  group_by(rater1, rater2) %>%
  na.omit() %>%
  summarise(se = sd(estimate) / sqrt(length(estimate)),
            estimate = mean(estimate), 
            lower = max(estimate - abs(qnorm(0.025)) * se, -1),
            upper = min(estimate + abs(qnorm(0.025)) * se, 1),
            .groups = "drop") %>%
  make_heatmap() +
  scale_fill_distiller(palette = 3, 
                       name = "Average Cohen's Kappa\nacross items 23, 16, 7, 19, 13 and 10") +
  inside_legend_theme + 
  colourbar_guides
```

## Analysing whether radiomic scores are similar between different rater groups and LLMs

Finally, we consider the final quantity of METRICS (i.e. a single score between 0 and 1 which quantifies how much a manuscript follows this rating). We can see that LLM-based assessments tend to over-estimate scores when relative to some readers (@fig-corr-matrix). However, when measuring correlations between all human rater groups and between LLMs and all human rater groups, these are relatively similar, hinting that there is similar variability between other reader groups (@fig-corr-matrix). Mean absolute errors are also not particularly higher than others (@fig-corr-matrix).

```{r}
#| warning: false
SCORES <- c(
    Item1 = 0.0368,
    Item2 = 0.0735,
    Item3 = 0.0919,
    Item4 = 0.0438,
    Item5 = 0.0292,
    Item6 = 0.0438,
    Item7 = 0.0292,
    Item8 = 0.0337,
    Item9 = 0.0225,
    Item10 = 0.0112,
    Item11 = 0.0622,
    Item12 = 0.0311,
    Item13 = 0.0415,
    Item14 = 0.0200,
    Item15 = 0.0200,
    Item16 = 0.0300,
    Item17 = 0.0200,
    Item18 = 0.0599,
    Item19 = 0.0300,
    Item20 = 0.0352,
    Item21 = 0.0234,
    Item22 = 0.0176,
    Item23 = 0.0117,
    Item24 = 0.0293,
    Item25 = 0.0176,
    Item26 = 0.0375,
    Item27 = 0.0749,
    Item28 = 0.0075,
    Item29 = 0.0075,
    Item30 = 0.0075
)

scores_df_long <- fuller_df %>%
  gather(key = "group", value = "value", 
         `no training_1`, `no training_2`, `no training_3`,
         `with training_1`, `with training_2`, `with training_3`,
         llm, llm_changes) %>%
  subset(grepl("Item", key)) %>%
  mutate(multiplier = ifelse(is.na(value), NA, SCORES[key]),
         score = multiplier * value) %>%
  group_by(group, title) %>%
  summarise(score = sum(score, na.rm = T) / sum(multiplier, na.rm = T),
            .groups = "drop") %>%
  mutate(group = factor(group, all_raters, all_rater_labels))

scores_df <- scores_df_long %>%
  spread(key = "group", value = score) %>%
  gather(key = "group", value = "value", 
         `Not\ntrained (1)`, `Not\ntrained (2)`, `Not\ntrained (3)`,
         `Trained (1)`, `Trained (2)`, `Trained (3)`) 

scores_df_wide <- scores_df_long %>% 
  spread(key = "group", value = "score")

all_corr <- list()
rater_combinations_pretty <- combn(all_rater_labels, 2)
for (idx in 1:ncol(rater_combinations_pretty)) {
  raters <- rater_combinations_pretty[, idx]
  sub_df <- scores_df_wide[,c(raters[1], raters[2])] %>%
    na.omit() %>%
    as.matrix()
  corr_output <- cor.test(sub_df[,1], sub_df[,2])
  mae <- mean(abs(sub_df[,1] - sub_df[,2]))
  mae_half_interval = sd(abs(sub_df[,1] - sub_df[,2])) / sqrt(nrow(sub_df)) * qnorm(1 - 0.025)
  curr_df <- data.frame(estimate = corr_output$estimate,
                        lower = corr_output$conf.int[1],
                        upper = corr_output$conf.int[2],
                        mae = mae,
                        mae_lower = mae - mae_half_interval,
                        mae_upper = mae + mae_half_interval
                        )
  curr_df$rater1 <- raters[1]
  curr_df$rater2 <- raters[2]
  all_corr[[length(all_corr) + 1]] <- curr_df
}
```

```{r}
#| fig.height: 2.7
#| fig.width: 4
#| warning: false
#| label: fig-corr
#| fig-cap: Association between LLM with and without changes and human rater groups.

scores_df %>%
  ggplot(aes(colour = gsub("\n", " ", group))) + 
  geom_abline(slope = 1) +
  geom_point(aes(x = value, y = LLM, shape = "LLM")) +
  geom_point(aes(x = value, y = `LLM + changes`, shape = "LLM + changes")) +
  geom_segment(aes(x = value, xend = value, 
                   y = `LLM`, yend = `LLM + changes`)) +
  theme_classic_2(base_size = 8) + 
  scale_shape_manual(values = c(1, 16), name = "LLM") +
  coord_cartesian(xlim = c(0, 1), ylim = c(0, 1)) +
  scale_colour_brewer(palette = "Set2", name = "Group") +
  theme(legend.key.size = unit(0, "lines")) + 
  ylab("METRICS score from LLM rater") +
  xlab("METRICS score from human rater")
```

```{r}
#| fig.height: 6
#| fig.width: 5.5
#| warning: false
#| label: fig-corr-matrix
#| fig-cap: Correlation values (Pearson's R; higher is better) and mean absolute erorr (lower is better) between LLM with and without changes and human rater groups.
pearsons_r_plot <- do.call(rbind, all_corr) %>% 
  mutate(rater1 = factor(rater1, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_rater_labels)) %>%
  make_heatmap() +
  scale_fill_distiller(palette = 3, name = "Pearson's R") +
  inside_legend_theme + 
  colourbar_guides

mae_plot <- do.call(rbind, all_corr) %>% 
  mutate(rater1 = factor(rater1, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_rater_labels)) %>%
  mutate(estimate = mae,
         lower = mae_lower,
         upper = mae_upper) %>%
  make_heatmap() +
  scale_fill_distiller(palette = 3, name = "Mean absolute error") +
  inside_legend_theme + 
  colourbar_guides

(pearsons_r_plot / mae_plot) +
  plot_annotation(tag_levels = c("A"))
```

However, and dispite some similarities, we show that there are more statistically significant differences when performing human-LLM comparisons than when comparing human-human comparisons. We do this through a post-hoc analysis with a Tukey HSD following a statistically significant ANOVA.

```{r}
scores.aov <- aov(lm(score ~ group, data = scores_df_long))

print(summary(scores.aov))

tukey.hsd <- scores.aov %>%
  TukeyHSD()

tukey.hsd <- tukey.hsd$group %>%
  as.data.frame
comparisons <- rownames(tukey.hsd)

tukey.hsd %>%
  mutate(comparisons = case_when(
    !grepl("rained", comparisons) ~ "LLM-LLM",
    !grepl("LLM", comparisons) ~ "Human-Human",
    .default = "Human-LLM")) %>%
  group_by(comparisons) %>%
  summarise(
    `Fraction of statistically significant Tukey HSD` = sum(`p adj` < 0.05) / length(diff),
    `Number of stat. sig. comparisons` = sum(`p adj` < 0.05),
    `Total number of comparisons` = length(diff),
    `Average absolute difference` = mean(abs(diff[`p adj` < 0.05])))
```

## Validation of LLM observations above with *Kocak et. al (2025)*

Around the time we were developing this, Kocak and others published "Radiomics for differentiating radiation-induced brain injury from recurrence in gliomas: systematic review, meta-analysis, and methodological quality evaluation using METRICS and RQS" in European Radiology @Kocak2025-uj. Here, three different raters (identified as raters 1-3) were tasked with classifying 27 papers using METRICS. Here, we provide LLM-based METRICS scores for only 22 as the remaining 5 were behind a paywall. It should be noted that the first author (and one of the raters as far we understand it) in *Kocak et al. (2025)* was also a co-author in @Akinci-D-Antonoli2025-ep and the first author of the original METRICS definition paper @Kocak2024-wk.

As visible below across @fig-cohen-kocak, the inter-rater agreements are relatively stable between readers and between readers and LLMs. An interesting outcome of this is that the LLM + changes do not lead to such stark improvements as before, hinting that dataset-specific prompt tuning may not be the best way of improving scores across multiple publications.

```{r}
df_original <- read_csv(
  "../data/kocak/inter-rater.csv", col_types = c(rep("c", 40), "i", "i")) 
df <- df_original %>% 
  gather(key = "key", value = "value", 
         -rater, -dois, -dois_clean, -title) %>% 
  subset(key != "Total METRICS score:") %>%
  subset(!grepl("Quality", key)) %>%
  mutate(key = gsub("#", "", key)) %>%
  mutate(value = match_class[value]) %>%
  mutate(rater = sprintf("Rater%s", rater))

all_dfs$kocak <- df

df_original
```

```{r}
llm_rating_files <- list.files(path = "../ratings/gemini/kocak", pattern = "*json", full.names = T)
names(llm_rating_files) <- lapply(
  llm_rating_files, function(x) gsub(".json", "", str_split(x, "/")[[1]][5])
)

all_llm_ratings <- list()
for (f in llm_rating_files) {
  title <- gsub(".json", "", str_split(f, "/")[[1]][5])
  json_data <- fromJSON(file = f)
  json_data <- json_data[!(names(json_data) %in% skip_columns)]
  json_df <- data.frame(
    key = names(json_data[-1]), 
    llm = unlist(lapply(json_data[-1], function(x) x$rating)), title = title)
  all_llm_ratings[[length(all_llm_ratings) + 1]] <- json_df
}

llm_df <- do.call(rbind, all_llm_ratings) %>%
  mutate(llm = match_class[llm]) %>%
  mutate(title = gsub("_", "/", title))

improved_llm_rating_files <- list.files(
  path = "../ratings_improved/gemini/kocak", 
  pattern = "*json", full.names = T)
names(improved_llm_rating_files) <- lapply(
  improved_llm_rating_files, function(x) gsub(".json", "", str_split(x, "/")[[1]][4])
)

all_improved_llm_ratings <- list()
for (f in improved_llm_rating_files) {
  title <- gsub(".json", "", str_split(f, "/")[[1]][5])
  json_data <- fromJSON(file = f)
  json_data <- json_data[!(names(json_data) %in% skip_columns)]
  json_df <- data.frame(
    key = names(json_data[-1]), 
    llm_changes = unlist(lapply(json_data[-1], function(x) x$rating)), title = title)
  all_improved_llm_ratings[[length(all_improved_llm_ratings) + 1]] <- json_df
}

improved_llm_df <- do.call(rbind, all_improved_llm_ratings) %>%
  mutate(llm_changes = match_class[llm_changes]) %>%
  mutate(title = gsub("_", "/", title))

full_df <- merge(
  pivot_wider(df, values_from = value, names_from = c("rater")), 
  llm_df, 
  by = c("key", "title"))

fuller_df <- merge(
  full_df,
  improved_llm_df, 
  by = c("key", "title"))
```

```{r}
#| warning: false
all_raters <- c("Rater1", "Rater2", "Rater3",
                "llm", "llm_changes")
all_rater_labels <- c(
  "Rater 1", "Rater 2", "Rater 3",
  "LLM", "LLM + changes"
)

rater_combinations <- combn(all_raters, 2)

all_cohens <- list()
for (idx in 1:ncol(rater_combinations)) {
  raters <- rater_combinations[, idx]
  sub_df <- subset(fuller_df, !grepl("Condition", key))[c(raters[1], raters[2])] %>%
    na.omit() %>%
    as.matrix()
  if ((nrow(sub_df) > 10) & (var(sub_df[,1]) > 0) & (var(sub_df[,2]) > 0)) { # guarantee that at least 10 articles were graded
    kk <- as.data.frame(cohen.kappa(sub_df)$confid)
    curr_df <- kk
    curr_df$rater1 <- raters[1]
    curr_df$rater2 <- raters[2]
    all_cohens[[length(all_cohens) + 1]] <- curr_df
  }
}

all_cohens_items <- list()
for (k in unique(fuller_df$key)) {
  for (idx in 1:ncol(rater_combinations)) {
    raters <- rater_combinations[, idx]
    sub_df <- subset(fuller_df, key == k)[c(raters[1], raters[2])] %>%
      na.omit() %>%
      as.matrix()
    if ((nrow(sub_df) > 10) & (var(sub_df[,1]) > 0) & (var(sub_df[,2]) > 0)) { # guarantee that at least 10 articles were graded
      kk <- as.data.frame(cohen.kappa(sub_df)$confid)
      curr_df <- kk
      curr_df$rater1 <- raters[1]
      curr_df$rater2 <- raters[2]
      curr_df$item <- k
      curr_df$n <- nrow(sub_df)
      all_cohens_items[[length(all_cohens_items) + 1]] <- curr_df 
    }
  }
}
```

```{r}
cohens_overall <- do.call(rbind, all_cohens) %>% 
  mutate(rater1 = factor(rater1, all_raters, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_raters, all_rater_labels))

cohens_average_items <- do.call(rbind, all_cohens_items) %>% 
  subset(grepl("Item", item)) %>%
  mutate(rater1 = factor(rater1, all_raters, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_raters, all_rater_labels)) %>%
  group_by(rater1, rater2) %>%
  na.omit() %>%
  summarise(se = sd(estimate) / sqrt(length(estimate)),
            estimate = mean(estimate), 
            lower = max(estimate - abs(qnorm(0.025)) * se, 0),
            upper = min(estimate + abs(qnorm(0.025)) * se, 1),
            .groups = "drop")

cohens_average_conditions <- do.call(rbind, all_cohens_items) %>% 
  subset(grepl("Condition", item)) %>%
  mutate(rater1 = factor(rater1, all_raters, all_rater_labels)) %>%
  mutate(rater2 = factor(rater2, all_raters, all_rater_labels)) %>%
  group_by(rater1, rater2) %>%
  na.omit() %>%
  summarise(se = sd(estimate) / sqrt(length(estimate)),
            estimate = mean(estimate), 
            lower = max(estimate - abs(qnorm(0.025)) * se, 0),
            upper = min(estimate + abs(qnorm(0.025)) * se, 1),
            .groups = "drop")

cohens_overall_plot <- make_heatmap(cohens_overall) +
  scale_fill_distiller(palette = 3, name = "Cohen's Kappa") +
  inside_legend_theme + 
  colourbar_guides

cohens_average_items_plot <- make_heatmap(cohens_average_items) +
  scale_fill_distiller(palette = 3, name = "Average Cohen's\nKappa across items") +
  inside_legend_theme + 
  colourbar_guides

cohens_average_conditions_plot <- make_heatmap(cohens_average_conditions) +
  scale_fill_distiller(palette = 3, name = "Average Cohen's\nKappa across conditions") +
  inside_legend_theme + 
  colourbar_guides
```

```{r}
#| fig.height: 6.0
#| fig.width: 4.2
#| label: fig-cohen-kocak
#| fig-cap: Cohen's Kappa calculated between different groups of raters and the LLM for _Kocak et al. (2025)_.

(cohens_overall_plot / 
  cohens_average_items_plot /
  cohens_average_conditions_plot) +
  plot_annotation(tag_levels = c("A"))
```

```{r}
scores_df_long <- fuller_df %>%
  gather(key = "group", value = "value", 
         Rater1, Rater2, Rater3,
         llm, llm_changes) %>%
  subset(grepl("Item", key)) %>%
  mutate(multiplier = ifelse(is.na(value), NA, SCORES[key]),
         score = multiplier * value) %>%
  group_by(group, title) %>%
  summarise(score = sum(score, na.rm = T) / sum(multiplier, na.rm = T),
            .groups = "drop") %>%
  mutate(group = factor(group, all_raters, all_rater_labels))

scores.aov <- aov(lm(score ~ group, data = scores_df_long))

print(summary(scores.aov))

tukey.hsd <- scores.aov %>%
  TukeyHSD()

tukey.hsd <- tukey.hsd$group %>%
  as.data.frame
comparisons <- rownames(tukey.hsd)

tukey.hsd %>%
  mutate(comparisons = case_when(
    !grepl("Rater", comparisons) ~ "LLM-LLM",
    !grepl("LLM", comparisons) ~ "Human-Human",
    .default = "Human-LLM")) %>%
  group_by(comparisons) %>%
  summarise(
    `Fraction of statistically significant Tukey HSD` = sum(`p adj` < 0.05) / length(diff),
    `Number of stat. sig. comparisons` = sum(`p adj` < 0.05),
    `Total number of comparisons` = length(diff),
    `Average absolute difference` = mean(abs(diff[`p adj` < 0.05])))
```

## Analysis with local LLMs

To better understand how local LLMs can perform this task, we make use of a small panel of local LLMs as provided by Ollama and with their default quantizations:

- Gemma3 (4b)
- Gemma3 (12b)
- Llama3.2 (3b)
- Qwen2.5 (3b)
- Qwen2.5 (7b)
- Mistral (7b)

```{r}
all_llm_ratings <- list()
for (rating_id in c("ratings", "ratings_improved")) {
  for (model in list.files(sprintf("../%s", rating_id), pattern = "*")) {
    for (file in list.files(sprintf("../%s/%s", rating_id, model), 
                            pattern = "*json", recursive = T,
                            full.names = T)) {
      information <- str_split(file, pattern = "/")[[1]]
      model <- information[3]
      paper_group <- ifelse(information[4] == "kocak", 
                            "Kocak (2025)", "Akinci D'Antonoli (2025)")
      paper <- information[5]
      json_data <- fromJSON(file = file)
      if (is.null(json_data$metadata$error)) {
        json_data <- json_data[!(names(json_data) %in% skip_columns)]
        json_df <- data.frame(
          key = names(json_data), 
          value_llm = unlist(lapply(json_data, function(x) x$rating)), 
          title = title,
          rater_llm = ifelse(rating_id == "ratings", "llm", "llm_changes"),
          set = paper_group, 
          model = model)
        all_llm_ratings[[length(all_llm_ratings) + 1]] <- json_df
      }
    }
  }
}

all_llm_df <- do.call(rbind, all_llm_ratings) %>%
  mutate(value_llm = match_class[value_llm]) %>%
  mutate(title = gsub("_", "/", title)) %>%
  mutate(key = gsub("_[A-Za-z0-9]+", "", key))
rownames(all_llm_df) <- NULL

all_raters <- c("no training_1","no training_2","no training_3",
                "with training_1","with training_2","with training_3",
                "Rater1", "Rater2", "Rater3",
                "llm", "llm_changes")
all_rater_labels <- c(
  "Not\ntrained (1)", "Not\ntrained (2)", "Not\ntrained (3)",
  "Trained (1)", "Trained (2)", "Trained (3)",
  "Rater 1", "Rater 2", "Rater 3",
  "LLM", "LLM + changes"
)

all_human_raters <- rbind(
  all_dfs$akinci_dantonoli %>%
    mutate(rater = paste(group, exp, sep = "_"), 
           set = "Akinci D'Antonoli (2025)") %>%
    select(rater, title, key, value, set),
  all_dfs$kocak %>%
    mutate(set = "Kocak (2025)") %>%
    select(rater, title, key, value, set)
)

complete_comparisons <- merge(
  all_human_raters, 
  all_llm_df, 
  by = c("title", "key")) %>%
  mutate(set = set.y) %>%
  select(-set.y, -set.x)

complete_comparisons
```

```{r}
cohens_for_all <- complete_comparisons %>%
  group_by(model, rater, rater_llm, set) %>%
  summarise(
    CK = list(cohen.kappa(data.frame(value, value_llm))),
    .groups = "drop") %>%
  rowwise() %>%
  mutate(lower = CK$confid[1, 1],
         estimate = CK$confid[1, 2],
         upper = CK$confid[1, 3]) %>%
  select(-CK) %>%
  rowwise() %>%
  mutate(model_name = gsub("-[0-9]+b.*", "", model),
         param_count = str_match(model, "[0-9]+(?=b)")[[1]]) %>%
  mutate(
    model_label = ifelse(
      model != "gemini",
      sprintf("%s (%sb)", str_to_title(model_name), param_count),
      "Gemini"))

model_label_order <- str_sort(unique(cohens_for_all$model_label), numeric = T)
model_label_order <- model_label_order[model_label_order != "Gemini"]
model_label_order <- c(model_label_order, "Gemini")

cohens_for_all$model_label <- factor(
  cohens_for_all$model_label,
  model_label_order)
```

```{r}
#| fig.height: 4
#| fig.width: 6
cohens_for_all %>%
  mutate(
    model_provider = gsub("[0-9\\.]+", "", gsub("-Small", "", gsub(" \\(.*\\)", "", model_label)))) %>%
  ggplot(aes(x = estimate, y = model_label, colour = set)) +
  geom_vline(xintercept = 0) +
  geom_point(position = position_jitter(height = 0.10)) +
  theme_classic_2(base_size = 8) +
  scale_colour_brewer(palette = "Set1", name = "Source") +
  xlab("Cohen's Kappa") +
  ylab("") + 
  facet_grid(model_provider ~ ., scales = "free", space = "free") +
  theme(legend.position = "bottom",
        legend.key.size = unit(0.5, "line"),
        strip.text.y = element_text(angle = 0))
```

# Conclusion and where to go from here

This was a somewhat quick analysis prompted by a concrete question --- can we facilitate what are oftentimes cumbersome analyses by getting the assistance of an LLM? The answer to this --- at least what this analysis shows --- is that:

1.  There is a reasonable amount of agreement between LLMs and human rater groups, but there is also a reasonable amount of differences between human rater groups and LLMs. This is not absurd as this system in particular --- METRICS --- relies quite heavily on expert knowledge. The paper that originated this short report highlights, to an extent, the differences in subjective evaluation associated with human raters @Akinci-D-Antonoli2025-ep. While more objective, LLM-based assessments may end up being too different. However, our confirmation using @Kocak2025-uj does show that these findings --- that there is a good number of similarities between human raters and LLMs --- generalise to other datasets.
2.  Improvements are possible from relatively simple analyses which place the human in the loop, suggesting that more data and further optimization can lead to a robust automated METRICS assessment system. While automated prompt engineering could be an alternative, there is a significant amount of work which goes into the annotation of these manuscripts, making these approaches --- which require a good number of examples --- quite tricky to handle when sample sizes are small. Our analysis on @Kocak2025-uj showed that prompt tuning using small sample sizes is likely to result in improvements which lead to limited improvements. However, both the tuning and its validation used relatively small dataset sizes. Perhaps automatic prompt tuning can improve this.

```{r}
sessionInfo()
```