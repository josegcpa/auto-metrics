@ARTICLE{Kocak2024-wk,
  title    = "{METhodological} {RadiomICs} Score ({METRICS}): a quality scoring
              tool for radiomics research endorsed by {EuSoMII}",
  author   = "Kocak, Burak and Akinci D'Antonoli, Tugba and Mercaldo, Nathaniel
              and Alberich-Bayarri, Angel and Baessler, Bettina and Ambrosini,
              Ilaria and Andreychenko, Anna E and Bakas, Spyridon and Beets-Tan,
              Regina G H and Bressem, Keno and Buvat, Irene and Cannella,
              Roberto and Cappellini, Luca Alessandro and Cavallo, Armando Ugo
              and Chepelev, Leonid L and Chu, Linda Chi Hang and Demircioglu,
              Aydin and deSouza, Nandita M and Dietzel, Matthias and Fanni,
              Salvatore Claudio and Fedorov, Andrey and Fournier, Laure S and
              Giannini, Valentina and Girometti, Rossano and Groot Lipman, Kevin
              B W and Kalarakis, Georgios and Kelly, Brendan S and Klontzas,
              Michail E and Koh, Dow-Mu and Kotter, Elmar and Lee, Ho Yun and
              Maas, Mario and Marti-Bonmati, Luis and Müller, Henning and
              Obuchowski, Nancy and Orlhac, Fanny and Papanikolaou, Nikolaos and
              Petrash, Ekaterina and Pfaehler, Elisabeth and Pinto Dos Santos,
              Daniel and Ponsiglione, Andrea and Sabater, Sebastià and
              Sardanelli, Francesco and Seeböck, Philipp and Sijtsema, Nanna M
              and Stanzione, Arnaldo and Traverso, Alberto and Ugga, Lorenzo and
              Vallières, Martin and van Dijk, Lisanne V and van Griethuysen,
              Joost J M and van Hamersvelt, Robbert W and van Ooijen, Peter and
              Vernuccio, Federica and Wang, Alan and Williams, Stuart and
              Witowski, Jan and Zhang, Zhongyi and Zwanenburg, Alex and Cuocolo,
              Renato",
  journal  = "Insights Imaging",
  volume   =  15,
  number   =  1,
  pages    =  8,
  abstract = "PURPOSE: To propose a new quality scoring tool, METhodological
              RadiomICs Score (METRICS), to assess and improve research quality
              of radiomics studies. METHODS: We conducted an online modified
              Delphi study with a group of international experts. It was
              performed in three consecutive stages: Stage\#1, item preparation;
              Stage\#2, panel discussion among EuSoMII Auditing Group members to
              identify the items to be voted; and Stage\#3, four rounds of the
              modified Delphi exercise by panelists to determine the items
              eligible for the METRICS and their weights. The consensus
              threshold was 75\%. Based on the median ranks derived from expert
              panel opinion and their rank-sum based conversion to importance
              scores, the category and item weights were calculated. RESULT: In
              total, 59 panelists from 19 countries participated in selection
              and ranking of the items and categories. Final METRICS tool
              included 30 items within 9 categories. According to their weights,
              the categories were in descending order of importance: study
              design, imaging data, image processing and feature extraction,
              metrics and comparison, testing, feature processing, preparation
              for modeling, segmentation, and open science. A web application
              and a repository were developed to streamline the calculation of
              the METRICS score and to collect feedback from the radiomics
              community. CONCLUSION: In this work, we developed a scoring tool
              for assessing the methodological quality of the radiomics
              research, with a large international panel and a modified Delphi
              protocol. With its conditional format to cover methodological
              variations, it provides a well-constructed framework for the key
              methodological concepts to assess the quality of radiomic research
              papers. CRITICAL RELEVANCE STATEMENT: A quality assessment tool,
              METhodological RadiomICs Score (METRICS), is made available by a
              large group of international domain experts, with transparent
              methodology, aiming at evaluating and improving research quality
              in radiomics and machine learning. KEY POINTS: • A methodological
              scoring tool, METRICS, was developed for assessing the quality of
              radiomics research, with a large international expert panel and a
              modified Delphi protocol. • The proposed scoring tool presents
              expert opinion-based importance weights of categories and items
              with a transparent methodology for the first time. • METRICS
              accounts for varying use cases, from handcrafted radiomics to
              entirely deep learning-based pipelines. • A web application has
              been developed to help with the calculation of the METRICS score (
              https://metricsscore.github.io/metrics/METRICS.html ) and a
              repository created to collect feedback from the radiomics
              community ( https://github.com/metricsscore/metrics ).",
  month    =  jan,
  year     =  2024,
  keywords = "Artificial intelligence; Deep learning; Guideline; Machine
              learning; Radiomics",
  language = "en"
}

@ARTICLE{Van_Timmeren2020-gy,
  title    = "Radiomics in medical imaging-``how-to'' guide and critical
              reflection",
  author   = "van Timmeren, Janita E and Cester, Davide and Tanadini-Lang,
              Stephanie and Alkadhi, Hatem and Baessler, Bettina",
  journal  = "Insights Imaging",
  volume   =  11,
  number   =  1,
  pages    =  91,
  abstract = "Radiomics is a quantitative approach to medical imaging, which
              aims at enhancing the existing data available to clinicians by
              means of advanced mathematical analysis. Through mathematical
              extraction of the spatial distribution of signal intensities and
              pixel interrelationships, radiomics quantifies textural
              information by using analysis methods from the field of artificial
              intelligence. Various studies from different fields in imaging
              have been published so far, highlighting the potential of
              radiomics to enhance clinical decision-making. However, the field
              faces several important challenges, which are mainly caused by the
              various technical factors influencing the extracted radiomic
              features.The aim of the present review is twofold: first, we
              present the typical workflow of a radiomics analysis and deliver a
              practical ``how-to'' guide for a typical radiomics analysis.
              Second, we discuss the current limitations of radiomics, suggest
              potential improvements, and summarize relevant literature on the
              subject.",
  month    =  aug,
  year     =  2020,
  keywords = "Machine learning; Quantitative imaging biomarkers; Radiomics;
              Robustness; Standardization",
  language = "en"
}

@ARTICLE{Kocak2024-sv,
  title     = "Exploring radiomics research quality scoring tools: a comparative
               analysis of {METRICS} and {RQS}",
  author    = "Koçak, Burak and Akinci D'Antonoli, Tugba and Cuocolo, Renato",
  journal   = "Diagn. Interv. Radiol.",
  publisher = "Galenos Yayinevi",
  volume    =  30,
  number    =  6,
  pages     = "366--369",
  month     =  nov,
  year      =  2024,
  language  = "en"
}

@ARTICLE{Lambin2017-rn,
  title     = "Radiomics: the bridge between medical imaging and personalized
               medicine",
  author    = "Lambin, Philippe and Leijenaar, Ralph T H and Deist, Timo M and
               Peerlings, Jurgen and de Jong, Evelyn E C and van Timmeren,
               Janita and Sanduleanu, Sebastian and Larue, Ruben T H M and Even,
               Aniek J G and Jochems, Arthur and van Wijk, Yvonka and Woodruff,
               Henry and van Soest, Johan and Lustberg, Tim and Roelofs, Erik
               and van Elmpt, Wouter and Dekker, Andre and Mottaghy, Felix M and
               Wildberger, Joachim E and Walsh, Sean",
  journal   = "Nat. Rev. Clin. Oncol.",
  publisher = "Nature Publishing Group",
  volume    =  14,
  number    =  12,
  pages     = "749--762",
  abstract  = "Radiomics, the high-throughput mining of quantitative image
               features from standard-of-care medical imaging that enables data
               to be extracted and applied within clinical-decision support
               systems to improve diagnostic, prognostic, and predictive
               accuracy, is gaining importance in cancer research. Radiomic
               analysis exploits sophisticated image analysis tools and the
               rapid development and validation of medical imaging data that
               uses image-based signatures for precision diagnosis and
               treatment, providing a powerful tool in modern medicine. Herein,
               we describe the process of radiomics, its pitfalls, challenges,
               opportunities, and its capacity to improve clinical decision
               making, emphasizing the utility for patients with cancer.
               Currently, the field of radiomics lacks standardized evaluation
               of both the scientific integrity and the clinical relevance of
               the numerous published radiomics investigations resulting from
               the rapid growth of this area. Rigorous evaluation criteria and
               reporting guidelines need to be established in order for
               radiomics to mature as a discipline. Herein, we provide guidance
               for investigations to meet this urgent need in the field of
               radiomics.",
  month     =  dec,
  year      =  2017,
  language  = "en"
}

@ARTICLE{Akinci-D-Antonoli2025-ep,
  title     = "Reproducibility of methodological radiomics score ({METRICS}): an
               intra- and inter-rater reliability study endorsed by {EuSoMII}",
  author    = "Akinci D'Antonoli, Tugba and Cavallo, Armando Ugo and Kocak,
               Burak and Borgheresi, Alessandra and Ponsiglione, Andrea and
               Stanzione, Arnaldo and Koltsakis, Emmanouil and Doniselli, Fabio
               Martino and Vernuccio, Federica and Ugga, Lorenzo and
               Triantafyllou, Matthaios and Huisman, Merel and Klontzas, Michail
               E and Trotta, Romina and Cannella, Roberto and Fanni, Salvatore
               Claudio and Cuocolo, Renato",
  journal   = "Eur. Radiol.",
  publisher = "Springer",
  pages     = "1--13",
  abstract  = "OBJECTIVES: To investigate the intra- and inter-rater reliability
               of the total methodological radiomics score (METRICS) and its
               items through a multi-reader analysis. MATERIALS AND METHODS: A
               total of 12 raters with different backgrounds and experience
               levels were recruited for the study. Based on their level of
               expertise, raters were randomly assigned to the following groups:
               two inter-rater reliability groups, and two intra-rater
               reliability groups, where each group included one group with and
               one group without a preliminary training session on the use of
               METRICS. Inter-rater reliability groups assessed all 34 papers,
               while intra-rater reliability groups completed the assessment of
               17 papers twice within 21 days each time, and a ``wash out''
               period of 60 days in between. RESULTS: Inter-rater reliability
               was poor to moderate between raters of group 1 (without training;
               ICC = 0.393; 95\% CI = 0.115-0.630; p = 0.002), and between
               raters of group 2 (with training; ICC = 0.433; 95\% CI =
               0.127-0.671; p = 0.002). The intra-rater analysis was excellent
               for raters 9 and 12, good to excellent for raters 8 and 10,
               moderate to excellent for rater 7, and poor to good for rater 11.
               CONCLUSION: The intra-rater reliability of the METRICS score was
               relatively good, while the inter-rater reliability was relatively
               low. This highlights the need for further efforts to achieve a
               common understanding of METRICS items, as well as resources
               consisting of explanations, elaborations, and examples to improve
               reproducibility and enhance their usability and robustness. KEY
               POINTS: Questions Guidelines and scoring tools are necessary to
               improve the quality of radiomics research; however, the
               application of these tools is challenging for less experienced
               raters. Findings Intra-rater reliability was high across all
               raters regardless of experience level or previous training, and
               inter-rater reliability was generally poor to moderate across
               raters. Clinical relevance Guidelines and scoring tools are
               necessary for proper reporting in radiomics research and for
               closing the gap between research and clinical implementation.
               There is a need for further resources offering explanations,
               elaborations, and examples to enhance the usability and
               robustness of these guidelines.",
  month     =  feb,
  year      =  2025,
  keywords  = "Artificial intelligence; Inter-observer variability;
               Intra-observer variability; Radiomics; Reproducibility of results",
  language  = "en"
}
